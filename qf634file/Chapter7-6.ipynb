{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c65022cc",
   "metadata": {},
   "source": [
    "In this model -- same adam, sigmoids, 12-8-8-1 structure, batch_size 100, epoch 500. Thus 40,000 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "617ee135",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c6deff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Dataset\n",
    "data = pd.read_csv(\"Churn_Modelling.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a80540e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CreditScore Geography  Gender  Age  Tenure    Balance  NumOfProducts  \\\n",
       "0          619    France  Female   42       2       0.00              1   \n",
       "1          608     Spain  Female   41       1   83807.86              1   \n",
       "2          502    France  Female   42       8  159660.80              3   \n",
       "3          699    France  Female   39       1       0.00              2   \n",
       "4          850     Spain  Female   43       2  125510.82              1   \n",
       "\n",
       "   HasCrCard  IsActiveMember  EstimatedSalary  \n",
       "0          1               1        101348.88  \n",
       "1          0               1        112542.58  \n",
       "2          1               0        113931.57  \n",
       "3          0               0         93826.63  \n",
       "4          1               1         79084.10  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generating Dependent Variable Vectors\n",
    "Y = data.iloc[:,-1].values\n",
    "X = data.iloc[:,3:13]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a98c381c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0\n",
      "1       0\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "       ..\n",
      "9995    1\n",
      "9996    1\n",
      "9997    0\n",
      "9998    1\n",
      "9999    0\n",
      "Name: Gender, Length: 10000, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Generating Dependent Variable Vectors\n",
    "Y = data.iloc[:,-1].values\n",
    "X = data.iloc[:,3:13]\n",
    "X['Gender']=X['Gender'].map({'Female':0,'Male':1})\n",
    "### above is used instead of a more complicated package involving -- from sklearn.preprocessing import LabelEncoder\n",
    "### converts Female -- 0, Male -- 1, i.e. hot-encoding categorical variables\n",
    "print (X['Gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e436f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding Categorical variable Geography\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ct =ColumnTransformer(transformers=[('encoder',OneHotEncoder(),[1])],remainder=\"passthrough\")\n",
    "X = np.array(ct.fit_transform(X))\n",
    "### Geography is transformed into France -- 1,0,0; Spain -- 0,0,1; Germany -- 0,1,0.\n",
    "### Moreover -- this encoded vector of ones-zeros is now put in first 3 cols. Credit Score pushed to 4th col."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3166a509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>619.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101348.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>608.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>112542.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>502.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>113931.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>699.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93826.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>850.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>79084.10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0    1    2      3    4     5    6          7    8    9    10         11\n",
       "0  1.0  0.0  0.0  619.0  0.0  42.0  2.0       0.00  1.0  1.0  1.0  101348.88\n",
       "1  0.0  0.0  1.0  608.0  0.0  41.0  1.0   83807.86  1.0  0.0  1.0  112542.58\n",
       "2  1.0  0.0  0.0  502.0  0.0  42.0  8.0  159660.80  3.0  1.0  0.0  113931.57\n",
       "3  1.0  0.0  0.0  699.0  0.0  39.0  1.0       0.00  2.0  0.0  0.0   93826.63\n",
       "4  0.0  0.0  1.0  850.0  0.0  43.0  2.0  125510.82  1.0  1.0  1.0   79084.10"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### convert X to dataframe X1\n",
    "X1 = pd.DataFrame(X)\n",
    "X1.head()\n",
    "### Note there are 12 features including onehotencoder for the Geography feature-- \n",
    "### The features are encoded using a one-hot (aka ‘one-of-K’ or ‘dummy’) encoding scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ef915ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting dataset into training and testing dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "740ede9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9850b84b",
   "metadata": {},
   "source": [
    "We call fit_transform() method on our training data and transform() method on our test data. Each feature in the training\n",
    "set is scaled to mean 0, variance 1. In sklearn.preprocessing.StandardScaler(), centering and scaling happens independently on each feature. The fit method is calculating the mean and variance of each of the features present in the data. The transform method is transforming all the features using the respective feature's mean and variance that are calculated in the statement\n",
    "before on X_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f43f1db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### This is the very first step while creating NNmodel. Here we are going to create our ann object by using a certain class of Keras \n",
    "### named Sequential. As a part of tensorflow 2.0, Keras is now integrated with tensorflow and is now considered as a \n",
    "### sub-library of tensorflow. The Sequential class is a part of the models module of Keras library which is a part of the \n",
    "### tensorflow library now. \n",
    "### It used to be \"import tensorflow as tf; from tensorflow import keras; from tensorflow.keras import layers\"\n",
    "### See documentation at https://keras.io/guides/sequential_model/\n",
    "\n",
    "#Initialising the NN model name -- NNmodel\n",
    "NNmodel = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34730cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating a network that has 2 hidden layers together with 1 input layer and 1 output layer. \n",
    "#Adding First Hidden Layer\n",
    "NNmodel.add(tf.keras.layers.Dense(units=8,activation=\"sigmoid\"))\n",
    "### units = 8 refer to 8 neurons in hidden layer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab12edcf",
   "metadata": {},
   "source": [
    "Above -- first hidden layer is created using the Dense class which is part of the layers module. This class accepts 2 inputs:-\n",
    "(1) units:- number of neurons that will be present in the respective layer (2) activation:- specify which activation function to be used. This example uses first input as 8. There is no correct answer which is the right number of neurons in the layer -- trial and error. Not too large to be computationally impractical or redundant; not too small to be ineffective.\n",
    "For the second input, we try the sigmoid or logistic function as an activation function for hidden layers. We can also try “relu”[rectified linear unit]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e22a225d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating 2nd hidden layer \n",
    "#Adding Second Hidden Layer -- note this is added sequentially to the first hidden layer\n",
    "NNmodel.add(tf.keras.layers.Dense(units=8,activation=\"sigmoid\"))\n",
    "### units = 8 refer to 8 neurons in hidden layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "303997d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### now we create the output layer -- this is added sequentially\n",
    "#Adding Output Layer\n",
    "NNmodel.add(tf.keras.layers.Dense(units=1,activation=\"sigmoid\"))\n",
    "### Only 1 output neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b4a623",
   "metadata": {},
   "source": [
    "For a binary classification problem as above, actual case output is 1 or 0. Hence we require only one neuron to output layer - output could be estimated probability of case actual output = 1. For multiclass classification problem, if the output contains m categories then we need to create m different neurons, one for each category. In the binary output case, the suitable activation function is the sigmoid function. For multiclass classification problem, the activation function is typically softmax. The softmax function predicts a multinomial probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70baaa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "### After creating the layers -- require compiling the NNmodel. Compiling allows the computer to run and understand the program \n",
    "### without the need of more fundamental steps in the programming. Compiling adds other elements or linking other libraries, and optimization,\n",
    "### such that after compiling the results are readily computed e.g. in a binary executable program as an output. \n",
    "#Compiling NNmodel\n",
    "NNmodel.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=['accuracy'])\n",
    "### Note optimizer here is a more sophisticated version of the Mean Square loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f9d245",
   "metadata": {},
   "source": [
    "Compile method above accepts inputs: (1) optimizer:- specifies which optimizer to be used in order to perform stochastic gradient descent (2) error/loss function, e.g., 'binary_crossentropy' here. For multiclass classification, it should be categorical_crossentropy, (3) metrics - the performance metrics to use in order to compute performance. 'accuracy' is one such  performance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51697cc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 12)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36a93b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "80/80 [==============================] - 0s 710us/step - loss: 0.5833 - accuracy: 0.7972\n",
      "Epoch 2/500\n",
      "80/80 [==============================] - 0s 569us/step - loss: 0.5151 - accuracy: 0.7972\n",
      "Epoch 3/500\n",
      "80/80 [==============================] - 0s 601us/step - loss: 0.4985 - accuracy: 0.7972\n",
      "Epoch 4/500\n",
      "80/80 [==============================] - 0s 559us/step - loss: 0.4912 - accuracy: 0.7972\n",
      "Epoch 5/500\n",
      "80/80 [==============================] - 0s 555us/step - loss: 0.4849 - accuracy: 0.7972\n",
      "Epoch 6/500\n",
      "80/80 [==============================] - 0s 561us/step - loss: 0.4781 - accuracy: 0.7972\n",
      "Epoch 7/500\n",
      "80/80 [==============================] - 0s 546us/step - loss: 0.4708 - accuracy: 0.7972\n",
      "Epoch 8/500\n",
      "80/80 [==============================] - 0s 556us/step - loss: 0.4636 - accuracy: 0.7972\n",
      "Epoch 9/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.4566 - accuracy: 0.7972\n",
      "Epoch 10/500\n",
      "80/80 [==============================] - 0s 559us/step - loss: 0.4501 - accuracy: 0.7972\n",
      "Epoch 11/500\n",
      "80/80 [==============================] - 0s 546us/step - loss: 0.4445 - accuracy: 0.7972\n",
      "Epoch 12/500\n",
      "80/80 [==============================] - 0s 556us/step - loss: 0.4397 - accuracy: 0.7972\n",
      "Epoch 13/500\n",
      "80/80 [==============================] - 0s 545us/step - loss: 0.4358 - accuracy: 0.7994\n",
      "Epoch 14/500\n",
      "80/80 [==============================] - 0s 548us/step - loss: 0.4326 - accuracy: 0.8074\n",
      "Epoch 15/500\n",
      "80/80 [==============================] - 0s 545us/step - loss: 0.4301 - accuracy: 0.8110\n",
      "Epoch 16/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.4280 - accuracy: 0.8130\n",
      "Epoch 17/500\n",
      "80/80 [==============================] - 0s 544us/step - loss: 0.4264 - accuracy: 0.8116\n",
      "Epoch 18/500\n",
      "80/80 [==============================] - 0s 548us/step - loss: 0.4248 - accuracy: 0.8129\n",
      "Epoch 19/500\n",
      "80/80 [==============================] - 0s 572us/step - loss: 0.4234 - accuracy: 0.8136\n",
      "Epoch 20/500\n",
      "80/80 [==============================] - 0s 595us/step - loss: 0.4219 - accuracy: 0.8148\n",
      "Epoch 21/500\n",
      "80/80 [==============================] - 0s 568us/step - loss: 0.4205 - accuracy: 0.8158\n",
      "Epoch 22/500\n",
      "80/80 [==============================] - 0s 569us/step - loss: 0.4190 - accuracy: 0.8154\n",
      "Epoch 23/500\n",
      "80/80 [==============================] - 0s 548us/step - loss: 0.4174 - accuracy: 0.8174\n",
      "Epoch 24/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.4157 - accuracy: 0.8200\n",
      "Epoch 25/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.4140 - accuracy: 0.8209\n",
      "Epoch 26/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.4123 - accuracy: 0.8245\n",
      "Epoch 27/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.4105 - accuracy: 0.8264\n",
      "Epoch 28/500\n",
      "80/80 [==============================] - 0s 547us/step - loss: 0.4087 - accuracy: 0.8261\n",
      "Epoch 29/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.4069 - accuracy: 0.8295\n",
      "Epoch 30/500\n",
      "80/80 [==============================] - 0s 552us/step - loss: 0.4052 - accuracy: 0.8301\n",
      "Epoch 31/500\n",
      "80/80 [==============================] - 0s 554us/step - loss: 0.4033 - accuracy: 0.8307\n",
      "Epoch 32/500\n",
      "80/80 [==============================] - 0s 561us/step - loss: 0.4017 - accuracy: 0.8317\n",
      "Epoch 33/500\n",
      "80/80 [==============================] - 0s 554us/step - loss: 0.3998 - accuracy: 0.8341\n",
      "Epoch 34/500\n",
      "80/80 [==============================] - 0s 569us/step - loss: 0.3979 - accuracy: 0.8341\n",
      "Epoch 35/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.3963 - accuracy: 0.8349\n",
      "Epoch 36/500\n",
      "80/80 [==============================] - 0s 548us/step - loss: 0.3944 - accuracy: 0.8361\n",
      "Epoch 37/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.3927 - accuracy: 0.8379\n",
      "Epoch 38/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3911 - accuracy: 0.8365\n",
      "Epoch 39/500\n",
      "80/80 [==============================] - 0s 571us/step - loss: 0.3895 - accuracy: 0.8386\n",
      "Epoch 40/500\n",
      "80/80 [==============================] - 0s 547us/step - loss: 0.3879 - accuracy: 0.8397\n",
      "Epoch 41/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3864 - accuracy: 0.8409\n",
      "Epoch 42/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.3849 - accuracy: 0.8418\n",
      "Epoch 43/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3832 - accuracy: 0.8431\n",
      "Epoch 44/500\n",
      "80/80 [==============================] - 0s 548us/step - loss: 0.3819 - accuracy: 0.8430\n",
      "Epoch 45/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.3805 - accuracy: 0.8440\n",
      "Epoch 46/500\n",
      "80/80 [==============================] - 0s 553us/step - loss: 0.3791 - accuracy: 0.8446\n",
      "Epoch 47/500\n",
      "80/80 [==============================] - 0s 548us/step - loss: 0.3779 - accuracy: 0.8439\n",
      "Epoch 48/500\n",
      "80/80 [==============================] - 0s 561us/step - loss: 0.3765 - accuracy: 0.8459\n",
      "Epoch 49/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3756 - accuracy: 0.8468\n",
      "Epoch 50/500\n",
      "80/80 [==============================] - 0s 569us/step - loss: 0.3741 - accuracy: 0.8469\n",
      "Epoch 51/500\n",
      "80/80 [==============================] - 0s 571us/step - loss: 0.3730 - accuracy: 0.8486\n",
      "Epoch 52/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3719 - accuracy: 0.8480\n",
      "Epoch 53/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3709 - accuracy: 0.8487\n",
      "Epoch 54/500\n",
      "80/80 [==============================] - 0s 561us/step - loss: 0.3698 - accuracy: 0.8501\n",
      "Epoch 55/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.3691 - accuracy: 0.8510\n",
      "Epoch 56/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3678 - accuracy: 0.8512\n",
      "Epoch 57/500\n",
      "80/80 [==============================] - 0s 564us/step - loss: 0.3667 - accuracy: 0.8521\n",
      "Epoch 58/500\n",
      "80/80 [==============================] - 0s 567us/step - loss: 0.3661 - accuracy: 0.8520\n",
      "Epoch 59/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3650 - accuracy: 0.8540\n",
      "Epoch 60/500\n",
      "80/80 [==============================] - 0s 556us/step - loss: 0.3641 - accuracy: 0.8531\n",
      "Epoch 61/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.3635 - accuracy: 0.8553\n",
      "Epoch 62/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3626 - accuracy: 0.8551\n",
      "Epoch 63/500\n",
      "80/80 [==============================] - 0s 561us/step - loss: 0.3619 - accuracy: 0.8535\n",
      "Epoch 64/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.3611 - accuracy: 0.8545\n",
      "Epoch 65/500\n",
      "80/80 [==============================] - 0s 556us/step - loss: 0.3603 - accuracy: 0.8553\n",
      "Epoch 66/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3597 - accuracy: 0.8549\n",
      "Epoch 67/500\n",
      "80/80 [==============================] - 0s 561us/step - loss: 0.3590 - accuracy: 0.8565\n",
      "Epoch 68/500\n",
      "80/80 [==============================] - 0s 552us/step - loss: 0.3582 - accuracy: 0.8559\n",
      "Epoch 69/500\n",
      "80/80 [==============================] - 0s 552us/step - loss: 0.3576 - accuracy: 0.8558\n",
      "Epoch 70/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.3569 - accuracy: 0.8555\n",
      "Epoch 71/500\n",
      "80/80 [==============================] - 0s 547us/step - loss: 0.3562 - accuracy: 0.8554\n",
      "Epoch 72/500\n",
      "80/80 [==============================] - 0s 545us/step - loss: 0.3556 - accuracy: 0.8562\n",
      "Epoch 73/500\n",
      "80/80 [==============================] - 0s 569us/step - loss: 0.3551 - accuracy: 0.8564\n",
      "Epoch 74/500\n",
      "80/80 [==============================] - 0s 571us/step - loss: 0.3543 - accuracy: 0.8549\n",
      "Epoch 75/500\n",
      "80/80 [==============================] - 0s 573us/step - loss: 0.3538 - accuracy: 0.8564\n",
      "Epoch 76/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3534 - accuracy: 0.8555\n",
      "Epoch 77/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.3528 - accuracy: 0.8571\n",
      "Epoch 78/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.3522 - accuracy: 0.8569\n",
      "Epoch 79/500\n",
      "80/80 [==============================] - 0s 581us/step - loss: 0.3516 - accuracy: 0.8562\n",
      "Epoch 80/500\n",
      "80/80 [==============================] - 0s 561us/step - loss: 0.3512 - accuracy: 0.8561\n",
      "Epoch 81/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3506 - accuracy: 0.8566\n",
      "Epoch 82/500\n",
      "80/80 [==============================] - 0s 607us/step - loss: 0.3502 - accuracy: 0.8583\n",
      "Epoch 83/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.3498 - accuracy: 0.8579\n",
      "Epoch 84/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3492 - accuracy: 0.8575\n",
      "Epoch 85/500\n",
      "80/80 [==============================] - 0s 560us/step - loss: 0.3488 - accuracy: 0.8581\n",
      "Epoch 86/500\n",
      "80/80 [==============================] - 0s 559us/step - loss: 0.3483 - accuracy: 0.8584\n",
      "Epoch 87/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.3479 - accuracy: 0.8566\n",
      "Epoch 88/500\n",
      "80/80 [==============================] - 0s 555us/step - loss: 0.3476 - accuracy: 0.8580\n",
      "Epoch 89/500\n",
      "80/80 [==============================] - 0s 595us/step - loss: 0.3472 - accuracy: 0.8569\n",
      "Epoch 90/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.3466 - accuracy: 0.8594\n",
      "Epoch 91/500\n",
      "80/80 [==============================] - 0s 573us/step - loss: 0.3463 - accuracy: 0.8591\n",
      "Epoch 92/500\n",
      "80/80 [==============================] - 0s 545us/step - loss: 0.3464 - accuracy: 0.8580\n",
      "Epoch 93/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3456 - accuracy: 0.8585\n",
      "Epoch 94/500\n",
      "80/80 [==============================] - 0s 546us/step - loss: 0.3455 - accuracy: 0.8589\n",
      "Epoch 95/500\n",
      "80/80 [==============================] - 0s 548us/step - loss: 0.3448 - accuracy: 0.8580\n",
      "Epoch 96/500\n",
      "80/80 [==============================] - 0s 556us/step - loss: 0.3447 - accuracy: 0.8580\n",
      "Epoch 97/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.3445 - accuracy: 0.8591\n",
      "Epoch 98/500\n",
      "80/80 [==============================] - 0s 559us/step - loss: 0.3439 - accuracy: 0.8591\n",
      "Epoch 99/500\n",
      "80/80 [==============================] - 0s 573us/step - loss: 0.3439 - accuracy: 0.8586\n",
      "Epoch 100/500\n",
      "80/80 [==============================] - 0s 569us/step - loss: 0.3433 - accuracy: 0.8580\n",
      "Epoch 101/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3432 - accuracy: 0.8597\n",
      "Epoch 102/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3431 - accuracy: 0.8601\n",
      "Epoch 103/500\n",
      "80/80 [==============================] - 0s 595us/step - loss: 0.3426 - accuracy: 0.8593\n",
      "Epoch 104/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3423 - accuracy: 0.8587\n",
      "Epoch 105/500\n",
      "80/80 [==============================] - 0s 505us/step - loss: 0.3420 - accuracy: 0.8594\n",
      "Epoch 106/500\n",
      "80/80 [==============================] - 0s 686us/step - loss: 0.3421 - accuracy: 0.8596\n",
      "Epoch 107/500\n",
      "80/80 [==============================] - 0s 573us/step - loss: 0.3415 - accuracy: 0.8591\n",
      "Epoch 108/500\n",
      "80/80 [==============================] - 0s 627us/step - loss: 0.3414 - accuracy: 0.8595\n",
      "Epoch 109/500\n",
      "80/80 [==============================] - 0s 591us/step - loss: 0.3412 - accuracy: 0.8595\n",
      "Epoch 110/500\n",
      "80/80 [==============================] - 0s 593us/step - loss: 0.3407 - accuracy: 0.8587\n",
      "Epoch 111/500\n",
      "80/80 [==============================] - 0s 670us/step - loss: 0.3407 - accuracy: 0.8591\n",
      "Epoch 112/500\n",
      "80/80 [==============================] - 0s 604us/step - loss: 0.3406 - accuracy: 0.8595\n",
      "Epoch 113/500\n",
      "80/80 [==============================] - 0s 574us/step - loss: 0.3405 - accuracy: 0.8601\n",
      "Epoch 114/500\n",
      "80/80 [==============================] - 0s 556us/step - loss: 0.3403 - accuracy: 0.8595\n",
      "Epoch 115/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.3400 - accuracy: 0.8593\n",
      "Epoch 116/500\n",
      "80/80 [==============================] - 0s 544us/step - loss: 0.3397 - accuracy: 0.8595\n",
      "Epoch 117/500\n",
      "80/80 [==============================] - 0s 574us/step - loss: 0.3396 - accuracy: 0.8604\n",
      "Epoch 118/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3392 - accuracy: 0.8590\n",
      "Epoch 119/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3392 - accuracy: 0.8604\n",
      "Epoch 120/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3391 - accuracy: 0.8594\n",
      "Epoch 121/500\n",
      "80/80 [==============================] - 0s 561us/step - loss: 0.3389 - accuracy: 0.8597\n",
      "Epoch 122/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.3387 - accuracy: 0.8595\n",
      "Epoch 123/500\n",
      "80/80 [==============================] - 0s 582us/step - loss: 0.3384 - accuracy: 0.8596\n",
      "Epoch 124/500\n",
      "80/80 [==============================] - 0s 608us/step - loss: 0.3381 - accuracy: 0.8594\n",
      "Epoch 125/500\n",
      "80/80 [==============================] - 0s 721us/step - loss: 0.3381 - accuracy: 0.8602\n",
      "Epoch 126/500\n",
      "80/80 [==============================] - 0s 756us/step - loss: 0.3378 - accuracy: 0.8597\n",
      "Epoch 127/500\n",
      "80/80 [==============================] - 0s 595us/step - loss: 0.3379 - accuracy: 0.8599\n",
      "Epoch 128/500\n",
      "80/80 [==============================] - 0s 593us/step - loss: 0.3377 - accuracy: 0.8595\n",
      "Epoch 129/500\n",
      "80/80 [==============================] - 0s 609us/step - loss: 0.3376 - accuracy: 0.8600\n",
      "Epoch 130/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3375 - accuracy: 0.8608\n",
      "Epoch 131/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3373 - accuracy: 0.8589\n",
      "Epoch 132/500\n",
      "80/80 [==============================] - 0s 556us/step - loss: 0.3374 - accuracy: 0.8605\n",
      "Epoch 133/500\n",
      "80/80 [==============================] - 0s 569us/step - loss: 0.3369 - accuracy: 0.8595\n",
      "Epoch 134/500\n",
      "80/80 [==============================] - 0s 599us/step - loss: 0.3369 - accuracy: 0.8601\n",
      "Epoch 135/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3368 - accuracy: 0.8609\n",
      "Epoch 136/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3367 - accuracy: 0.8612\n",
      "Epoch 137/500\n",
      "80/80 [==============================] - 0s 595us/step - loss: 0.3365 - accuracy: 0.8608\n",
      "Epoch 138/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3363 - accuracy: 0.8604\n",
      "Epoch 139/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3362 - accuracy: 0.8608\n",
      "Epoch 140/500\n",
      "80/80 [==============================] - 0s 576us/step - loss: 0.3362 - accuracy: 0.8601\n",
      "Epoch 141/500\n",
      "80/80 [==============================] - 0s 575us/step - loss: 0.3359 - accuracy: 0.8600\n",
      "Epoch 142/500\n",
      "80/80 [==============================] - 0s 584us/step - loss: 0.3359 - accuracy: 0.8610\n",
      "Epoch 143/500\n",
      "80/80 [==============================] - 0s 582us/step - loss: 0.3357 - accuracy: 0.8606\n",
      "Epoch 144/500\n",
      "80/80 [==============================] - 0s 608us/step - loss: 0.3357 - accuracy: 0.8609\n",
      "Epoch 145/500\n",
      "80/80 [==============================] - 0s 490us/step - loss: 0.3355 - accuracy: 0.8614\n",
      "Epoch 146/500\n",
      "80/80 [==============================] - 0s 661us/step - loss: 0.3355 - accuracy: 0.8612\n",
      "Epoch 147/500\n",
      "80/80 [==============================] - 0s 614us/step - loss: 0.3356 - accuracy: 0.8609\n",
      "Epoch 148/500\n",
      "80/80 [==============================] - 0s 707us/step - loss: 0.3354 - accuracy: 0.8604\n",
      "Epoch 149/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3352 - accuracy: 0.8608\n",
      "Epoch 150/500\n",
      "80/80 [==============================] - 0s 595us/step - loss: 0.3351 - accuracy: 0.8602\n",
      "Epoch 151/500\n",
      "80/80 [==============================] - 0s 595us/step - loss: 0.3349 - accuracy: 0.8618\n",
      "Epoch 152/500\n",
      "80/80 [==============================] - 0s 595us/step - loss: 0.3349 - accuracy: 0.8602\n",
      "Epoch 153/500\n",
      "80/80 [==============================] - 0s 582us/step - loss: 0.3347 - accuracy: 0.8608\n",
      "Epoch 154/500\n",
      "80/80 [==============================] - 0s 584us/step - loss: 0.3348 - accuracy: 0.8622\n",
      "Epoch 155/500\n",
      "80/80 [==============================] - 0s 607us/step - loss: 0.3345 - accuracy: 0.8615\n",
      "Epoch 156/500\n",
      "80/80 [==============================] - 0s 595us/step - loss: 0.3348 - accuracy: 0.8612\n",
      "Epoch 157/500\n",
      "80/80 [==============================] - 0s 596us/step - loss: 0.3342 - accuracy: 0.8618\n",
      "Epoch 158/500\n",
      "80/80 [==============================] - 0s 596us/step - loss: 0.3343 - accuracy: 0.8606\n",
      "Epoch 159/500\n",
      "80/80 [==============================] - 0s 595us/step - loss: 0.3342 - accuracy: 0.8622\n",
      "Epoch 160/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 583us/step - loss: 0.3343 - accuracy: 0.8615\n",
      "Epoch 161/500\n",
      "80/80 [==============================] - 0s 598us/step - loss: 0.3341 - accuracy: 0.8610\n",
      "Epoch 162/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3340 - accuracy: 0.8609\n",
      "Epoch 163/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.3337 - accuracy: 0.8594\n",
      "Epoch 164/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3338 - accuracy: 0.8611\n",
      "Epoch 165/500\n",
      "80/80 [==============================] - 0s 596us/step - loss: 0.3336 - accuracy: 0.8612\n",
      "Epoch 166/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3336 - accuracy: 0.8611\n",
      "Epoch 167/500\n",
      "80/80 [==============================] - 0s 596us/step - loss: 0.3335 - accuracy: 0.8615\n",
      "Epoch 168/500\n",
      "80/80 [==============================] - 0s 582us/step - loss: 0.3334 - accuracy: 0.8615\n",
      "Epoch 169/500\n",
      "80/80 [==============================] - 0s 595us/step - loss: 0.3336 - accuracy: 0.8611\n",
      "Epoch 170/500\n",
      "80/80 [==============================] - 0s 595us/step - loss: 0.3333 - accuracy: 0.8608\n",
      "Epoch 171/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3332 - accuracy: 0.8610\n",
      "Epoch 172/500\n",
      "80/80 [==============================] - 0s 586us/step - loss: 0.3333 - accuracy: 0.8618\n",
      "Epoch 173/500\n",
      "80/80 [==============================] - 0s 595us/step - loss: 0.3330 - accuracy: 0.8609\n",
      "Epoch 174/500\n",
      "80/80 [==============================] - 0s 582us/step - loss: 0.3328 - accuracy: 0.8627\n",
      "Epoch 175/500\n",
      "80/80 [==============================] - 0s 582us/step - loss: 0.3329 - accuracy: 0.8618\n",
      "Epoch 176/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3327 - accuracy: 0.8610\n",
      "Epoch 177/500\n",
      "80/80 [==============================] - 0s 608us/step - loss: 0.3330 - accuracy: 0.8614\n",
      "Epoch 178/500\n",
      "80/80 [==============================] - 0s 595us/step - loss: 0.3328 - accuracy: 0.8625\n",
      "Epoch 179/500\n",
      "80/80 [==============================] - 0s 608us/step - loss: 0.3327 - accuracy: 0.8610\n",
      "Epoch 180/500\n",
      "80/80 [==============================] - 0s 582us/step - loss: 0.3328 - accuracy: 0.8614\n",
      "Epoch 181/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3326 - accuracy: 0.8616\n",
      "Epoch 182/500\n",
      "80/80 [==============================] - 0s 595us/step - loss: 0.3324 - accuracy: 0.8615\n",
      "Epoch 183/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3324 - accuracy: 0.8616\n",
      "Epoch 184/500\n",
      "80/80 [==============================] - 0s 573us/step - loss: 0.3323 - accuracy: 0.8630\n",
      "Epoch 185/500\n",
      "80/80 [==============================] - 0s 596us/step - loss: 0.3319 - accuracy: 0.8622\n",
      "Epoch 186/500\n",
      "80/80 [==============================] - 0s 582us/step - loss: 0.3320 - accuracy: 0.8608\n",
      "Epoch 187/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3321 - accuracy: 0.8627\n",
      "Epoch 188/500\n",
      "80/80 [==============================] - 0s 594us/step - loss: 0.3319 - accuracy: 0.8622\n",
      "Epoch 189/500\n",
      "80/80 [==============================] - 0s 596us/step - loss: 0.3318 - accuracy: 0.8635\n",
      "Epoch 190/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3318 - accuracy: 0.8624\n",
      "Epoch 191/500\n",
      "80/80 [==============================] - 0s 593us/step - loss: 0.3317 - accuracy: 0.8619\n",
      "Epoch 192/500\n",
      "80/80 [==============================] - 0s 608us/step - loss: 0.3315 - accuracy: 0.8616\n",
      "Epoch 193/500\n",
      "80/80 [==============================] - 0s 581us/step - loss: 0.3316 - accuracy: 0.8616\n",
      "Epoch 194/500\n",
      "80/80 [==============================] - 0s 624us/step - loss: 0.3317 - accuracy: 0.8622\n",
      "Epoch 195/500\n",
      "80/80 [==============================] - 0s 595us/step - loss: 0.3313 - accuracy: 0.8622\n",
      "Epoch 196/500\n",
      "80/80 [==============================] - 0s 594us/step - loss: 0.3314 - accuracy: 0.8631\n",
      "Epoch 197/500\n",
      "80/80 [==============================] - 0s 600us/step - loss: 0.3314 - accuracy: 0.8619\n",
      "Epoch 198/500\n",
      "80/80 [==============================] - 0s 582us/step - loss: 0.3312 - accuracy: 0.8631\n",
      "Epoch 199/500\n",
      "80/80 [==============================] - 0s 581us/step - loss: 0.3315 - accuracy: 0.8624\n",
      "Epoch 200/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3312 - accuracy: 0.8644\n",
      "Epoch 201/500\n",
      "80/80 [==============================] - 0s 582us/step - loss: 0.3312 - accuracy: 0.8630\n",
      "Epoch 202/500\n",
      "80/80 [==============================] - 0s 599us/step - loss: 0.3311 - accuracy: 0.8629\n",
      "Epoch 203/500\n",
      "80/80 [==============================] - 0s 567us/step - loss: 0.3309 - accuracy: 0.8640\n",
      "Epoch 204/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3309 - accuracy: 0.8631\n",
      "Epoch 205/500\n",
      "80/80 [==============================] - 0s 569us/step - loss: 0.3309 - accuracy: 0.8631\n",
      "Epoch 206/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3311 - accuracy: 0.8636\n",
      "Epoch 207/500\n",
      "80/80 [==============================] - 0s 582us/step - loss: 0.3308 - accuracy: 0.8635\n",
      "Epoch 208/500\n",
      "80/80 [==============================] - 0s 650us/step - loss: 0.3311 - accuracy: 0.8624\n",
      "Epoch 209/500\n",
      "80/80 [==============================] - 0s 591us/step - loss: 0.3306 - accuracy: 0.8631\n",
      "Epoch 210/500\n",
      "80/80 [==============================] - 0s 585us/step - loss: 0.3306 - accuracy: 0.8641\n",
      "Epoch 211/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3304 - accuracy: 0.8625\n",
      "Epoch 212/500\n",
      "80/80 [==============================] - 0s 608us/step - loss: 0.3306 - accuracy: 0.8641\n",
      "Epoch 213/500\n",
      "80/80 [==============================] - 0s 696us/step - loss: 0.3304 - accuracy: 0.8637\n",
      "Epoch 214/500\n",
      "80/80 [==============================] - 0s 762us/step - loss: 0.3304 - accuracy: 0.8643\n",
      "Epoch 215/500\n",
      "80/80 [==============================] - 0s 620us/step - loss: 0.3305 - accuracy: 0.8635\n",
      "Epoch 216/500\n",
      "80/80 [==============================] - 0s 693us/step - loss: 0.3303 - accuracy: 0.8635\n",
      "Epoch 217/500\n",
      "80/80 [==============================] - 0s 619us/step - loss: 0.3303 - accuracy: 0.8626\n",
      "Epoch 218/500\n",
      "80/80 [==============================] - 0s 646us/step - loss: 0.3300 - accuracy: 0.8637\n",
      "Epoch 219/500\n",
      "80/80 [==============================] - 0s 632us/step - loss: 0.3299 - accuracy: 0.8636\n",
      "Epoch 220/500\n",
      "80/80 [==============================] - 0s 620us/step - loss: 0.3300 - accuracy: 0.8643\n",
      "Epoch 221/500\n",
      "80/80 [==============================] - 0s 633us/step - loss: 0.3303 - accuracy: 0.8630\n",
      "Epoch 222/500\n",
      "80/80 [==============================] - 0s 648us/step - loss: 0.3298 - accuracy: 0.8646\n",
      "Epoch 223/500\n",
      "80/80 [==============================] - 0s 633us/step - loss: 0.3299 - accuracy: 0.8639\n",
      "Epoch 224/500\n",
      "80/80 [==============================] - 0s 646us/step - loss: 0.3298 - accuracy: 0.8645\n",
      "Epoch 225/500\n",
      "80/80 [==============================] - 0s 645us/step - loss: 0.3297 - accuracy: 0.8641\n",
      "Epoch 226/500\n",
      "80/80 [==============================] - 0s 620us/step - loss: 0.3295 - accuracy: 0.8635\n",
      "Epoch 227/500\n",
      "80/80 [==============================] - 0s 646us/step - loss: 0.3298 - accuracy: 0.8646\n",
      "Epoch 228/500\n",
      "80/80 [==============================] - 0s 623us/step - loss: 0.3295 - accuracy: 0.8637\n",
      "Epoch 229/500\n",
      "80/80 [==============================] - 0s 658us/step - loss: 0.3295 - accuracy: 0.8636\n",
      "Epoch 230/500\n",
      "80/80 [==============================] - 0s 620us/step - loss: 0.3295 - accuracy: 0.8635\n",
      "Epoch 231/500\n",
      "80/80 [==============================] - 0s 657us/step - loss: 0.3295 - accuracy: 0.8624\n",
      "Epoch 232/500\n",
      "80/80 [==============================] - 0s 648us/step - loss: 0.3293 - accuracy: 0.8643\n",
      "Epoch 233/500\n",
      "80/80 [==============================] - 0s 615us/step - loss: 0.3295 - accuracy: 0.8646\n",
      "Epoch 234/500\n",
      "80/80 [==============================] - 0s 621us/step - loss: 0.3293 - accuracy: 0.8661\n",
      "Epoch 235/500\n",
      "80/80 [==============================] - 0s 595us/step - loss: 0.3292 - accuracy: 0.8644\n",
      "Epoch 236/500\n",
      "80/80 [==============================] - 0s 582us/step - loss: 0.3291 - accuracy: 0.8654\n",
      "Epoch 237/500\n",
      "80/80 [==============================] - 0s 586us/step - loss: 0.3292 - accuracy: 0.8650\n",
      "Epoch 238/500\n",
      "80/80 [==============================] - 0s 607us/step - loss: 0.3292 - accuracy: 0.8637\n",
      "Epoch 239/500\n",
      "80/80 [==============================] - 0s 571us/step - loss: 0.3290 - accuracy: 0.8654\n",
      "Epoch 240/500\n",
      "80/80 [==============================] - 0s 582us/step - loss: 0.3290 - accuracy: 0.8652\n",
      "Epoch 241/500\n",
      "80/80 [==============================] - 0s 584us/step - loss: 0.3292 - accuracy: 0.8655\n",
      "Epoch 242/500\n",
      "80/80 [==============================] - 0s 569us/step - loss: 0.3288 - accuracy: 0.8643\n",
      "Epoch 243/500\n",
      "80/80 [==============================] - 0s 582us/step - loss: 0.3290 - accuracy: 0.8643\n",
      "Epoch 244/500\n",
      "80/80 [==============================] - 0s 582us/step - loss: 0.3290 - accuracy: 0.8639\n",
      "Epoch 245/500\n",
      "80/80 [==============================] - 0s 596us/step - loss: 0.3288 - accuracy: 0.8655\n",
      "Epoch 246/500\n",
      "80/80 [==============================] - 0s 595us/step - loss: 0.3289 - accuracy: 0.8639\n",
      "Epoch 247/500\n",
      "80/80 [==============================] - 0s 613us/step - loss: 0.3287 - accuracy: 0.8651\n",
      "Epoch 248/500\n",
      "80/80 [==============================] - 0s 580us/step - loss: 0.3290 - accuracy: 0.8652\n",
      "Epoch 249/500\n",
      "80/80 [==============================] - 0s 582us/step - loss: 0.3285 - accuracy: 0.8646\n",
      "Epoch 250/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.3288 - accuracy: 0.8643\n",
      "Epoch 251/500\n",
      "80/80 [==============================] - 0s 620us/step - loss: 0.3283 - accuracy: 0.8660\n",
      "Epoch 252/500\n",
      "80/80 [==============================] - 0s 608us/step - loss: 0.3287 - accuracy: 0.8648\n",
      "Epoch 253/500\n",
      "80/80 [==============================] - 0s 596us/step - loss: 0.3285 - accuracy: 0.8661\n",
      "Epoch 254/500\n",
      "80/80 [==============================] - 0s 607us/step - loss: 0.3284 - accuracy: 0.8655\n",
      "Epoch 255/500\n",
      "80/80 [==============================] - 0s 595us/step - loss: 0.3283 - accuracy: 0.8646\n",
      "Epoch 256/500\n",
      "80/80 [==============================] - 0s 659us/step - loss: 0.3282 - accuracy: 0.8639\n",
      "Epoch 257/500\n",
      "80/80 [==============================] - 0s 632us/step - loss: 0.3283 - accuracy: 0.8660\n",
      "Epoch 258/500\n",
      "80/80 [==============================] - 0s 645us/step - loss: 0.3281 - accuracy: 0.8658\n",
      "Epoch 259/500\n",
      "80/80 [==============================] - 0s 655us/step - loss: 0.3282 - accuracy: 0.8660\n",
      "Epoch 260/500\n",
      "80/80 [==============================] - 0s 634us/step - loss: 0.3283 - accuracy: 0.8652\n",
      "Epoch 261/500\n",
      "80/80 [==============================] - 0s 645us/step - loss: 0.3282 - accuracy: 0.8659\n",
      "Epoch 262/500\n",
      "80/80 [==============================] - 0s 658us/step - loss: 0.3280 - accuracy: 0.8656\n",
      "Epoch 263/500\n",
      "80/80 [==============================] - 0s 658us/step - loss: 0.3280 - accuracy: 0.8656\n",
      "Epoch 264/500\n",
      "80/80 [==============================] - 0s 660us/step - loss: 0.3279 - accuracy: 0.8651\n",
      "Epoch 265/500\n",
      "80/80 [==============================] - 0s 646us/step - loss: 0.3280 - accuracy: 0.8652\n",
      "Epoch 266/500\n",
      "80/80 [==============================] - 0s 658us/step - loss: 0.3279 - accuracy: 0.8654\n",
      "Epoch 267/500\n",
      "80/80 [==============================] - 0s 643us/step - loss: 0.3280 - accuracy: 0.8651\n",
      "Epoch 268/500\n",
      "80/80 [==============================] - 0s 620us/step - loss: 0.3280 - accuracy: 0.8661\n",
      "Epoch 269/500\n",
      "80/80 [==============================] - 0s 619us/step - loss: 0.3278 - accuracy: 0.8650\n",
      "Epoch 270/500\n",
      "80/80 [==============================] - 0s 684us/step - loss: 0.3279 - accuracy: 0.8662\n",
      "Epoch 271/500\n",
      "80/80 [==============================] - 0s 646us/step - loss: 0.3278 - accuracy: 0.8645\n",
      "Epoch 272/500\n",
      "80/80 [==============================] - 0s 654us/step - loss: 0.3278 - accuracy: 0.8656\n",
      "Epoch 273/500\n",
      "80/80 [==============================] - 0s 658us/step - loss: 0.3275 - accuracy: 0.8660\n",
      "Epoch 274/500\n",
      "80/80 [==============================] - 0s 594us/step - loss: 0.3278 - accuracy: 0.8651\n",
      "Epoch 275/500\n",
      "80/80 [==============================] - 0s 597us/step - loss: 0.3279 - accuracy: 0.8660\n",
      "Epoch 276/500\n",
      "80/80 [==============================] - 0s 607us/step - loss: 0.3277 - accuracy: 0.8654\n",
      "Epoch 277/500\n",
      "80/80 [==============================] - 0s 595us/step - loss: 0.3276 - accuracy: 0.8656\n",
      "Epoch 278/500\n",
      "80/80 [==============================] - 0s 582us/step - loss: 0.3276 - accuracy: 0.8656\n",
      "Epoch 279/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3275 - accuracy: 0.8649\n",
      "Epoch 280/500\n",
      "80/80 [==============================] - 0s 609us/step - loss: 0.3275 - accuracy: 0.8662\n",
      "Epoch 281/500\n",
      "80/80 [==============================] - 0s 594us/step - loss: 0.3275 - accuracy: 0.8658\n",
      "Epoch 282/500\n",
      "80/80 [==============================] - 0s 571us/step - loss: 0.3274 - accuracy: 0.8648\n",
      "Epoch 283/500\n",
      "80/80 [==============================] - 0s 596us/step - loss: 0.3276 - accuracy: 0.8650\n",
      "Epoch 284/500\n",
      "80/80 [==============================] - 0s 598us/step - loss: 0.3275 - accuracy: 0.8659\n",
      "Epoch 285/500\n",
      "80/80 [==============================] - 0s 596us/step - loss: 0.3274 - accuracy: 0.8658\n",
      "Epoch 286/500\n",
      "80/80 [==============================] - 0s 607us/step - loss: 0.3274 - accuracy: 0.8646\n",
      "Epoch 287/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3273 - accuracy: 0.8650\n",
      "Epoch 288/500\n",
      "80/80 [==============================] - 0s 595us/step - loss: 0.3271 - accuracy: 0.8650\n",
      "Epoch 289/500\n",
      "80/80 [==============================] - 0s 582us/step - loss: 0.3274 - accuracy: 0.8650\n",
      "Epoch 290/500\n",
      "80/80 [==============================] - 0s 595us/step - loss: 0.3271 - accuracy: 0.8659\n",
      "Epoch 291/500\n",
      "80/80 [==============================] - 0s 595us/step - loss: 0.3272 - accuracy: 0.8645\n",
      "Epoch 292/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3270 - accuracy: 0.8659\n",
      "Epoch 293/500\n",
      "80/80 [==============================] - 0s 582us/step - loss: 0.3271 - accuracy: 0.8661\n",
      "Epoch 294/500\n",
      "80/80 [==============================] - 0s 594us/step - loss: 0.3270 - accuracy: 0.8646\n",
      "Epoch 295/500\n",
      "80/80 [==============================] - 0s 596us/step - loss: 0.3272 - accuracy: 0.8662\n",
      "Epoch 296/500\n",
      "80/80 [==============================] - 0s 622us/step - loss: 0.3270 - accuracy: 0.8649\n",
      "Epoch 297/500\n",
      "80/80 [==============================] - 0s 593us/step - loss: 0.3270 - accuracy: 0.8659\n",
      "Epoch 298/500\n",
      "80/80 [==============================] - 0s 596us/step - loss: 0.3271 - accuracy: 0.8648\n",
      "Epoch 299/500\n",
      "80/80 [==============================] - 0s 607us/step - loss: 0.3268 - accuracy: 0.8658\n",
      "Epoch 300/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3269 - accuracy: 0.8660\n",
      "Epoch 301/500\n",
      "80/80 [==============================] - 0s 599us/step - loss: 0.3268 - accuracy: 0.8656\n",
      "Epoch 302/500\n",
      "80/80 [==============================] - 0s 581us/step - loss: 0.3267 - accuracy: 0.8661\n",
      "Epoch 303/500\n",
      "80/80 [==============================] - 0s 609us/step - loss: 0.3269 - accuracy: 0.8660\n",
      "Epoch 304/500\n",
      "80/80 [==============================] - 0s 594us/step - loss: 0.3268 - accuracy: 0.8656\n",
      "Epoch 305/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3268 - accuracy: 0.8652\n",
      "Epoch 306/500\n",
      "80/80 [==============================] - 0s 582us/step - loss: 0.3267 - accuracy: 0.8652\n",
      "Epoch 307/500\n",
      "80/80 [==============================] - 0s 595us/step - loss: 0.3267 - accuracy: 0.8659\n",
      "Epoch 308/500\n",
      "80/80 [==============================] - 0s 584us/step - loss: 0.3267 - accuracy: 0.8661\n",
      "Epoch 309/500\n",
      "80/80 [==============================] - 0s 582us/step - loss: 0.3268 - accuracy: 0.8665\n",
      "Epoch 310/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3266 - accuracy: 0.8651\n",
      "Epoch 311/500\n",
      "80/80 [==============================] - 0s 608us/step - loss: 0.3266 - accuracy: 0.8652\n",
      "Epoch 312/500\n",
      "80/80 [==============================] - 0s 595us/step - loss: 0.3267 - accuracy: 0.8670\n",
      "Epoch 313/500\n",
      "80/80 [==============================] - 0s 595us/step - loss: 0.3264 - accuracy: 0.8654\n",
      "Epoch 314/500\n",
      "80/80 [==============================] - 0s 595us/step - loss: 0.3266 - accuracy: 0.8658\n",
      "Epoch 315/500\n",
      "80/80 [==============================] - 0s 599us/step - loss: 0.3268 - accuracy: 0.8676\n",
      "Epoch 316/500\n",
      "80/80 [==============================] - 0s 582us/step - loss: 0.3266 - accuracy: 0.8654\n",
      "Epoch 317/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3265 - accuracy: 0.8651\n",
      "Epoch 318/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 594us/step - loss: 0.3264 - accuracy: 0.8651\n",
      "Epoch 319/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3265 - accuracy: 0.8646\n",
      "Epoch 320/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3265 - accuracy: 0.8664\n",
      "Epoch 321/500\n",
      "80/80 [==============================] - 0s 595us/step - loss: 0.3264 - accuracy: 0.8664\n",
      "Epoch 322/500\n",
      "80/80 [==============================] - 0s 582us/step - loss: 0.3265 - accuracy: 0.8650\n",
      "Epoch 323/500\n",
      "80/80 [==============================] - 0s 595us/step - loss: 0.3266 - accuracy: 0.8668\n",
      "Epoch 324/500\n",
      "80/80 [==============================] - 0s 608us/step - loss: 0.3263 - accuracy: 0.8666\n",
      "Epoch 325/500\n",
      "80/80 [==============================] - 0s 620us/step - loss: 0.3265 - accuracy: 0.8669\n",
      "Epoch 326/500\n",
      "80/80 [==============================] - 0s 596us/step - loss: 0.3264 - accuracy: 0.8649\n",
      "Epoch 327/500\n",
      "80/80 [==============================] - 0s 599us/step - loss: 0.3265 - accuracy: 0.8659\n",
      "Epoch 328/500\n",
      "80/80 [==============================] - 0s 622us/step - loss: 0.3264 - accuracy: 0.8654\n",
      "Epoch 329/500\n",
      "80/80 [==============================] - 0s 614us/step - loss: 0.3263 - accuracy: 0.8649\n",
      "Epoch 330/500\n",
      "80/80 [==============================] - 0s 609us/step - loss: 0.3263 - accuracy: 0.8655\n",
      "Epoch 331/500\n",
      "80/80 [==============================] - 0s 594us/step - loss: 0.3263 - accuracy: 0.8651\n",
      "Epoch 332/500\n",
      "80/80 [==============================] - 0s 621us/step - loss: 0.3263 - accuracy: 0.8658\n",
      "Epoch 333/500\n",
      "80/80 [==============================] - 0s 582us/step - loss: 0.3264 - accuracy: 0.8658\n",
      "Epoch 334/500\n",
      "80/80 [==============================] - 0s 596us/step - loss: 0.3261 - accuracy: 0.8666\n",
      "Epoch 335/500\n",
      "80/80 [==============================] - 0s 598us/step - loss: 0.3263 - accuracy: 0.8649\n",
      "Epoch 336/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3260 - accuracy: 0.8656\n",
      "Epoch 337/500\n",
      "80/80 [==============================] - 0s 595us/step - loss: 0.3260 - accuracy: 0.8661\n",
      "Epoch 338/500\n",
      "80/80 [==============================] - 0s 582us/step - loss: 0.3263 - accuracy: 0.8650\n",
      "Epoch 339/500\n",
      "80/80 [==============================] - 0s 581us/step - loss: 0.3258 - accuracy: 0.8655\n",
      "Epoch 340/500\n",
      "80/80 [==============================] - 0s 584us/step - loss: 0.3262 - accuracy: 0.8658\n",
      "Epoch 341/500\n",
      "80/80 [==============================] - 0s 582us/step - loss: 0.3260 - accuracy: 0.8674\n",
      "Epoch 342/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3258 - accuracy: 0.8660\n",
      "Epoch 343/500\n",
      "80/80 [==============================] - 0s 584us/step - loss: 0.3260 - accuracy: 0.8654\n",
      "Epoch 344/500\n",
      "80/80 [==============================] - 0s 594us/step - loss: 0.3258 - accuracy: 0.8662\n",
      "Epoch 345/500\n",
      "80/80 [==============================] - 0s 609us/step - loss: 0.3261 - accuracy: 0.8656\n",
      "Epoch 346/500\n",
      "80/80 [==============================] - 0s 606us/step - loss: 0.3258 - accuracy: 0.8652\n",
      "Epoch 347/500\n",
      "80/80 [==============================] - 0s 620us/step - loss: 0.3257 - accuracy: 0.8655\n",
      "Epoch 348/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3261 - accuracy: 0.8662\n",
      "Epoch 349/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3258 - accuracy: 0.8665\n",
      "Epoch 350/500\n",
      "80/80 [==============================] - 0s 586us/step - loss: 0.3258 - accuracy: 0.8660\n",
      "Epoch 351/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3259 - accuracy: 0.8662\n",
      "Epoch 352/500\n",
      "80/80 [==============================] - 0s 608us/step - loss: 0.3259 - accuracy: 0.8674\n",
      "Epoch 353/500\n",
      "80/80 [==============================] - 0s 582us/step - loss: 0.3257 - accuracy: 0.8664\n",
      "Epoch 354/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3259 - accuracy: 0.8658\n",
      "Epoch 355/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3260 - accuracy: 0.8669\n",
      "Epoch 356/500\n",
      "80/80 [==============================] - 0s 581us/step - loss: 0.3257 - accuracy: 0.8656\n",
      "Epoch 357/500\n",
      "80/80 [==============================] - 0s 596us/step - loss: 0.3257 - accuracy: 0.8655\n",
      "Epoch 358/500\n",
      "80/80 [==============================] - 0s 582us/step - loss: 0.3258 - accuracy: 0.8660\n",
      "Epoch 359/500\n",
      "80/80 [==============================] - 0s 596us/step - loss: 0.3257 - accuracy: 0.8650\n",
      "Epoch 360/500\n",
      "80/80 [==============================] - 0s 595us/step - loss: 0.3257 - accuracy: 0.8658\n",
      "Epoch 361/500\n",
      "80/80 [==============================] - 0s 594us/step - loss: 0.3256 - accuracy: 0.8673\n",
      "Epoch 362/500\n",
      "80/80 [==============================] - 0s 599us/step - loss: 0.3259 - accuracy: 0.8659\n",
      "Epoch 363/500\n",
      "80/80 [==============================] - 0s 582us/step - loss: 0.3254 - accuracy: 0.8661\n",
      "Epoch 364/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3257 - accuracy: 0.8662\n",
      "Epoch 365/500\n",
      "80/80 [==============================] - 0s 595us/step - loss: 0.3257 - accuracy: 0.8656\n",
      "Epoch 366/500\n",
      "80/80 [==============================] - 0s 596us/step - loss: 0.3255 - accuracy: 0.8661\n",
      "Epoch 367/500\n",
      "80/80 [==============================] - 0s 582us/step - loss: 0.3254 - accuracy: 0.8669\n",
      "Epoch 368/500\n",
      "80/80 [==============================] - 0s 633us/step - loss: 0.3254 - accuracy: 0.8655\n",
      "Epoch 369/500\n",
      "80/80 [==============================] - 0s 594us/step - loss: 0.3255 - accuracy: 0.8658\n",
      "Epoch 370/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3254 - accuracy: 0.8670\n",
      "Epoch 371/500\n",
      "80/80 [==============================] - 0s 582us/step - loss: 0.3254 - accuracy: 0.8654\n",
      "Epoch 372/500\n",
      "80/80 [==============================] - 0s 594us/step - loss: 0.3256 - accuracy: 0.8652\n",
      "Epoch 373/500\n",
      "80/80 [==============================] - 0s 608us/step - loss: 0.3254 - accuracy: 0.8656\n",
      "Epoch 374/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3254 - accuracy: 0.8660\n",
      "Epoch 375/500\n",
      "80/80 [==============================] - 0s 595us/step - loss: 0.3252 - accuracy: 0.8662\n",
      "Epoch 376/500\n",
      "80/80 [==============================] - 0s 582us/step - loss: 0.3256 - accuracy: 0.8665\n",
      "Epoch 377/500\n",
      "80/80 [==============================] - 0s 595us/step - loss: 0.3254 - accuracy: 0.8654\n",
      "Epoch 378/500\n",
      "80/80 [==============================] - 0s 587us/step - loss: 0.3253 - accuracy: 0.8659\n",
      "Epoch 379/500\n",
      "80/80 [==============================] - 0s 607us/step - loss: 0.3254 - accuracy: 0.8671\n",
      "Epoch 380/500\n",
      "80/80 [==============================] - 0s 595us/step - loss: 0.3254 - accuracy: 0.8675\n",
      "Epoch 381/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3253 - accuracy: 0.8659\n",
      "Epoch 382/500\n",
      "80/80 [==============================] - 0s 594us/step - loss: 0.3252 - accuracy: 0.8668\n",
      "Epoch 383/500\n",
      "80/80 [==============================] - 0s 596us/step - loss: 0.3253 - accuracy: 0.8662\n",
      "Epoch 384/500\n",
      "80/80 [==============================] - 0s 582us/step - loss: 0.3254 - accuracy: 0.8670\n",
      "Epoch 385/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.3253 - accuracy: 0.8649\n",
      "Epoch 386/500\n",
      "80/80 [==============================] - 0s 596us/step - loss: 0.3253 - accuracy: 0.8662\n",
      "Epoch 387/500\n",
      "80/80 [==============================] - 0s 584us/step - loss: 0.3251 - accuracy: 0.8671\n",
      "Epoch 388/500\n",
      "80/80 [==============================] - 0s 593us/step - loss: 0.3252 - accuracy: 0.8651\n",
      "Epoch 389/500\n",
      "80/80 [==============================] - 0s 599us/step - loss: 0.3254 - accuracy: 0.8659\n",
      "Epoch 390/500\n",
      "80/80 [==============================] - 0s 585us/step - loss: 0.3253 - accuracy: 0.8658\n",
      "Epoch 391/500\n",
      "80/80 [==============================] - 0s 581us/step - loss: 0.3251 - accuracy: 0.8668\n",
      "Epoch 392/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3251 - accuracy: 0.8662\n",
      "Epoch 393/500\n",
      "80/80 [==============================] - 0s 608us/step - loss: 0.3250 - accuracy: 0.8668\n",
      "Epoch 394/500\n",
      "80/80 [==============================] - 0s 582us/step - loss: 0.3250 - accuracy: 0.8666\n",
      "Epoch 395/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3250 - accuracy: 0.8671\n",
      "Epoch 396/500\n",
      "80/80 [==============================] - 0s 582us/step - loss: 0.3250 - accuracy: 0.8666\n",
      "Epoch 397/500\n",
      "80/80 [==============================] - 0s 582us/step - loss: 0.3250 - accuracy: 0.8673\n",
      "Epoch 398/500\n",
      "80/80 [==============================] - 0s 596us/step - loss: 0.3253 - accuracy: 0.8646\n",
      "Epoch 399/500\n",
      "80/80 [==============================] - 0s 599us/step - loss: 0.3251 - accuracy: 0.8671\n",
      "Epoch 400/500\n",
      "80/80 [==============================] - 0s 595us/step - loss: 0.3251 - accuracy: 0.8664\n",
      "Epoch 401/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3250 - accuracy: 0.8660\n",
      "Epoch 402/500\n",
      "80/80 [==============================] - 0s 595us/step - loss: 0.3248 - accuracy: 0.8665\n",
      "Epoch 403/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3251 - accuracy: 0.8668\n",
      "Epoch 404/500\n",
      "80/80 [==============================] - 0s 596us/step - loss: 0.3249 - accuracy: 0.8665\n",
      "Epoch 405/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3248 - accuracy: 0.8670\n",
      "Epoch 406/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3249 - accuracy: 0.8655\n",
      "Epoch 407/500\n",
      "80/80 [==============================] - 0s 595us/step - loss: 0.3248 - accuracy: 0.8669\n",
      "Epoch 408/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.3248 - accuracy: 0.8661\n",
      "Epoch 409/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.3250 - accuracy: 0.8661\n",
      "Epoch 410/500\n",
      "80/80 [==============================] - 0s 608us/step - loss: 0.3249 - accuracy: 0.8669\n",
      "Epoch 411/500\n",
      "80/80 [==============================] - 0s 596us/step - loss: 0.3248 - accuracy: 0.8668\n",
      "Epoch 412/500\n",
      "80/80 [==============================] - 0s 599us/step - loss: 0.3247 - accuracy: 0.8658\n",
      "Epoch 413/500\n",
      "80/80 [==============================] - 0s 595us/step - loss: 0.3248 - accuracy: 0.8668\n",
      "Epoch 414/500\n",
      "80/80 [==============================] - 0s 595us/step - loss: 0.3249 - accuracy: 0.8671\n",
      "Epoch 415/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3248 - accuracy: 0.8659\n",
      "Epoch 416/500\n",
      "80/80 [==============================] - 0s 594us/step - loss: 0.3249 - accuracy: 0.8668\n",
      "Epoch 417/500\n",
      "80/80 [==============================] - 0s 595us/step - loss: 0.3248 - accuracy: 0.8668\n",
      "Epoch 418/500\n",
      "80/80 [==============================] - 0s 596us/step - loss: 0.3247 - accuracy: 0.8664\n",
      "Epoch 419/500\n",
      "80/80 [==============================] - 0s 582us/step - loss: 0.3248 - accuracy: 0.8665\n",
      "Epoch 420/500\n",
      "80/80 [==============================] - 0s 582us/step - loss: 0.3248 - accuracy: 0.8659\n",
      "Epoch 421/500\n",
      "80/80 [==============================] - 0s 595us/step - loss: 0.3248 - accuracy: 0.8674\n",
      "Epoch 422/500\n",
      "80/80 [==============================] - 0s 584us/step - loss: 0.3246 - accuracy: 0.8673\n",
      "Epoch 423/500\n",
      "80/80 [==============================] - 0s 582us/step - loss: 0.3249 - accuracy: 0.8661\n",
      "Epoch 424/500\n",
      "80/80 [==============================] - 0s 597us/step - loss: 0.3247 - accuracy: 0.8660\n",
      "Epoch 425/500\n",
      "80/80 [==============================] - 0s 599us/step - loss: 0.3245 - accuracy: 0.8675\n",
      "Epoch 426/500\n",
      "80/80 [==============================] - 0s 594us/step - loss: 0.3246 - accuracy: 0.8677\n",
      "Epoch 427/500\n",
      "80/80 [==============================] - 0s 596us/step - loss: 0.3245 - accuracy: 0.8664\n",
      "Epoch 428/500\n",
      "80/80 [==============================] - 0s 594us/step - loss: 0.3246 - accuracy: 0.8675\n",
      "Epoch 429/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3245 - accuracy: 0.8671\n",
      "Epoch 430/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3246 - accuracy: 0.8664\n",
      "Epoch 431/500\n",
      "80/80 [==============================] - 0s 595us/step - loss: 0.3244 - accuracy: 0.8681\n",
      "Epoch 432/500\n",
      "80/80 [==============================] - 0s 582us/step - loss: 0.3244 - accuracy: 0.8674\n",
      "Epoch 433/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3244 - accuracy: 0.8671\n",
      "Epoch 434/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3246 - accuracy: 0.8664\n",
      "Epoch 435/500\n",
      "80/80 [==============================] - 0s 596us/step - loss: 0.3244 - accuracy: 0.8679\n",
      "Epoch 436/500\n",
      "80/80 [==============================] - 0s 594us/step - loss: 0.3243 - accuracy: 0.8673\n",
      "Epoch 437/500\n",
      "80/80 [==============================] - 0s 586us/step - loss: 0.3245 - accuracy: 0.8660\n",
      "Epoch 438/500\n",
      "80/80 [==============================] - 0s 584us/step - loss: 0.3245 - accuracy: 0.8671\n",
      "Epoch 439/500\n",
      "80/80 [==============================] - 0s 592us/step - loss: 0.3243 - accuracy: 0.8675\n",
      "Epoch 440/500\n",
      "80/80 [==============================] - 0s 595us/step - loss: 0.3245 - accuracy: 0.8673\n",
      "Epoch 441/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3245 - accuracy: 0.8677\n",
      "Epoch 442/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.3242 - accuracy: 0.8665\n",
      "Epoch 443/500\n",
      "80/80 [==============================] - 0s 582us/step - loss: 0.3249 - accuracy: 0.8671\n",
      "Epoch 444/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3244 - accuracy: 0.8674\n",
      "Epoch 445/500\n",
      "80/80 [==============================] - 0s 581us/step - loss: 0.3243 - accuracy: 0.8664\n",
      "Epoch 446/500\n",
      "80/80 [==============================] - 0s 596us/step - loss: 0.3240 - accuracy: 0.8679\n",
      "Epoch 447/500\n",
      "80/80 [==============================] - 0s 586us/step - loss: 0.3245 - accuracy: 0.8674\n",
      "Epoch 448/500\n",
      "80/80 [==============================] - 0s 582us/step - loss: 0.3244 - accuracy: 0.8675\n",
      "Epoch 449/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3243 - accuracy: 0.8670\n",
      "Epoch 450/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3242 - accuracy: 0.8669\n",
      "Epoch 451/500\n",
      "80/80 [==============================] - 0s 608us/step - loss: 0.3241 - accuracy: 0.8669\n",
      "Epoch 452/500\n",
      "80/80 [==============================] - 0s 582us/step - loss: 0.3245 - accuracy: 0.8674\n",
      "Epoch 453/500\n",
      "80/80 [==============================] - 0s 582us/step - loss: 0.3244 - accuracy: 0.8669\n",
      "Epoch 454/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3242 - accuracy: 0.8669\n",
      "Epoch 455/500\n",
      "80/80 [==============================] - 0s 608us/step - loss: 0.3244 - accuracy: 0.8668\n",
      "Epoch 456/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3242 - accuracy: 0.8670\n",
      "Epoch 457/500\n",
      "80/80 [==============================] - 0s 595us/step - loss: 0.3242 - accuracy: 0.8666\n",
      "Epoch 458/500\n",
      "80/80 [==============================] - 0s 595us/step - loss: 0.3242 - accuracy: 0.8659\n",
      "Epoch 459/500\n",
      "80/80 [==============================] - 0s 596us/step - loss: 0.3240 - accuracy: 0.8673\n",
      "Epoch 460/500\n",
      "80/80 [==============================] - 0s 599us/step - loss: 0.3241 - accuracy: 0.8675\n",
      "Epoch 461/500\n",
      "80/80 [==============================] - 0s 606us/step - loss: 0.3243 - accuracy: 0.8676\n",
      "Epoch 462/500\n",
      "80/80 [==============================] - 0s 582us/step - loss: 0.3240 - accuracy: 0.8679\n",
      "Epoch 463/500\n",
      "80/80 [==============================] - 0s 582us/step - loss: 0.3241 - accuracy: 0.8683\n",
      "Epoch 464/500\n",
      "80/80 [==============================] - 0s 584us/step - loss: 0.3242 - accuracy: 0.8662\n",
      "Epoch 465/500\n",
      "80/80 [==============================] - 0s 596us/step - loss: 0.3243 - accuracy: 0.8686\n",
      "Epoch 466/500\n",
      "80/80 [==============================] - 0s 569us/step - loss: 0.3241 - accuracy: 0.8676\n",
      "Epoch 467/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3241 - accuracy: 0.8680\n",
      "Epoch 468/500\n",
      "80/80 [==============================] - 0s 581us/step - loss: 0.3242 - accuracy: 0.8664\n",
      "Epoch 469/500\n",
      "80/80 [==============================] - 0s 595us/step - loss: 0.3240 - accuracy: 0.8677\n",
      "Epoch 470/500\n",
      "80/80 [==============================] - 0s 607us/step - loss: 0.3244 - accuracy: 0.8662\n",
      "Epoch 471/500\n",
      "80/80 [==============================] - 0s 596us/step - loss: 0.3240 - accuracy: 0.8666\n",
      "Epoch 472/500\n",
      "80/80 [==============================] - 0s 596us/step - loss: 0.3242 - accuracy: 0.8675\n",
      "Epoch 473/500\n",
      "80/80 [==============================] - 0s 584us/step - loss: 0.3241 - accuracy: 0.8680\n",
      "Epoch 474/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3240 - accuracy: 0.8677\n",
      "Epoch 475/500\n",
      "80/80 [==============================] - 0s 581us/step - loss: 0.3239 - accuracy: 0.8683\n",
      "Epoch 476/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 596us/step - loss: 0.3239 - accuracy: 0.8676\n",
      "Epoch 477/500\n",
      "80/80 [==============================] - 0s 582us/step - loss: 0.3240 - accuracy: 0.8683\n",
      "Epoch 478/500\n",
      "80/80 [==============================] - 0s 595us/step - loss: 0.3238 - accuracy: 0.8684\n",
      "Epoch 479/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.3240 - accuracy: 0.8683\n",
      "Epoch 480/500\n",
      "80/80 [==============================] - 0s 607us/step - loss: 0.3241 - accuracy: 0.8671\n",
      "Epoch 481/500\n",
      "80/80 [==============================] - 0s 595us/step - loss: 0.3239 - accuracy: 0.8679\n",
      "Epoch 482/500\n",
      "80/80 [==============================] - 0s 611us/step - loss: 0.3239 - accuracy: 0.8676\n",
      "Epoch 483/500\n",
      "80/80 [==============================] - 0s 605us/step - loss: 0.3240 - accuracy: 0.8677\n",
      "Epoch 484/500\n",
      "80/80 [==============================] - 0s 582us/step - loss: 0.3239 - accuracy: 0.8680\n",
      "Epoch 485/500\n",
      "80/80 [==============================] - 0s 587us/step - loss: 0.3236 - accuracy: 0.8679\n",
      "Epoch 486/500\n",
      "80/80 [==============================] - 0s 571us/step - loss: 0.3239 - accuracy: 0.8685\n",
      "Epoch 487/500\n",
      "80/80 [==============================] - 0s 594us/step - loss: 0.3238 - accuracy: 0.8671\n",
      "Epoch 488/500\n",
      "80/80 [==============================] - 0s 595us/step - loss: 0.3239 - accuracy: 0.8683\n",
      "Epoch 489/500\n",
      "80/80 [==============================] - 0s 595us/step - loss: 0.3238 - accuracy: 0.8675\n",
      "Epoch 490/500\n",
      "80/80 [==============================] - 0s 582us/step - loss: 0.3240 - accuracy: 0.8680\n",
      "Epoch 491/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3237 - accuracy: 0.8687\n",
      "Epoch 492/500\n",
      "80/80 [==============================] - 0s 608us/step - loss: 0.3239 - accuracy: 0.8675\n",
      "Epoch 493/500\n",
      "80/80 [==============================] - 0s 594us/step - loss: 0.3242 - accuracy: 0.8673\n",
      "Epoch 494/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3238 - accuracy: 0.8683\n",
      "Epoch 495/500\n",
      "80/80 [==============================] - 0s 600us/step - loss: 0.3236 - accuracy: 0.8684\n",
      "Epoch 496/500\n",
      "80/80 [==============================] - 0s 594us/step - loss: 0.3236 - accuracy: 0.8677\n",
      "Epoch 497/500\n",
      "80/80 [==============================] - 0s 595us/step - loss: 0.3237 - accuracy: 0.8681\n",
      "Epoch 498/500\n",
      "80/80 [==============================] - 0s 595us/step - loss: 0.3237 - accuracy: 0.8673\n",
      "Epoch 499/500\n",
      "80/80 [==============================] - 0s 595us/step - loss: 0.3237 - accuracy: 0.8690\n",
      "Epoch 500/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.3237 - accuracy: 0.8679\n"
     ]
    }
   ],
   "source": [
    "#### Last step in creation of NNmodel. NNmodel is trained on the training set here with Tensor-Keras .fit based on Compiler\n",
    "#Fitting NNmodel\n",
    "history=NNmodel.fit(X_train,Y_train,batch_size=100,epochs = 500)\n",
    "### Note that tf.keras.models.Sequential() by default uses glorot initializer -- drawing intial weights from a uniform \n",
    "### distribution -- see other possibilities in https://keras.io/api/layers/initializers/\n",
    "### Or you could try own customized wts inputs using\n",
    "### for layer in model.layers:\n",
    "###    init_layer_weight = [] # the weights yourself in this layer\n",
    "###    layer.set_weights(init_layer_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73398e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (100, 8)                  104       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (100, 8)                  72        \n",
      "                                                                 \n",
      " dense_2 (Dense)             (100, 1)                  9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 185\n",
      "Trainable params: 185\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NNmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3afe099c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.7993749976158142, 0.8073750138282776, 0.8109999895095825, 0.8130000233650208, 0.8116250038146973, 0.812874972820282, 0.8136249780654907, 0.8147500157356262, 0.815750002861023, 0.8153749704360962, 0.8173750042915344, 0.8199999928474426, 0.8208749890327454, 0.8245000243186951, 0.8263750076293945, 0.8261250257492065, 0.8295000195503235, 0.8301249742507935, 0.8307499885559082, 0.8317499756813049, 0.8341249823570251, 0.8341249823570251, 0.8348749876022339, 0.8361250162124634, 0.8378750085830688, 0.8364999890327454, 0.8386250138282776, 0.8397499918937683, 0.8408750295639038, 0.8417500257492065, 0.8431249856948853, 0.8429999947547913, 0.843999981880188, 0.8446249961853027, 0.843874990940094, 0.8458750247955322, 0.846750020980835, 0.846875011920929, 0.8486250042915344, 0.8479999899864197, 0.8487499952316284, 0.8501250147819519, 0.8510000109672546, 0.8512499928474426, 0.8521249890327454, 0.8519999980926514, 0.8539999723434448, 0.8531249761581421, 0.8552500009536743, 0.8551250100135803, 0.8535000085830688, 0.8544999957084656, 0.8552500009536743, 0.8548750281333923, 0.8565000295639038, 0.8558750152587891, 0.8557500243186951, 0.8554999828338623, 0.8553749918937683, 0.856249988079071, 0.856374979019165, 0.8548750281333923, 0.856374979019165, 0.8554999828338623, 0.8571249842643738, 0.8568750023841858, 0.856249988079071, 0.856124997138977, 0.8566250205039978, 0.8582500219345093, 0.8578749895095825, 0.8575000166893005, 0.8581249713897705, 0.8583750128746033, 0.8566250205039978, 0.8579999804496765, 0.8568750023841858, 0.859375, 0.859125018119812, 0.8579999804496765, 0.8585000038146973, 0.8588749766349792, 0.8579999804496765, 0.8579999804496765, 0.859125018119812, 0.859125018119812, 0.8586249947547913, 0.8579999804496765, 0.859749972820282, 0.8601250052452087, 0.859250009059906, 0.8587499856948853, 0.859375, 0.859624981880188, 0.859125018119812, 0.859499990940094, 0.859499990940094, 0.8587499856948853, 0.859125018119812, 0.859499990940094, 0.8601250052452087, 0.859499990940094, 0.859250009059906, 0.859499990940094, 0.8603749871253967, 0.859000027179718, 0.8603749871253967, 0.859375, 0.859749972820282, 0.859499990940094, 0.859624981880188, 0.859375, 0.8602499961853027, 0.859749972820282, 0.8598750233650208, 0.859499990940094, 0.8600000143051147, 0.8607500195503235, 0.8588749766349792, 0.8604999780654907, 0.859499990940094, 0.8601250052452087, 0.8608750104904175, 0.8612499833106995, 0.8607500195503235, 0.8603749871253967, 0.8607500195503235, 0.8601250052452087, 0.8600000143051147, 0.8610000014305115, 0.8606250286102295, 0.8608750104904175, 0.8613749742507935, 0.8612499833106995, 0.8608750104904175, 0.8603749871253967, 0.8607500195503235, 0.8602499961853027, 0.8617500066757202, 0.8602499961853027, 0.8607500195503235, 0.8622499704360962, 0.8615000247955322, 0.8612499833106995, 0.8617500066757202, 0.8606250286102295, 0.8622499704360962, 0.8615000247955322, 0.8610000014305115, 0.8608750104904175, 0.859375, 0.8611249923706055, 0.8612499833106995, 0.8611249923706055, 0.8615000247955322, 0.8615000247955322, 0.8611249923706055, 0.8607500195503235, 0.8610000014305115, 0.8617500066757202, 0.8608750104904175, 0.8627499938011169, 0.8617500066757202, 0.8610000014305115, 0.8613749742507935, 0.862500011920929, 0.8610000014305115, 0.8613749742507935, 0.8616250157356262, 0.8615000247955322, 0.8616250157356262, 0.8629999756813049, 0.8622499704360962, 0.8607500195503235, 0.8627499938011169, 0.8622499704360962, 0.8634999990463257, 0.862375020980835, 0.8618749976158142, 0.8616250157356262, 0.8616250157356262, 0.8622499704360962, 0.8622499704360962, 0.8631250262260437, 0.8618749976158142, 0.8631250262260437, 0.862375020980835, 0.8643749952316284, 0.8629999756813049, 0.8628749847412109, 0.8640000224113464, 0.8631250262260437, 0.8631250262260437, 0.8636249899864197, 0.8634999990463257, 0.862375020980835, 0.8631250262260437, 0.8641250133514404, 0.862500011920929, 0.8641250133514404, 0.8637499809265137, 0.8642500042915344, 0.8634999990463257, 0.8634999990463257, 0.862625002861023, 0.8637499809265137, 0.8636249899864197, 0.8642500042915344, 0.8629999756813049, 0.8646249771118164, 0.8638749718666077, 0.8644999861717224, 0.8641250133514404, 0.8634999990463257, 0.8646249771118164, 0.8637499809265137, 0.8636249899864197, 0.8634999990463257, 0.862375020980835, 0.8642500042915344, 0.8646249771118164, 0.8661249876022339, 0.8643749952316284, 0.8653749823570251, 0.8650000095367432, 0.8637499809265137, 0.8653749823570251, 0.8652499914169312, 0.8654999732971191, 0.8642500042915344, 0.8642500042915344, 0.8638749718666077, 0.8654999732971191, 0.8638749718666077, 0.8651250004768372, 0.8652499914169312, 0.8646249771118164, 0.8642500042915344, 0.8659999966621399, 0.8647500276565552, 0.8661249876022339, 0.8654999732971191, 0.8646249771118164, 0.8638749718666077, 0.8659999966621399, 0.8657500147819519, 0.8659999966621399, 0.8652499914169312, 0.8658750057220459, 0.8656250238418579, 0.8656250238418579, 0.8651250004768372, 0.8652499914169312, 0.8653749823570251, 0.8651250004768372, 0.8661249876022339, 0.8650000095367432, 0.8662499785423279, 0.8644999861717224, 0.8656250238418579, 0.8659999966621399, 0.8651250004768372, 0.8659999966621399, 0.8653749823570251, 0.8656250238418579, 0.8656250238418579, 0.8648750185966492, 0.8662499785423279, 0.8657500147819519, 0.8647500276565552, 0.8650000095367432, 0.8658750057220459, 0.8657500147819519, 0.8646249771118164, 0.8650000095367432, 0.8650000095367432, 0.8650000095367432, 0.8658750057220459, 0.8644999861717224, 0.8658750057220459, 0.8661249876022339, 0.8646249771118164, 0.8662499785423279, 0.8648750185966492, 0.8658750057220459, 0.8647500276565552, 0.8657500147819519, 0.8659999966621399, 0.8656250238418579, 0.8661249876022339, 0.8659999966621399, 0.8656250238418579, 0.8652499914169312, 0.8652499914169312, 0.8658750057220459, 0.8661249876022339, 0.8665000200271606, 0.8651250004768372, 0.8652499914169312, 0.8669999837875366, 0.8653749823570251, 0.8657500147819519, 0.8676249980926514, 0.8653749823570251, 0.8651250004768372, 0.8651250004768372, 0.8646249771118164, 0.8663750290870667, 0.8663750290870667, 0.8650000095367432, 0.8667500019073486, 0.8666250109672546, 0.8668749928474426, 0.8648750185966492, 0.8658750057220459, 0.8653749823570251, 0.8648750185966492, 0.8654999732971191, 0.8651250004768372, 0.8657500147819519, 0.8657500147819519, 0.8666250109672546, 0.8648750185966492, 0.8656250238418579, 0.8661249876022339, 0.8650000095367432, 0.8654999732971191, 0.8657500147819519, 0.8673750162124634, 0.8659999966621399, 0.8653749823570251, 0.8662499785423279, 0.8656250238418579, 0.8652499914169312, 0.8654999732971191, 0.8662499785423279, 0.8665000200271606, 0.8659999966621399, 0.8662499785423279, 0.8673750162124634, 0.8663750290870667, 0.8657500147819519, 0.8668749928474426, 0.8656250238418579, 0.8654999732971191, 0.8659999966621399, 0.8650000095367432, 0.8657500147819519, 0.8672500252723694, 0.8658750057220459, 0.8661249876022339, 0.8662499785423279, 0.8656250238418579, 0.8661249876022339, 0.8668749928474426, 0.8654999732971191, 0.8657500147819519, 0.8669999837875366, 0.8653749823570251, 0.8652499914169312, 0.8656250238418579, 0.8659999966621399, 0.8662499785423279, 0.8665000200271606, 0.8653749823570251, 0.8658750057220459, 0.8671249747276306, 0.8675000071525574, 0.8658750057220459, 0.8667500019073486, 0.8662499785423279, 0.8669999837875366, 0.8648750185966492, 0.8662499785423279, 0.8671249747276306, 0.8651250004768372, 0.8658750057220459, 0.8657500147819519, 0.8667500019073486, 0.8662499785423279, 0.8667500019073486, 0.8666250109672546, 0.8671249747276306, 0.8666250109672546, 0.8672500252723694, 0.8646249771118164, 0.8671249747276306, 0.8663750290870667, 0.8659999966621399, 0.8665000200271606, 0.8667500019073486, 0.8665000200271606, 0.8669999837875366, 0.8654999732971191, 0.8668749928474426, 0.8661249876022339, 0.8661249876022339, 0.8668749928474426, 0.8667500019073486, 0.8657500147819519, 0.8667500019073486, 0.8671249747276306, 0.8658750057220459, 0.8667500019073486, 0.8667500019073486, 0.8663750290870667, 0.8665000200271606, 0.8658750057220459, 0.8673750162124634, 0.8672500252723694, 0.8661249876022339, 0.8659999966621399, 0.8675000071525574, 0.8677499890327454, 0.8663750290870667, 0.8675000071525574, 0.8671249747276306, 0.8663750290870667, 0.8681250214576721, 0.8673750162124634, 0.8671249747276306, 0.8663750290870667, 0.8678749799728394, 0.8672500252723694, 0.8659999966621399, 0.8671249747276306, 0.8675000071525574, 0.8672500252723694, 0.8677499890327454, 0.8665000200271606, 0.8671249747276306, 0.8673750162124634, 0.8663750290870667, 0.8678749799728394, 0.8673750162124634, 0.8675000071525574, 0.8669999837875366, 0.8668749928474426, 0.8668749928474426, 0.8673750162124634, 0.8668749928474426, 0.8668749928474426, 0.8667500019073486, 0.8669999837875366, 0.8666250109672546, 0.8658750057220459, 0.8672500252723694, 0.8675000071525574, 0.8676249980926514, 0.8678749799728394, 0.8682500123977661, 0.8662499785423279, 0.8686249852180481, 0.8676249980926514, 0.8679999709129333, 0.8663750290870667, 0.8677499890327454, 0.8662499785423279, 0.8666250109672546, 0.8675000071525574, 0.8679999709129333, 0.8677499890327454, 0.8682500123977661, 0.8676249980926514, 0.8682500123977661, 0.8683750033378601, 0.8682500123977661, 0.8671249747276306, 0.8678749799728394, 0.8676249980926514, 0.8677499890327454, 0.8679999709129333, 0.8678749799728394, 0.8684999942779541, 0.8671249747276306, 0.8682500123977661, 0.8675000071525574, 0.8679999709129333, 0.8687499761581421, 0.8675000071525574, 0.8672500252723694, 0.8682500123977661, 0.8683750033378601, 0.8677499890327454, 0.8681250214576721, 0.8672500252723694, 0.8690000176429749, 0.8678749799728394]\n",
      "[0.5832542777061462, 0.5151048898696899, 0.4984625279903412, 0.4912351965904236, 0.4849123954772949, 0.4781073033809662, 0.4708373546600342, 0.4635999798774719, 0.45661523938179016, 0.45012277364730835, 0.4445437788963318, 0.43967321515083313, 0.435769647359848, 0.43256914615631104, 0.43013137578964233, 0.4280426800251007, 0.42635875940322876, 0.42475825548171997, 0.423370897769928, 0.421878457069397, 0.4205130934715271, 0.418952077627182, 0.417366087436676, 0.4156753122806549, 0.41401195526123047, 0.41230496764183044, 0.4104502201080322, 0.40866315364837646, 0.40693241357803345, 0.4051823019981384, 0.40330618619918823, 0.4016961455345154, 0.39981916546821594, 0.3979114890098572, 0.39626118540763855, 0.39444002509117126, 0.3927006125450134, 0.391105055809021, 0.38946065306663513, 0.3878912329673767, 0.38639938831329346, 0.38488587737083435, 0.383189857006073, 0.3819066882133484, 0.3805169463157654, 0.37913572788238525, 0.37788137793540955, 0.3765046000480652, 0.3755582273006439, 0.37407955527305603, 0.37302449345588684, 0.37191101908683777, 0.37086910009384155, 0.36975133419036865, 0.3691035807132721, 0.3677910566329956, 0.36673030257225037, 0.36609724164009094, 0.3650319576263428, 0.3640652000904083, 0.36345940828323364, 0.3626440167427063, 0.3618737757205963, 0.36111900210380554, 0.36027759313583374, 0.3596660792827606, 0.35903146862983704, 0.3582283854484558, 0.35763683915138245, 0.35688796639442444, 0.356186181306839, 0.35558634996414185, 0.3551401197910309, 0.3543221652507782, 0.3538459837436676, 0.3533722758293152, 0.35281044244766235, 0.3522184193134308, 0.3516450524330139, 0.35119351744651794, 0.35064753890037537, 0.3501822352409363, 0.3498188555240631, 0.3491767644882202, 0.34878864884376526, 0.3483089804649353, 0.3479354977607727, 0.3476278483867645, 0.3472141921520233, 0.34664005041122437, 0.34627583622932434, 0.3463706076145172, 0.3455911874771118, 0.34552279114723206, 0.34477490186691284, 0.34469500184059143, 0.3444614112377167, 0.34391000866889954, 0.34385642409324646, 0.3433334231376648, 0.34315845370292664, 0.343082070350647, 0.3425505459308624, 0.34233760833740234, 0.3419898450374603, 0.34211593866348267, 0.3415370285511017, 0.341403067111969, 0.3412014842033386, 0.3407009541988373, 0.3406517207622528, 0.34059637784957886, 0.3405010402202606, 0.34028515219688416, 0.34001418948173523, 0.33970823884010315, 0.33959102630615234, 0.3391645550727844, 0.3392350375652313, 0.339135080575943, 0.33890974521636963, 0.33865997195243835, 0.3384188115596771, 0.3381037414073944, 0.3380521237850189, 0.3378145098686218, 0.3379031717777252, 0.33768078684806824, 0.33762410283088684, 0.3375445008277893, 0.3373095691204071, 0.3374311029911041, 0.3369140625, 0.33690133690834045, 0.3367965519428253, 0.33674994111061096, 0.33649587631225586, 0.33632025122642517, 0.33623939752578735, 0.3362409174442291, 0.33590373396873474, 0.3358594477176666, 0.33566322922706604, 0.33569207787513733, 0.33548980951309204, 0.33549225330352783, 0.3355814516544342, 0.3353891968727112, 0.33520275354385376, 0.33513143658638, 0.33490678668022156, 0.3349061608314514, 0.3346928656101227, 0.334788978099823, 0.3344850242137909, 0.3347781300544739, 0.3342481255531311, 0.3342861533164978, 0.334150105714798, 0.33430495858192444, 0.33411186933517456, 0.33399105072021484, 0.33370012044906616, 0.3338397741317749, 0.333591490983963, 0.3335804045200348, 0.3335352838039398, 0.3333599865436554, 0.333551824092865, 0.3332854211330414, 0.33324265480041504, 0.33330607414245605, 0.33298182487487793, 0.33284303545951843, 0.33288970589637756, 0.3326975107192993, 0.33295172452926636, 0.3327849805355072, 0.3327080011367798, 0.33280834555625916, 0.3325727581977844, 0.33239877223968506, 0.33244848251342773, 0.33230096101760864, 0.33194857835769653, 0.3320443630218506, 0.33206984400749207, 0.3318710923194885, 0.3318386673927307, 0.3318077325820923, 0.33169591426849365, 0.33152005076408386, 0.3316183388233185, 0.33173584938049316, 0.3313140571117401, 0.3313983976840973, 0.3313559889793396, 0.3312065005302429, 0.33153101801872253, 0.3311513066291809, 0.33124494552612305, 0.3311474323272705, 0.3308599293231964, 0.33090487122535706, 0.33091458678245544, 0.3311360478401184, 0.3307988941669464, 0.3310509920120239, 0.330597460269928, 0.33058246970176697, 0.33041104674339294, 0.33057430386543274, 0.3303844630718231, 0.33041369915008545, 0.3305141031742096, 0.33028385043144226, 0.3302710950374603, 0.33004286885261536, 0.32989180088043213, 0.3299790918827057, 0.330292671918869, 0.3298308253288269, 0.32987621426582336, 0.3298064172267914, 0.3297480642795563, 0.32946279644966125, 0.32977721095085144, 0.329474538564682, 0.3295392394065857, 0.32947155833244324, 0.329512357711792, 0.3293416202068329, 0.3294571340084076, 0.3293013870716095, 0.32919809222221375, 0.3291439414024353, 0.3291614353656769, 0.32919996976852417, 0.3290194869041443, 0.32904356718063354, 0.32918640971183777, 0.32884326577186584, 0.32896676659584045, 0.32898077368736267, 0.3287927806377411, 0.328933984041214, 0.32865187525749207, 0.32896870374679565, 0.3285495936870575, 0.3287683427333832, 0.32827481627464294, 0.3286539316177368, 0.32847410440444946, 0.3284496068954468, 0.32825329899787903, 0.32817989587783813, 0.3283230662345886, 0.3281107544898987, 0.32819512486457825, 0.328312486410141, 0.3282383382320404, 0.3280360996723175, 0.3280448913574219, 0.32793116569519043, 0.3280113637447357, 0.3279227614402771, 0.32797449827194214, 0.32800963521003723, 0.32780295610427856, 0.32787665724754333, 0.32783249020576477, 0.32784029841423035, 0.3275078535079956, 0.3278493881225586, 0.3279433846473694, 0.32767775654792786, 0.3276495337486267, 0.32759931683540344, 0.3274870216846466, 0.3274846076965332, 0.32751181721687317, 0.32742583751678467, 0.3275643289089203, 0.3275095522403717, 0.3274018466472626, 0.32741284370422363, 0.3272947669029236, 0.3271145522594452, 0.32741451263427734, 0.32713454961776733, 0.32720571756362915, 0.3270416259765625, 0.32712411880493164, 0.327004998922348, 0.32716870307922363, 0.32702070474624634, 0.3270280063152313, 0.327129065990448, 0.3267979025840759, 0.32694149017333984, 0.32681822776794434, 0.3267400562763214, 0.3268752992153168, 0.32675701379776, 0.32682979106903076, 0.32672786712646484, 0.3267100751399994, 0.32669466733932495, 0.326835036277771, 0.32660213112831116, 0.32656437158584595, 0.3266512155532837, 0.3264271914958954, 0.326568067073822, 0.32676512002944946, 0.32661837339401245, 0.32647085189819336, 0.3263949453830719, 0.3264656960964203, 0.326456755399704, 0.32635024189949036, 0.3264930844306946, 0.32657310366630554, 0.3263475298881531, 0.3265448212623596, 0.3264082670211792, 0.32649490237236023, 0.32640540599823, 0.32630330324172974, 0.3262709081172943, 0.32629358768463135, 0.3262736201286316, 0.32636120915412903, 0.3261187672615051, 0.32631146907806396, 0.32595303654670715, 0.3259809911251068, 0.3263000547885895, 0.3258441090583801, 0.32615935802459717, 0.3259831666946411, 0.32581594586372375, 0.3260061740875244, 0.32584425806999207, 0.3260546624660492, 0.32582324743270874, 0.3257339298725128, 0.3260575234889984, 0.32580098509788513, 0.32577085494995117, 0.32586002349853516, 0.3258574604988098, 0.32569706439971924, 0.325904905796051, 0.32596102356910706, 0.32568010687828064, 0.32568737864494324, 0.3258064091205597, 0.3257174789905548, 0.32571184635162354, 0.3255554139614105, 0.32593250274658203, 0.32544079422950745, 0.3257001042366028, 0.3256565034389496, 0.32551273703575134, 0.325430691242218, 0.32537898421287537, 0.3254549503326416, 0.32539355754852295, 0.32537731528282166, 0.32561472058296204, 0.325366735458374, 0.32542359828948975, 0.3252057135105133, 0.3255821764469147, 0.3253854811191559, 0.3253421187400818, 0.32539474964141846, 0.3253542482852936, 0.32532528042793274, 0.3252411484718323, 0.325318843126297, 0.32544371485710144, 0.32529664039611816, 0.3252752721309662, 0.3251110017299652, 0.32523345947265625, 0.3253992795944214, 0.3253173232078552, 0.32505002617836, 0.32507139444351196, 0.32496392726898193, 0.3249564468860626, 0.32501769065856934, 0.3249889016151428, 0.324966162443161, 0.32527318596839905, 0.32506948709487915, 0.3250581622123718, 0.3250395655632019, 0.32481831312179565, 0.3250613212585449, 0.32490289211273193, 0.3248080909252167, 0.3248835504055023, 0.32484111189842224, 0.32483023405075073, 0.3249543309211731, 0.32487303018569946, 0.32477423548698425, 0.324685275554657, 0.3247503936290741, 0.3248625695705414, 0.324757844209671, 0.32492491602897644, 0.3248371481895447, 0.3247424364089966, 0.3247799277305603, 0.3247830867767334, 0.3248327374458313, 0.32463130354881287, 0.32485097646713257, 0.3247111141681671, 0.32453763484954834, 0.3245660960674286, 0.324452668428421, 0.3245680034160614, 0.3245117664337158, 0.32459673285484314, 0.32444044947624207, 0.32436826825141907, 0.32439377903938293, 0.3245772123336792, 0.32437384128570557, 0.32431066036224365, 0.3244866728782654, 0.3244526982307434, 0.3242724537849426, 0.3245319426059723, 0.3245360255241394, 0.3242394030094147, 0.3249064087867737, 0.32444989681243896, 0.32431429624557495, 0.3240400552749634, 0.3244885802268982, 0.32436680793762207, 0.3242817521095276, 0.3242032527923584, 0.32413265109062195, 0.32445228099823, 0.3244071900844574, 0.32420530915260315, 0.3244016468524933, 0.324201375246048, 0.32416602969169617, 0.32415395975112915, 0.32403266429901123, 0.32413509488105774, 0.32425999641418457, 0.32404643297195435, 0.324126273393631, 0.32422930002212524, 0.3242524266242981, 0.32410457730293274, 0.32407090067863464, 0.3241809606552124, 0.324036180973053, 0.3244282007217407, 0.3239746391773224, 0.324210524559021, 0.3240785300731659, 0.3240374028682709, 0.3238999545574188, 0.3238999843597412, 0.3239731788635254, 0.32384514808654785, 0.324003130197525, 0.32412096858024597, 0.3238756060600281, 0.32392892241477966, 0.3240297734737396, 0.32388541102409363, 0.32356593012809753, 0.3239014744758606, 0.32380229234695435, 0.3238999843597412, 0.3237667381763458, 0.3239513635635376, 0.3237215280532837, 0.32386091351509094, 0.32423561811447144, 0.3237522840499878, 0.3236387073993683, 0.3236197531223297, 0.3237093687057495, 0.32374268770217896, 0.32372260093688965, 0.3236856460571289]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(history.history['accuracy'])\n",
    "print(history.history['loss'])\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy and loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['accuracy', 'loss'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a4a041b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.00150113 -0.58312392 -0.57273139 -1.55489968  0.91509065  0.10629772\n",
      "  -0.70174202 -0.26396987  0.80225696  0.64376017  0.97725852 -0.00249134]]\n"
     ]
    }
   ],
   "source": [
    "### Now we use the trained weights and biases to try to predict based on a new case\n",
    "tr=sc.transform([[1, 0, 0, 500, 1, 40, 3, 60000, 2, 1, 1, 100000]])\n",
    "print(tr)  ### tr.shape is (1,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86520ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 64ms/step\n",
      "[[0.04271373]]\n"
     ]
    }
   ],
   "source": [
    "### Example\n",
    "### Predicting result for Single Observation\n",
    "print(NNmodel.predict(tr))\n",
    "### note in each recompute -- this no. will change slightly because of the random initiation of the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0be7271f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 611us/step - loss: 0.3230 - accuracy: 0.8680\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3230210244655609, 0.8679999709129333]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Now we use the trained NNmodel to predict output in X_train sample\n",
    "NNmodel.evaluate(X_train,Y_train)  ### evaluates the loss and accuracy as specified in the Compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ba646c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 470us/step\n"
     ]
    }
   ],
   "source": [
    "### Now we use the trained NNmodel to predict output in X_train sample -- computing manually via .predict\n",
    "TE=NNmodel.predict(X_train)  ### note X_train has 8000 data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "82ce3372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TE.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b21a584d",
   "metadata": {},
   "outputs": [],
   "source": [
    "h=(TE > 0.5).astype(int) ### Convert TE>0.5 == true ==> 1, False to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb651f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8000, 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(h)\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a697768",
   "metadata": {},
   "outputs": [],
   "source": [
    "### replace all elements in numpy array of value 0 with value -1\n",
    "h[h==0]=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c9d41502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " ...\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8000, 1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(h)\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8b950e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### replace all elements in numpy array of value 0 with value -1\n",
    "Y_train1=Y_train\n",
    "Y_train1[Y_train1==0]=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "53b50eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1 -1  1 ...  1 -1  1]\n"
     ]
    }
   ],
   "source": [
    "print(Y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec6af287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fe3de8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6944 0.868\n"
     ]
    }
   ],
   "source": [
    "J=np.multiply(Y_train1.T,h.T)  ### element by element multiplication\n",
    "c=np.count_nonzero(J > 0) \n",
    "print(c,c/8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4416dde4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 531us/step - loss: 0.3319 - accuracy: 0.8625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3319126069545746, 0.862500011920929]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Now we use the trained NNmodel to predict output in X_test sample\n",
    "NNmodel.evaluate(X_test,Y_test)  ### evaluates the loss and accuracy as specified in the Compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f34f4bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 567us/step\n"
     ]
    }
   ],
   "source": [
    "### Now we use the trained NNmodel to predict output in X_test sample -- computing manually via .predict\n",
    "TE1=NNmodel.predict(X_test)  ### note X_test has 2000 data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "14712483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1725 0.8625\n"
     ]
    }
   ],
   "source": [
    "h1=(TE1 > 0.5).astype(int) ### Convert TE1>0.5 == true ==> 1, False to 0\n",
    "h1[h1==0]=-1\n",
    "Y_test1=Y_test\n",
    "Y_test1[Y_test1==0]=-1\n",
    "J1=np.multiply(Y_test1.T,h1.T)  ### element by element multiplication\n",
    "c1=np.count_nonzero(J1 > 0) \n",
    "print(c1,c1/2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5667ff07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
