{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "617ee135",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#Importing necessary Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c6deff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Dataset\n",
    "data = pd.read_csv(\"Churn_Modelling.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a80540e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CreditScore Geography  Gender  Age  Tenure    Balance  NumOfProducts  \\\n",
       "0          619    France  Female   42       2       0.00              1   \n",
       "1          608     Spain  Female   41       1   83807.86              1   \n",
       "2          502    France  Female   42       8  159660.80              3   \n",
       "3          699    France  Female   39       1       0.00              2   \n",
       "4          850     Spain  Female   43       2  125510.82              1   \n",
       "\n",
       "   HasCrCard  IsActiveMember  EstimatedSalary  \n",
       "0          1               1        101348.88  \n",
       "1          0               1        112542.58  \n",
       "2          1               0        113931.57  \n",
       "3          0               0         93826.63  \n",
       "4          1               1         79084.10  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generating Dependent Variable Vectors\n",
    "Y = data.iloc[:,-1].values\n",
    "X = data.iloc[:,3:13]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a98c381c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0\n",
      "1       0\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "       ..\n",
      "9995    1\n",
      "9996    1\n",
      "9997    0\n",
      "9998    1\n",
      "9999    0\n",
      "Name: Gender, Length: 10000, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Generating Dependent Variable Vectors\n",
    "Y = data.iloc[:,-1].values\n",
    "X = data.iloc[:,3:13]\n",
    "X['Gender']=X['Gender'].map({'Female':0,'Male':1})\n",
    "### above is used instead of a more complicated package involving -- from sklearn.preprocessing import LabelEncoder\n",
    "### converts Female -- 0, Male -- 1, i.e. hot-encoding categorical variables\n",
    "print (X['Gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e436f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding Categorical variable Geography\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ct =ColumnTransformer(transformers=[('encoder',OneHotEncoder(),[1])],remainder=\"passthrough\")\n",
    "X = np.array(ct.fit_transform(X))\n",
    "### Geography is transformed into France -- 1,0,0; Spain -- 0,0,1; Germany -- 0,1,0.\n",
    "### Moreover -- this encoded vector of ones-zeros is now put in first 3 cols. Credit Score pushed to 4th col."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3166a509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>619.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101348.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>608.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>112542.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>502.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>113931.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>699.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93826.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>850.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>79084.10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0    1    2      3    4     5    6          7    8    9    10         11\n",
       "0  1.0  0.0  0.0  619.0  0.0  42.0  2.0       0.00  1.0  1.0  1.0  101348.88\n",
       "1  0.0  0.0  1.0  608.0  0.0  41.0  1.0   83807.86  1.0  0.0  1.0  112542.58\n",
       "2  1.0  0.0  0.0  502.0  0.0  42.0  8.0  159660.80  3.0  1.0  0.0  113931.57\n",
       "3  1.0  0.0  0.0  699.0  0.0  39.0  1.0       0.00  2.0  0.0  0.0   93826.63\n",
       "4  0.0  0.0  1.0  850.0  0.0  43.0  2.0  125510.82  1.0  1.0  1.0   79084.10"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### convert X to dataframe X1\n",
    "X1 = pd.DataFrame(X)\n",
    "X1.head()\n",
    "### Note there are 12 features including onehotencoder for the Geography feature-- \n",
    "### The features are encoded using a one-hot (aka ‘one-of-K’ or ‘dummy’) encoding scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ef915ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting dataset into training and testing dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "740ede9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9850b84b",
   "metadata": {},
   "source": [
    "We call fit_transform() method on our training data and transform() method on our test data. Each feature in the training\n",
    "set is scaled to mean 0, variance 1. In sklearn.preprocessing.StandardScaler(), centering and scaling happens independently on each feature. The fit method is calculating the mean and variance of each of the features present in the data. The transform method is transforming all the features using the respective feature's mean and variance that are calculated in the statement\n",
    "before on X_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f43f1db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### This is the very first step while creating NNmodel -- you can rename this model. Here we are going to create NNmodel object \n",
    "### by using a certain class of Keras named Sequential. As a part of tensorflow 2.0, Keras is now integrated with \n",
    "### tensorflow and is now considered as a sub-library of tensorflow. The Sequential class is a part of the models module \n",
    "### of Keras library which is a part of the tensorflow library now. \n",
    "### It used to be \"import tensorflow as tf; from tensorflow import keras; from tensorflow.keras import layers\"\n",
    "### See documentation at https://keras.io/guides/sequential_model/\n",
    "\n",
    "#Initialising the NN model name -- NNmodel\n",
    "NNmodel = tf.keras.models.Sequential()\n",
    "### Sequential specifies to keras that the model NNmodel is created sequentially and the output of each layer added \n",
    "### is input to the next specified layer. Note that keras Sequential is not appropriate when the model has multiple outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34730cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating a network that has 1 hidden layer together with 1 input layer and 1 output layer. \n",
    "#Adding First Hidden Layer\n",
    "NNmodel.add(tf.keras.layers.Dense(units=2,activation=\"sigmoid\"))\n",
    "### units = 2 refer to 2 neurons in hidden layer \n",
    "### modelname.add is used to add a layer to the neural network -- need to specify as an argument what type of layer --\n",
    "### Dense is used to specify the fully connected layer - i.e. all neurons are forward connect to all forward layer nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab12edcf",
   "metadata": {},
   "source": [
    "Above -- first hidden layer is created using the Dense class which is part of the layers module. This class accepts 2 inputs:-\n",
    "(1) units:- number of neurons that will be present in the respective layer (2) activation:- specify which activation function to be used. This example uses first input as 2. There is no correct answer which is the right number of neurons in the layer -- trial and error. Not too large to be computationally impractical or redundant; not too small to be ineffective.\n",
    "For the second input, we try the sigmoid or logistic function as an activation function for hidden layers. We can also try “relu”[rectified linear unit]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "303997d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### now we create the output layer\n",
    "#Adding Output Layer\n",
    "NNmodel.add(tf.keras.layers.Dense(units=1,activation=\"sigmoid\"))\n",
    "### Only 1 output neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b4a623",
   "metadata": {},
   "source": [
    "For a binary classification problem as above, actual case output is 1 or 0. Hence we require only one neuron to output layer - output could be estimated probability of case actual output = 1. For multiclass classification problem, if the output contains m categories then we need to create m different neurons, one for each category. In the binary output case, the suitable activation function is the sigmoid function. For multiclass classification problem, the activation function is typically softmax. The softmax function predicts a multinomial probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70baaa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "### After creating the layers -- require compiling the NNmodel. Compiling allows the computer to run and understand the program \n",
    "### without the need of more fundamental steps in the programming. Compiling adds other elements or linking other libraries, and optimization,\n",
    "### such that after compiling the results are readily computed e.g. in a binary executable program as an output. \n",
    "#Compiling NNmodel\n",
    "NNmodel.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=['accuracy'])\n",
    "### Note optimizer here is a more sophisticated version of the Mean Square loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f9d245",
   "metadata": {},
   "source": [
    "Compile method above accepts inputs: (1) optimizer:- specifies which optimizer to be used in order to perform stochastic gradient descent (2) error/loss function, e.g., 'binary_crossentropy' here. For multiclass classification, it should be categorical_crossentropy, (3) metrics - the performance metrics to use in order to compute performance. 'accuracy' is one such  performance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51697cc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 12)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36a93b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "80/80 [==============================] - 0s 687us/step - loss: 0.9665 - accuracy: 0.2027\n",
      "Epoch 2/500\n",
      "80/80 [==============================] - 0s 646us/step - loss: 0.8834 - accuracy: 0.2041\n",
      "Epoch 3/500\n",
      "80/80 [==============================] - 0s 535us/step - loss: 0.8148 - accuracy: 0.2285\n",
      "Epoch 4/500\n",
      "80/80 [==============================] - 0s 544us/step - loss: 0.7584 - accuracy: 0.3266\n",
      "Epoch 5/500\n",
      "80/80 [==============================] - 0s 546us/step - loss: 0.7121 - accuracy: 0.4699\n",
      "Epoch 6/500\n",
      "80/80 [==============================] - 0s 585us/step - loss: 0.6743 - accuracy: 0.6190\n",
      "Epoch 7/500\n",
      "80/80 [==============================] - 0s 534us/step - loss: 0.6433 - accuracy: 0.7325\n",
      "Epoch 8/500\n",
      "80/80 [==============================] - 0s 542us/step - loss: 0.6180 - accuracy: 0.7921\n",
      "Epoch 9/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.5973 - accuracy: 0.8005\n",
      "Epoch 10/500\n",
      "80/80 [==============================] - 0s 537us/step - loss: 0.5804 - accuracy: 0.7972\n",
      "Epoch 11/500\n",
      "80/80 [==============================] - 0s 544us/step - loss: 0.5665 - accuracy: 0.7972\n",
      "Epoch 12/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.5551 - accuracy: 0.7972\n",
      "Epoch 13/500\n",
      "80/80 [==============================] - 0s 545us/step - loss: 0.5457 - accuracy: 0.7972\n",
      "Epoch 14/500\n",
      "80/80 [==============================] - 0s 523us/step - loss: 0.5380 - accuracy: 0.7972\n",
      "Epoch 15/500\n",
      "80/80 [==============================] - 0s 531us/step - loss: 0.5315 - accuracy: 0.7972\n",
      "Epoch 16/500\n",
      "80/80 [==============================] - 0s 520us/step - loss: 0.5261 - accuracy: 0.7972\n",
      "Epoch 17/500\n",
      "80/80 [==============================] - 0s 548us/step - loss: 0.5216 - accuracy: 0.7972\n",
      "Epoch 18/500\n",
      "80/80 [==============================] - 0s 520us/step - loss: 0.5177 - accuracy: 0.7972\n",
      "Epoch 19/500\n",
      "80/80 [==============================] - 0s 535us/step - loss: 0.5142 - accuracy: 0.7972\n",
      "Epoch 20/500\n",
      "80/80 [==============================] - 0s 520us/step - loss: 0.5111 - accuracy: 0.7972\n",
      "Epoch 21/500\n",
      "80/80 [==============================] - 0s 531us/step - loss: 0.5084 - accuracy: 0.7972\n",
      "Epoch 22/500\n",
      "80/80 [==============================] - 0s 523us/step - loss: 0.5057 - accuracy: 0.7972\n",
      "Epoch 23/500\n",
      "80/80 [==============================] - 0s 532us/step - loss: 0.5033 - accuracy: 0.7972\n",
      "Epoch 24/500\n",
      "80/80 [==============================] - 0s 533us/step - loss: 0.5009 - accuracy: 0.7972\n",
      "Epoch 25/500\n",
      "80/80 [==============================] - 0s 548us/step - loss: 0.4986 - accuracy: 0.7972\n",
      "Epoch 26/500\n",
      "80/80 [==============================] - 0s 520us/step - loss: 0.4963 - accuracy: 0.7972\n",
      "Epoch 27/500\n",
      "80/80 [==============================] - 0s 533us/step - loss: 0.4940 - accuracy: 0.7972\n",
      "Epoch 28/500\n",
      "80/80 [==============================] - 0s 536us/step - loss: 0.4917 - accuracy: 0.7972\n",
      "Epoch 29/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.4894 - accuracy: 0.7972\n",
      "Epoch 30/500\n",
      "80/80 [==============================] - 0s 532us/step - loss: 0.4871 - accuracy: 0.7972\n",
      "Epoch 31/500\n",
      "80/80 [==============================] - 0s 536us/step - loss: 0.4848 - accuracy: 0.7972\n",
      "Epoch 32/500\n",
      "80/80 [==============================] - 0s 520us/step - loss: 0.4825 - accuracy: 0.7972\n",
      "Epoch 33/500\n",
      "80/80 [==============================] - 0s 543us/step - loss: 0.4801 - accuracy: 0.7972\n",
      "Epoch 34/500\n",
      "80/80 [==============================] - 0s 535us/step - loss: 0.4777 - accuracy: 0.7972\n",
      "Epoch 35/500\n",
      "80/80 [==============================] - 0s 534us/step - loss: 0.4753 - accuracy: 0.7972\n",
      "Epoch 36/500\n",
      "80/80 [==============================] - 0s 531us/step - loss: 0.4727 - accuracy: 0.7972\n",
      "Epoch 37/500\n",
      "80/80 [==============================] - 0s 535us/step - loss: 0.4700 - accuracy: 0.7972\n",
      "Epoch 38/500\n",
      "80/80 [==============================] - 0s 545us/step - loss: 0.4671 - accuracy: 0.7972\n",
      "Epoch 39/500\n",
      "80/80 [==============================] - 0s 523us/step - loss: 0.4640 - accuracy: 0.7972\n",
      "Epoch 40/500\n",
      "80/80 [==============================] - 0s 533us/step - loss: 0.4607 - accuracy: 0.7972\n",
      "Epoch 41/500\n",
      "80/80 [==============================] - 0s 519us/step - loss: 0.4573 - accuracy: 0.7972\n",
      "Epoch 42/500\n",
      "80/80 [==============================] - 0s 529us/step - loss: 0.4539 - accuracy: 0.7972\n",
      "Epoch 43/500\n",
      "80/80 [==============================] - 0s 538us/step - loss: 0.4506 - accuracy: 0.7972\n",
      "Epoch 44/500\n",
      "80/80 [==============================] - 0s 532us/step - loss: 0.4476 - accuracy: 0.7972\n",
      "Epoch 45/500\n",
      "80/80 [==============================] - 0s 536us/step - loss: 0.4448 - accuracy: 0.7972\n",
      "Epoch 46/500\n",
      "80/80 [==============================] - 0s 545us/step - loss: 0.4424 - accuracy: 0.7972\n",
      "Epoch 47/500\n",
      "80/80 [==============================] - 0s 545us/step - loss: 0.4402 - accuracy: 0.7972\n",
      "Epoch 48/500\n",
      "80/80 [==============================] - 0s 523us/step - loss: 0.4382 - accuracy: 0.7972\n",
      "Epoch 49/500\n",
      "80/80 [==============================] - 0s 532us/step - loss: 0.4365 - accuracy: 0.7972\n",
      "Epoch 50/500\n",
      "80/80 [==============================] - 0s 545us/step - loss: 0.4350 - accuracy: 0.7972\n",
      "Epoch 51/500\n",
      "80/80 [==============================] - 0s 548us/step - loss: 0.4337 - accuracy: 0.7972\n",
      "Epoch 52/500\n",
      "80/80 [==============================] - 0s 544us/step - loss: 0.4326 - accuracy: 0.7972\n",
      "Epoch 53/500\n",
      "80/80 [==============================] - 0s 546us/step - loss: 0.4316 - accuracy: 0.7972\n",
      "Epoch 54/500\n",
      "80/80 [==============================] - 0s 535us/step - loss: 0.4307 - accuracy: 0.7972\n",
      "Epoch 55/500\n",
      "80/80 [==============================] - 0s 512us/step - loss: 0.4300 - accuracy: 0.7972\n",
      "Epoch 56/500\n",
      "80/80 [==============================] - 0s 545us/step - loss: 0.4293 - accuracy: 0.7972\n",
      "Epoch 57/500\n",
      "80/80 [==============================] - 0s 549us/step - loss: 0.4287 - accuracy: 0.7972\n",
      "Epoch 58/500\n",
      "80/80 [==============================] - 0s 531us/step - loss: 0.4281 - accuracy: 0.7972\n",
      "Epoch 59/500\n",
      "80/80 [==============================] - 0s 551us/step - loss: 0.4276 - accuracy: 0.7972\n",
      "Epoch 60/500\n",
      "80/80 [==============================] - 0s 536us/step - loss: 0.4271 - accuracy: 0.7972\n",
      "Epoch 61/500\n",
      "80/80 [==============================] - 0s 544us/step - loss: 0.4266 - accuracy: 0.7972\n",
      "Epoch 62/500\n",
      "80/80 [==============================] - 0s 533us/step - loss: 0.4262 - accuracy: 0.7972\n",
      "Epoch 63/500\n",
      "80/80 [==============================] - 0s 534us/step - loss: 0.4257 - accuracy: 0.7972\n",
      "Epoch 64/500\n",
      "80/80 [==============================] - 0s 546us/step - loss: 0.4251 - accuracy: 0.8023\n",
      "Epoch 65/500\n",
      "80/80 [==============================] - 0s 531us/step - loss: 0.4246 - accuracy: 0.8066\n",
      "Epoch 66/500\n",
      "80/80 [==============================] - 0s 535us/step - loss: 0.4240 - accuracy: 0.8092\n",
      "Epoch 67/500\n",
      "80/80 [==============================] - 0s 545us/step - loss: 0.4234 - accuracy: 0.8120\n",
      "Epoch 68/500\n",
      "80/80 [==============================] - 0s 544us/step - loss: 0.4228 - accuracy: 0.8159\n",
      "Epoch 69/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.4221 - accuracy: 0.8183\n",
      "Epoch 70/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.4215 - accuracy: 0.8198\n",
      "Epoch 71/500\n",
      "80/80 [==============================] - 0s 528us/step - loss: 0.4208 - accuracy: 0.8202\n",
      "Epoch 72/500\n",
      "80/80 [==============================] - 0s 532us/step - loss: 0.4202 - accuracy: 0.8220\n",
      "Epoch 73/500\n",
      "80/80 [==============================] - 0s 543us/step - loss: 0.4196 - accuracy: 0.8236\n",
      "Epoch 74/500\n",
      "80/80 [==============================] - 0s 531us/step - loss: 0.4189 - accuracy: 0.8232\n",
      "Epoch 75/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.4184 - accuracy: 0.8236\n",
      "Epoch 76/500\n",
      "80/80 [==============================] - 0s 535us/step - loss: 0.4178 - accuracy: 0.8241\n",
      "Epoch 77/500\n",
      "80/80 [==============================] - 0s 520us/step - loss: 0.4173 - accuracy: 0.8236\n",
      "Epoch 78/500\n",
      "80/80 [==============================] - 0s 533us/step - loss: 0.4168 - accuracy: 0.8241\n",
      "Epoch 79/500\n",
      "80/80 [==============================] - 0s 535us/step - loss: 0.4163 - accuracy: 0.8244\n",
      "Epoch 80/500\n",
      "80/80 [==============================] - 0s 520us/step - loss: 0.4159 - accuracy: 0.8259\n",
      "Epoch 81/500\n",
      "80/80 [==============================] - 0s 531us/step - loss: 0.4154 - accuracy: 0.8256\n",
      "Epoch 82/500\n",
      "80/80 [==============================] - 0s 536us/step - loss: 0.4151 - accuracy: 0.8263\n",
      "Epoch 83/500\n",
      "80/80 [==============================] - 0s 532us/step - loss: 0.4147 - accuracy: 0.8279\n",
      "Epoch 84/500\n",
      "80/80 [==============================] - 0s 548us/step - loss: 0.4143 - accuracy: 0.8294\n",
      "Epoch 85/500\n",
      "80/80 [==============================] - 0s 646us/step - loss: 0.4140 - accuracy: 0.8291\n",
      "Epoch 86/500\n",
      "80/80 [==============================] - 0s 620us/step - loss: 0.4137 - accuracy: 0.8299\n",
      "Epoch 87/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.4134 - accuracy: 0.8303\n",
      "Epoch 88/500\n",
      "80/80 [==============================] - 0s 569us/step - loss: 0.4131 - accuracy: 0.8311\n",
      "Epoch 89/500\n",
      "80/80 [==============================] - 0s 596us/step - loss: 0.4128 - accuracy: 0.8304\n",
      "Epoch 90/500\n",
      "80/80 [==============================] - 0s 569us/step - loss: 0.4125 - accuracy: 0.8311\n",
      "Epoch 91/500\n",
      "80/80 [==============================] - 0s 584us/step - loss: 0.4123 - accuracy: 0.8316\n",
      "Epoch 92/500\n",
      "80/80 [==============================] - 0s 545us/step - loss: 0.4120 - accuracy: 0.8320\n",
      "Epoch 93/500\n",
      "80/80 [==============================] - 0s 595us/step - loss: 0.4119 - accuracy: 0.8315\n",
      "Epoch 94/500\n",
      "80/80 [==============================] - 0s 595us/step - loss: 0.4116 - accuracy: 0.8315\n",
      "Epoch 95/500\n",
      "80/80 [==============================] - 0s 595us/step - loss: 0.4114 - accuracy: 0.8317\n",
      "Epoch 96/500\n",
      "80/80 [==============================] - 0s 582us/step - loss: 0.4112 - accuracy: 0.8322\n",
      "Epoch 97/500\n",
      "80/80 [==============================] - 0s 574us/step - loss: 0.4110 - accuracy: 0.8326\n",
      "Epoch 98/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.4108 - accuracy: 0.8325\n",
      "Epoch 99/500\n",
      "80/80 [==============================] - 0s 582us/step - loss: 0.4107 - accuracy: 0.8328\n",
      "Epoch 100/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.4105 - accuracy: 0.8324\n",
      "Epoch 101/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.4103 - accuracy: 0.8328\n",
      "Epoch 102/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.4102 - accuracy: 0.8329\n",
      "Epoch 103/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.4100 - accuracy: 0.8331\n",
      "Epoch 104/500\n",
      "80/80 [==============================] - 0s 608us/step - loss: 0.4099 - accuracy: 0.8331\n",
      "Epoch 105/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.4097 - accuracy: 0.8335\n",
      "Epoch 106/500\n",
      "80/80 [==============================] - 0s 586us/step - loss: 0.4096 - accuracy: 0.8331\n",
      "Epoch 107/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.4095 - accuracy: 0.8334\n",
      "Epoch 108/500\n",
      "80/80 [==============================] - 0s 608us/step - loss: 0.4094 - accuracy: 0.8334\n",
      "Epoch 109/500\n",
      "80/80 [==============================] - 0s 633us/step - loss: 0.4092 - accuracy: 0.8335\n",
      "Epoch 110/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.4091 - accuracy: 0.8336\n",
      "Epoch 111/500\n",
      "80/80 [==============================] - 0s 594us/step - loss: 0.4090 - accuracy: 0.8340\n",
      "Epoch 112/500\n",
      "80/80 [==============================] - 0s 576us/step - loss: 0.4089 - accuracy: 0.8336\n",
      "Epoch 113/500\n",
      "80/80 [==============================] - 0s 577us/step - loss: 0.4088 - accuracy: 0.8332\n",
      "Epoch 114/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.4088 - accuracy: 0.8335\n",
      "Epoch 115/500\n",
      "80/80 [==============================] - 0s 556us/step - loss: 0.4086 - accuracy: 0.8334\n",
      "Epoch 116/500\n",
      "80/80 [==============================] - 0s 533us/step - loss: 0.4086 - accuracy: 0.8340\n",
      "Epoch 117/500\n",
      "80/80 [==============================] - 0s 525us/step - loss: 0.4085 - accuracy: 0.8339\n",
      "Epoch 118/500\n",
      "80/80 [==============================] - 0s 530us/step - loss: 0.4084 - accuracy: 0.8336\n",
      "Epoch 119/500\n",
      "80/80 [==============================] - 0s 547us/step - loss: 0.4083 - accuracy: 0.8340\n",
      "Epoch 120/500\n",
      "80/80 [==============================] - 0s 545us/step - loss: 0.4082 - accuracy: 0.8341\n",
      "Epoch 121/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.4081 - accuracy: 0.8339\n",
      "Epoch 122/500\n",
      "80/80 [==============================] - 0s 523us/step - loss: 0.4080 - accuracy: 0.8342\n",
      "Epoch 123/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.4080 - accuracy: 0.8342\n",
      "Epoch 124/500\n",
      "80/80 [==============================] - 0s 519us/step - loss: 0.4079 - accuracy: 0.8344\n",
      "Epoch 125/500\n",
      "80/80 [==============================] - 0s 507us/step - loss: 0.4078 - accuracy: 0.8345\n",
      "Epoch 126/500\n",
      "80/80 [==============================] - 0s 523us/step - loss: 0.4078 - accuracy: 0.8351\n",
      "Epoch 127/500\n",
      "80/80 [==============================] - 0s 545us/step - loss: 0.4077 - accuracy: 0.8351\n",
      "Epoch 128/500\n",
      "80/80 [==============================] - 0s 535us/step - loss: 0.4076 - accuracy: 0.8351\n",
      "Epoch 129/500\n",
      "80/80 [==============================] - 0s 532us/step - loss: 0.4076 - accuracy: 0.8350\n",
      "Epoch 130/500\n",
      "80/80 [==============================] - 0s 545us/step - loss: 0.4075 - accuracy: 0.8342\n",
      "Epoch 131/500\n",
      "80/80 [==============================] - 0s 536us/step - loss: 0.4075 - accuracy: 0.8354\n",
      "Epoch 132/500\n",
      "80/80 [==============================] - 0s 532us/step - loss: 0.4074 - accuracy: 0.8361\n",
      "Epoch 133/500\n",
      "80/80 [==============================] - 0s 546us/step - loss: 0.4073 - accuracy: 0.8356\n",
      "Epoch 134/500\n",
      "80/80 [==============================] - 0s 534us/step - loss: 0.4073 - accuracy: 0.8359\n",
      "Epoch 135/500\n",
      "80/80 [==============================] - 0s 533us/step - loss: 0.4073 - accuracy: 0.8355\n",
      "Epoch 136/500\n",
      "80/80 [==============================] - 0s 544us/step - loss: 0.4072 - accuracy: 0.8357\n",
      "Epoch 137/500\n",
      "80/80 [==============================] - 0s 535us/step - loss: 0.4072 - accuracy: 0.8359\n",
      "Epoch 138/500\n",
      "80/80 [==============================] - 0s 532us/step - loss: 0.4071 - accuracy: 0.8361\n",
      "Epoch 139/500\n",
      "80/80 [==============================] - 0s 521us/step - loss: 0.4070 - accuracy: 0.8359\n",
      "Epoch 140/500\n",
      "80/80 [==============================] - 0s 547us/step - loss: 0.4070 - accuracy: 0.8364\n",
      "Epoch 141/500\n",
      "80/80 [==============================] - 0s 519us/step - loss: 0.4070 - accuracy: 0.8357\n",
      "Epoch 142/500\n",
      "80/80 [==============================] - 0s 545us/step - loss: 0.4069 - accuracy: 0.8357\n",
      "Epoch 143/500\n",
      "80/80 [==============================] - 0s 535us/step - loss: 0.4069 - accuracy: 0.8356\n",
      "Epoch 144/500\n",
      "80/80 [==============================] - 0s 545us/step - loss: 0.4069 - accuracy: 0.8359\n",
      "Epoch 145/500\n",
      "80/80 [==============================] - 0s 519us/step - loss: 0.4068 - accuracy: 0.8357\n",
      "Epoch 146/500\n",
      "80/80 [==============================] - 0s 549us/step - loss: 0.4068 - accuracy: 0.8354\n",
      "Epoch 147/500\n",
      "80/80 [==============================] - 0s 520us/step - loss: 0.4067 - accuracy: 0.8359\n",
      "Epoch 148/500\n",
      "80/80 [==============================] - 0s 522us/step - loss: 0.4067 - accuracy: 0.8355\n",
      "Epoch 149/500\n",
      "80/80 [==============================] - 0s 532us/step - loss: 0.4066 - accuracy: 0.8363\n",
      "Epoch 150/500\n",
      "80/80 [==============================] - 0s 545us/step - loss: 0.4066 - accuracy: 0.8357\n",
      "Epoch 151/500\n",
      "80/80 [==============================] - 0s 535us/step - loss: 0.4066 - accuracy: 0.8356\n",
      "Epoch 152/500\n",
      "80/80 [==============================] - 0s 519us/step - loss: 0.4065 - accuracy: 0.8357\n",
      "Epoch 153/500\n",
      "80/80 [==============================] - 0s 522us/step - loss: 0.4065 - accuracy: 0.8359\n",
      "Epoch 154/500\n",
      "80/80 [==============================] - 0s 522us/step - loss: 0.4065 - accuracy: 0.8355\n",
      "Epoch 155/500\n",
      "80/80 [==============================] - 0s 520us/step - loss: 0.4064 - accuracy: 0.8356\n",
      "Epoch 156/500\n",
      "80/80 [==============================] - 0s 548us/step - loss: 0.4064 - accuracy: 0.8353\n",
      "Epoch 157/500\n",
      "80/80 [==============================] - 0s 519us/step - loss: 0.4064 - accuracy: 0.8361\n",
      "Epoch 158/500\n",
      "80/80 [==============================] - 0s 519us/step - loss: 0.4064 - accuracy: 0.8357\n",
      "Epoch 159/500\n",
      "80/80 [==============================] - 0s 536us/step - loss: 0.4063 - accuracy: 0.8360\n",
      "Epoch 160/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 533us/step - loss: 0.4064 - accuracy: 0.8361\n",
      "Epoch 161/500\n",
      "80/80 [==============================] - 0s 532us/step - loss: 0.4063 - accuracy: 0.8356\n",
      "Epoch 162/500\n",
      "80/80 [==============================] - 0s 535us/step - loss: 0.4063 - accuracy: 0.8355\n",
      "Epoch 163/500\n",
      "80/80 [==============================] - 0s 520us/step - loss: 0.4062 - accuracy: 0.8364\n",
      "Epoch 164/500\n",
      "80/80 [==============================] - 0s 536us/step - loss: 0.4062 - accuracy: 0.8361\n",
      "Epoch 165/500\n",
      "80/80 [==============================] - 0s 507us/step - loss: 0.4062 - accuracy: 0.8356\n",
      "Epoch 166/500\n",
      "80/80 [==============================] - 0s 532us/step - loss: 0.4061 - accuracy: 0.8361\n",
      "Epoch 167/500\n",
      "80/80 [==============================] - 0s 536us/step - loss: 0.4061 - accuracy: 0.8365\n",
      "Epoch 168/500\n",
      "80/80 [==============================] - 0s 533us/step - loss: 0.4061 - accuracy: 0.8361\n",
      "Epoch 169/500\n",
      "80/80 [==============================] - 0s 543us/step - loss: 0.4061 - accuracy: 0.8366\n",
      "Epoch 170/500\n",
      "80/80 [==============================] - 0s 536us/step - loss: 0.4061 - accuracy: 0.8367\n",
      "Epoch 171/500\n",
      "80/80 [==============================] - 0s 520us/step - loss: 0.4061 - accuracy: 0.8366\n",
      "Epoch 172/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.4060 - accuracy: 0.8361\n",
      "Epoch 173/500\n",
      "80/80 [==============================] - 0s 510us/step - loss: 0.4060 - accuracy: 0.8367\n",
      "Epoch 174/500\n",
      "80/80 [==============================] - 0s 544us/step - loss: 0.4060 - accuracy: 0.8369\n",
      "Epoch 175/500\n",
      "80/80 [==============================] - 0s 546us/step - loss: 0.4060 - accuracy: 0.8369\n",
      "Epoch 176/500\n",
      "80/80 [==============================] - 0s 522us/step - loss: 0.4059 - accuracy: 0.8367\n",
      "Epoch 177/500\n",
      "80/80 [==============================] - 0s 507us/step - loss: 0.4059 - accuracy: 0.8360\n",
      "Epoch 178/500\n",
      "80/80 [==============================] - 0s 510us/step - loss: 0.4059 - accuracy: 0.8364\n",
      "Epoch 179/500\n",
      "80/80 [==============================] - 0s 520us/step - loss: 0.4059 - accuracy: 0.8369\n",
      "Epoch 180/500\n",
      "80/80 [==============================] - 0s 531us/step - loss: 0.4059 - accuracy: 0.8369\n",
      "Epoch 181/500\n",
      "80/80 [==============================] - 0s 510us/step - loss: 0.4058 - accuracy: 0.8364\n",
      "Epoch 182/500\n",
      "80/80 [==============================] - 0s 519us/step - loss: 0.4058 - accuracy: 0.8367\n",
      "Epoch 183/500\n",
      "80/80 [==============================] - 0s 524us/step - loss: 0.4058 - accuracy: 0.8372\n",
      "Epoch 184/500\n",
      "80/80 [==============================] - 0s 507us/step - loss: 0.4058 - accuracy: 0.8366\n",
      "Epoch 185/500\n",
      "80/80 [==============================] - 0s 497us/step - loss: 0.4057 - accuracy: 0.8369\n",
      "Epoch 186/500\n",
      "80/80 [==============================] - 0s 520us/step - loss: 0.4057 - accuracy: 0.8365\n",
      "Epoch 187/500\n",
      "80/80 [==============================] - 0s 543us/step - loss: 0.4057 - accuracy: 0.8374\n",
      "Epoch 188/500\n",
      "80/80 [==============================] - 0s 536us/step - loss: 0.4057 - accuracy: 0.8371\n",
      "Epoch 189/500\n",
      "80/80 [==============================] - 0s 544us/step - loss: 0.4057 - accuracy: 0.8369\n",
      "Epoch 190/500\n",
      "80/80 [==============================] - 0s 532us/step - loss: 0.4056 - accuracy: 0.8371\n",
      "Epoch 191/500\n",
      "80/80 [==============================] - 0s 511us/step - loss: 0.4057 - accuracy: 0.8372\n",
      "Epoch 192/500\n",
      "80/80 [==============================] - 0s 494us/step - loss: 0.4056 - accuracy: 0.8369\n",
      "Epoch 193/500\n",
      "80/80 [==============================] - 0s 521us/step - loss: 0.4057 - accuracy: 0.8371\n",
      "Epoch 194/500\n",
      "80/80 [==============================] - 0s 522us/step - loss: 0.4056 - accuracy: 0.8370\n",
      "Epoch 195/500\n",
      "80/80 [==============================] - 0s 538us/step - loss: 0.4056 - accuracy: 0.8370\n",
      "Epoch 196/500\n",
      "80/80 [==============================] - 0s 498us/step - loss: 0.4056 - accuracy: 0.8371\n",
      "Epoch 197/500\n",
      "80/80 [==============================] - 0s 525us/step - loss: 0.4056 - accuracy: 0.8374\n",
      "Epoch 198/500\n",
      "80/80 [==============================] - 0s 511us/step - loss: 0.4055 - accuracy: 0.8369\n",
      "Epoch 199/500\n",
      "80/80 [==============================] - 0s 508us/step - loss: 0.4056 - accuracy: 0.8366\n",
      "Epoch 200/500\n",
      "80/80 [==============================] - 0s 535us/step - loss: 0.4055 - accuracy: 0.8369\n",
      "Epoch 201/500\n",
      "80/80 [==============================] - 0s 536us/step - loss: 0.4055 - accuracy: 0.8374\n",
      "Epoch 202/500\n",
      "80/80 [==============================] - 0s 505us/step - loss: 0.4055 - accuracy: 0.8370\n",
      "Epoch 203/500\n",
      "80/80 [==============================] - 0s 519us/step - loss: 0.4055 - accuracy: 0.8369\n",
      "Epoch 204/500\n",
      "80/80 [==============================] - 0s 523us/step - loss: 0.4055 - accuracy: 0.8370\n",
      "Epoch 205/500\n",
      "80/80 [==============================] - 0s 520us/step - loss: 0.4055 - accuracy: 0.8374\n",
      "Epoch 206/500\n",
      "80/80 [==============================] - 0s 532us/step - loss: 0.4055 - accuracy: 0.8370\n",
      "Epoch 207/500\n",
      "80/80 [==============================] - 0s 576us/step - loss: 0.4054 - accuracy: 0.8372\n",
      "Epoch 208/500\n",
      "80/80 [==============================] - 0s 521us/step - loss: 0.4054 - accuracy: 0.8372\n",
      "Epoch 209/500\n",
      "80/80 [==============================] - 0s 490us/step - loss: 0.4054 - accuracy: 0.8370\n",
      "Epoch 210/500\n",
      "80/80 [==============================] - 0s 536us/step - loss: 0.4054 - accuracy: 0.8374\n",
      "Epoch 211/500\n",
      "80/80 [==============================] - 0s 533us/step - loss: 0.4054 - accuracy: 0.8371\n",
      "Epoch 212/500\n",
      "80/80 [==============================] - 0s 534us/step - loss: 0.4054 - accuracy: 0.8366\n",
      "Epoch 213/500\n",
      "80/80 [==============================] - 0s 520us/step - loss: 0.4053 - accuracy: 0.8372\n",
      "Epoch 214/500\n",
      "80/80 [==============================] - 0s 533us/step - loss: 0.4053 - accuracy: 0.8376\n",
      "Epoch 215/500\n",
      "80/80 [==============================] - 0s 535us/step - loss: 0.4053 - accuracy: 0.8371\n",
      "Epoch 216/500\n",
      "80/80 [==============================] - 0s 520us/step - loss: 0.4053 - accuracy: 0.8375\n",
      "Epoch 217/500\n",
      "80/80 [==============================] - 0s 545us/step - loss: 0.4053 - accuracy: 0.8371\n",
      "Epoch 218/500\n",
      "80/80 [==============================] - 0s 542us/step - loss: 0.4053 - accuracy: 0.8375\n",
      "Epoch 219/500\n",
      "80/80 [==============================] - 0s 512us/step - loss: 0.4053 - accuracy: 0.8367\n",
      "Epoch 220/500\n",
      "80/80 [==============================] - 0s 544us/step - loss: 0.4052 - accuracy: 0.8371\n",
      "Epoch 221/500\n",
      "80/80 [==============================] - 0s 526us/step - loss: 0.4053 - accuracy: 0.8369\n",
      "Epoch 222/500\n",
      "80/80 [==============================] - 0s 530us/step - loss: 0.4052 - accuracy: 0.8376\n",
      "Epoch 223/500\n",
      "80/80 [==============================] - 0s 535us/step - loss: 0.4052 - accuracy: 0.8369\n",
      "Epoch 224/500\n",
      "80/80 [==============================] - 0s 519us/step - loss: 0.4052 - accuracy: 0.8375\n",
      "Epoch 225/500\n",
      "80/80 [==============================] - 0s 532us/step - loss: 0.4052 - accuracy: 0.8374\n",
      "Epoch 226/500\n",
      "80/80 [==============================] - 0s 510us/step - loss: 0.4052 - accuracy: 0.8369\n",
      "Epoch 227/500\n",
      "80/80 [==============================] - 0s 520us/step - loss: 0.4052 - accuracy: 0.8367\n",
      "Epoch 228/500\n",
      "80/80 [==============================] - 0s 543us/step - loss: 0.4052 - accuracy: 0.8364\n",
      "Epoch 229/500\n",
      "80/80 [==============================] - 0s 512us/step - loss: 0.4052 - accuracy: 0.8369\n",
      "Epoch 230/500\n",
      "80/80 [==============================] - 0s 519us/step - loss: 0.4051 - accuracy: 0.8370\n",
      "Epoch 231/500\n",
      "80/80 [==============================] - 0s 523us/step - loss: 0.4051 - accuracy: 0.8371\n",
      "Epoch 232/500\n",
      "80/80 [==============================] - 0s 544us/step - loss: 0.4051 - accuracy: 0.8374\n",
      "Epoch 233/500\n",
      "80/80 [==============================] - 0s 544us/step - loss: 0.4051 - accuracy: 0.8372\n",
      "Epoch 234/500\n",
      "80/80 [==============================] - 0s 523us/step - loss: 0.4051 - accuracy: 0.8372\n",
      "Epoch 235/500\n",
      "80/80 [==============================] - 0s 532us/step - loss: 0.4051 - accuracy: 0.8367\n",
      "Epoch 236/500\n",
      "80/80 [==============================] - 0s 545us/step - loss: 0.4051 - accuracy: 0.8369\n",
      "Epoch 237/500\n",
      "80/80 [==============================] - 0s 535us/step - loss: 0.4051 - accuracy: 0.8370\n",
      "Epoch 238/500\n",
      "80/80 [==============================] - 0s 537us/step - loss: 0.4051 - accuracy: 0.8371\n",
      "Epoch 239/500\n",
      "80/80 [==============================] - 0s 525us/step - loss: 0.4051 - accuracy: 0.8371\n",
      "Epoch 240/500\n",
      "80/80 [==============================] - 0s 534us/step - loss: 0.4050 - accuracy: 0.8374\n",
      "Epoch 241/500\n",
      "80/80 [==============================] - 0s 521us/step - loss: 0.4050 - accuracy: 0.8367\n",
      "Epoch 242/500\n",
      "80/80 [==============================] - 0s 537us/step - loss: 0.4051 - accuracy: 0.8374\n",
      "Epoch 243/500\n",
      "80/80 [==============================] - 0s 531us/step - loss: 0.4050 - accuracy: 0.8372\n",
      "Epoch 244/500\n",
      "80/80 [==============================] - 0s 534us/step - loss: 0.4050 - accuracy: 0.8371\n",
      "Epoch 245/500\n",
      "80/80 [==============================] - 0s 545us/step - loss: 0.4050 - accuracy: 0.8375\n",
      "Epoch 246/500\n",
      "80/80 [==============================] - 0s 535us/step - loss: 0.4050 - accuracy: 0.8371\n",
      "Epoch 247/500\n",
      "80/80 [==============================] - 0s 532us/step - loss: 0.4050 - accuracy: 0.8374\n",
      "Epoch 248/500\n",
      "80/80 [==============================] - 0s 523us/step - loss: 0.4050 - accuracy: 0.8374\n",
      "Epoch 249/500\n",
      "80/80 [==============================] - 0s 507us/step - loss: 0.4050 - accuracy: 0.8370\n",
      "Epoch 250/500\n",
      "80/80 [==============================] - 0s 523us/step - loss: 0.4049 - accuracy: 0.8376\n",
      "Epoch 251/500\n",
      "80/80 [==============================] - 0s 519us/step - loss: 0.4050 - accuracy: 0.8375\n",
      "Epoch 252/500\n",
      "80/80 [==============================] - 0s 507us/step - loss: 0.4049 - accuracy: 0.8375\n",
      "Epoch 253/500\n",
      "80/80 [==============================] - 0s 498us/step - loss: 0.4049 - accuracy: 0.8374\n",
      "Epoch 254/500\n",
      "80/80 [==============================] - 0s 533us/step - loss: 0.4049 - accuracy: 0.8372\n",
      "Epoch 255/500\n",
      "80/80 [==============================] - 0s 498us/step - loss: 0.4049 - accuracy: 0.8374\n",
      "Epoch 256/500\n",
      "80/80 [==============================] - 0s 506us/step - loss: 0.4049 - accuracy: 0.8375\n",
      "Epoch 257/500\n",
      "80/80 [==============================] - 0s 524us/step - loss: 0.4049 - accuracy: 0.8371\n",
      "Epoch 258/500\n",
      "80/80 [==============================] - 0s 527us/step - loss: 0.4049 - accuracy: 0.8372\n",
      "Epoch 259/500\n",
      "80/80 [==============================] - 0s 541us/step - loss: 0.4049 - accuracy: 0.8374\n",
      "Epoch 260/500\n",
      "80/80 [==============================] - 0s 532us/step - loss: 0.4049 - accuracy: 0.8371\n",
      "Epoch 261/500\n",
      "80/80 [==============================] - 0s 532us/step - loss: 0.4049 - accuracy: 0.8376\n",
      "Epoch 262/500\n",
      "80/80 [==============================] - 0s 535us/step - loss: 0.4048 - accuracy: 0.8374\n",
      "Epoch 263/500\n",
      "80/80 [==============================] - 0s 533us/step - loss: 0.4049 - accuracy: 0.8372\n",
      "Epoch 264/500\n",
      "80/80 [==============================] - 0s 519us/step - loss: 0.4049 - accuracy: 0.8369\n",
      "Epoch 265/500\n",
      "80/80 [==============================] - 0s 523us/step - loss: 0.4049 - accuracy: 0.8376\n",
      "Epoch 266/500\n",
      "80/80 [==============================] - 0s 520us/step - loss: 0.4048 - accuracy: 0.8367\n",
      "Epoch 267/500\n",
      "80/80 [==============================] - 0s 532us/step - loss: 0.4048 - accuracy: 0.8367\n",
      "Epoch 268/500\n",
      "80/80 [==============================] - 0s 548us/step - loss: 0.4048 - accuracy: 0.8372\n",
      "Epoch 269/500\n",
      "80/80 [==============================] - 0s 538us/step - loss: 0.4048 - accuracy: 0.8370\n",
      "Epoch 270/500\n",
      "80/80 [==============================] - 0s 545us/step - loss: 0.4048 - accuracy: 0.8371\n",
      "Epoch 271/500\n",
      "80/80 [==============================] - 0s 536us/step - loss: 0.4047 - accuracy: 0.8372\n",
      "Epoch 272/500\n",
      "80/80 [==============================] - 0s 532us/step - loss: 0.4048 - accuracy: 0.8370\n",
      "Epoch 273/500\n",
      "80/80 [==============================] - 0s 539us/step - loss: 0.4048 - accuracy: 0.8375\n",
      "Epoch 274/500\n",
      "80/80 [==============================] - 0s 523us/step - loss: 0.4048 - accuracy: 0.8374\n",
      "Epoch 275/500\n",
      "80/80 [==============================] - 0s 532us/step - loss: 0.4047 - accuracy: 0.8376\n",
      "Epoch 276/500\n",
      "80/80 [==============================] - 0s 537us/step - loss: 0.4047 - accuracy: 0.8371\n",
      "Epoch 277/500\n",
      "80/80 [==============================] - 0s 519us/step - loss: 0.4047 - accuracy: 0.8375\n",
      "Epoch 278/500\n",
      "80/80 [==============================] - 0s 545us/step - loss: 0.4047 - accuracy: 0.8375\n",
      "Epoch 279/500\n",
      "80/80 [==============================] - 0s 535us/step - loss: 0.4047 - accuracy: 0.8376\n",
      "Epoch 280/500\n",
      "80/80 [==============================] - 0s 544us/step - loss: 0.4047 - accuracy: 0.8371\n",
      "Epoch 281/500\n",
      "80/80 [==============================] - 0s 534us/step - loss: 0.4047 - accuracy: 0.8370\n",
      "Epoch 282/500\n",
      "80/80 [==============================] - 0s 523us/step - loss: 0.4048 - accuracy: 0.8378\n",
      "Epoch 283/500\n",
      "80/80 [==============================] - 0s 521us/step - loss: 0.4047 - accuracy: 0.8381\n",
      "Epoch 284/500\n",
      "80/80 [==============================] - 0s 535us/step - loss: 0.4047 - accuracy: 0.8375\n",
      "Epoch 285/500\n",
      "80/80 [==============================] - 0s 531us/step - loss: 0.4047 - accuracy: 0.8378\n",
      "Epoch 286/500\n",
      "80/80 [==============================] - 0s 533us/step - loss: 0.4047 - accuracy: 0.8372\n",
      "Epoch 287/500\n",
      "80/80 [==============================] - 0s 536us/step - loss: 0.4047 - accuracy: 0.8375\n",
      "Epoch 288/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.4047 - accuracy: 0.8371\n",
      "Epoch 289/500\n",
      "80/80 [==============================] - 0s 532us/step - loss: 0.4047 - accuracy: 0.8371\n",
      "Epoch 290/500\n",
      "80/80 [==============================] - 0s 540us/step - loss: 0.4047 - accuracy: 0.8375\n",
      "Epoch 291/500\n",
      "80/80 [==============================] - 0s 528us/step - loss: 0.4047 - accuracy: 0.8376\n",
      "Epoch 292/500\n",
      "80/80 [==============================] - 0s 521us/step - loss: 0.4046 - accuracy: 0.8378\n",
      "Epoch 293/500\n",
      "80/80 [==============================] - 0s 535us/step - loss: 0.4046 - accuracy: 0.8376\n",
      "Epoch 294/500\n",
      "80/80 [==============================] - 0s 520us/step - loss: 0.4046 - accuracy: 0.8375\n",
      "Epoch 295/500\n",
      "80/80 [==============================] - 0s 519us/step - loss: 0.4046 - accuracy: 0.8374\n",
      "Epoch 296/500\n",
      "80/80 [==============================] - 0s 549us/step - loss: 0.4046 - accuracy: 0.8378\n",
      "Epoch 297/500\n",
      "80/80 [==============================] - 0s 519us/step - loss: 0.4047 - accuracy: 0.8370\n",
      "Epoch 298/500\n",
      "80/80 [==============================] - 0s 545us/step - loss: 0.4046 - accuracy: 0.8388\n",
      "Epoch 299/500\n",
      "80/80 [==============================] - 0s 536us/step - loss: 0.4046 - accuracy: 0.8380\n",
      "Epoch 300/500\n",
      "80/80 [==============================] - 0s 532us/step - loss: 0.4046 - accuracy: 0.8382\n",
      "Epoch 301/500\n",
      "80/80 [==============================] - 0s 532us/step - loss: 0.4046 - accuracy: 0.8375\n",
      "Epoch 302/500\n",
      "80/80 [==============================] - 0s 537us/step - loss: 0.4046 - accuracy: 0.8379\n",
      "Epoch 303/500\n",
      "80/80 [==============================] - 0s 518us/step - loss: 0.4046 - accuracy: 0.8380\n",
      "Epoch 304/500\n",
      "80/80 [==============================] - 0s 522us/step - loss: 0.4046 - accuracy: 0.8381\n",
      "Epoch 305/500\n",
      "80/80 [==============================] - 0s 506us/step - loss: 0.4046 - accuracy: 0.8381\n",
      "Epoch 306/500\n",
      "80/80 [==============================] - 0s 532us/step - loss: 0.4045 - accuracy: 0.8382\n",
      "Epoch 307/500\n",
      "80/80 [==============================] - 0s 541us/step - loss: 0.4045 - accuracy: 0.8374\n",
      "Epoch 308/500\n",
      "80/80 [==============================] - 0s 527us/step - loss: 0.4046 - accuracy: 0.8380\n",
      "Epoch 309/500\n",
      "80/80 [==============================] - 0s 524us/step - loss: 0.4046 - accuracy: 0.8375\n",
      "Epoch 310/500\n",
      "80/80 [==============================] - 0s 532us/step - loss: 0.4045 - accuracy: 0.8381\n",
      "Epoch 311/500\n",
      "80/80 [==============================] - 0s 533us/step - loss: 0.4045 - accuracy: 0.8376\n",
      "Epoch 312/500\n",
      "80/80 [==============================] - 0s 560us/step - loss: 0.4045 - accuracy: 0.8386\n",
      "Epoch 313/500\n",
      "80/80 [==============================] - 0s 520us/step - loss: 0.4045 - accuracy: 0.8379\n",
      "Epoch 314/500\n",
      "80/80 [==============================] - 0s 533us/step - loss: 0.4045 - accuracy: 0.8381\n",
      "Epoch 315/500\n",
      "80/80 [==============================] - 0s 534us/step - loss: 0.4045 - accuracy: 0.8380\n",
      "Epoch 316/500\n",
      "80/80 [==============================] - 0s 533us/step - loss: 0.4045 - accuracy: 0.8382\n",
      "Epoch 317/500\n",
      "80/80 [==============================] - 0s 532us/step - loss: 0.4045 - accuracy: 0.8382\n",
      "Epoch 318/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 535us/step - loss: 0.4045 - accuracy: 0.8381\n",
      "Epoch 319/500\n",
      "80/80 [==============================] - 0s 532us/step - loss: 0.4045 - accuracy: 0.8372\n",
      "Epoch 320/500\n",
      "80/80 [==============================] - 0s 532us/step - loss: 0.4045 - accuracy: 0.8381\n",
      "Epoch 321/500\n",
      "80/80 [==============================] - 0s 523us/step - loss: 0.4045 - accuracy: 0.8381\n",
      "Epoch 322/500\n",
      "80/80 [==============================] - 0s 533us/step - loss: 0.4045 - accuracy: 0.8384\n",
      "Epoch 323/500\n",
      "80/80 [==============================] - 0s 522us/step - loss: 0.4045 - accuracy: 0.8381\n",
      "Epoch 324/500\n",
      "80/80 [==============================] - 0s 533us/step - loss: 0.4044 - accuracy: 0.8381\n",
      "Epoch 325/500\n",
      "80/80 [==============================] - 0s 533us/step - loss: 0.4044 - accuracy: 0.8382\n",
      "Epoch 326/500\n",
      "80/80 [==============================] - 0s 561us/step - loss: 0.4045 - accuracy: 0.8385\n",
      "Epoch 327/500\n",
      "80/80 [==============================] - 0s 519us/step - loss: 0.4044 - accuracy: 0.8384\n",
      "Epoch 328/500\n",
      "80/80 [==============================] - 0s 545us/step - loss: 0.4045 - accuracy: 0.8372\n",
      "Epoch 329/500\n",
      "80/80 [==============================] - 0s 536us/step - loss: 0.4044 - accuracy: 0.8382\n",
      "Epoch 330/500\n",
      "80/80 [==============================] - 0s 544us/step - loss: 0.4044 - accuracy: 0.8382\n",
      "Epoch 331/500\n",
      "80/80 [==============================] - 0s 532us/step - loss: 0.4044 - accuracy: 0.8378\n",
      "Epoch 332/500\n",
      "80/80 [==============================] - 0s 548us/step - loss: 0.4045 - accuracy: 0.8380\n",
      "Epoch 333/500\n",
      "80/80 [==============================] - 0s 525us/step - loss: 0.4044 - accuracy: 0.8381\n",
      "Epoch 334/500\n",
      "80/80 [==============================] - 0s 534us/step - loss: 0.4045 - accuracy: 0.8384\n",
      "Epoch 335/500\n",
      "80/80 [==============================] - 0s 537us/step - loss: 0.4044 - accuracy: 0.8386\n",
      "Epoch 336/500\n",
      "80/80 [==============================] - 0s 531us/step - loss: 0.4045 - accuracy: 0.8381\n",
      "Epoch 337/500\n",
      "80/80 [==============================] - 0s 532us/step - loss: 0.4044 - accuracy: 0.8381\n",
      "Epoch 338/500\n",
      "80/80 [==============================] - 0s 542us/step - loss: 0.4044 - accuracy: 0.8381\n",
      "Epoch 339/500\n",
      "80/80 [==============================] - 0s 542us/step - loss: 0.4044 - accuracy: 0.8384\n",
      "Epoch 340/500\n",
      "80/80 [==============================] - 0s 532us/step - loss: 0.4044 - accuracy: 0.8388\n",
      "Epoch 341/500\n",
      "80/80 [==============================] - 0s 536us/step - loss: 0.4044 - accuracy: 0.8376\n",
      "Epoch 342/500\n",
      "80/80 [==============================] - 0s 569us/step - loss: 0.4044 - accuracy: 0.8384\n",
      "Epoch 343/500\n",
      "80/80 [==============================] - 0s 532us/step - loss: 0.4044 - accuracy: 0.8384\n",
      "Epoch 344/500\n",
      "80/80 [==============================] - 0s 523us/step - loss: 0.4044 - accuracy: 0.8386\n",
      "Epoch 345/500\n",
      "80/80 [==============================] - 0s 519us/step - loss: 0.4044 - accuracy: 0.8371\n",
      "Epoch 346/500\n",
      "80/80 [==============================] - 0s 532us/step - loss: 0.4044 - accuracy: 0.8382\n",
      "Epoch 347/500\n",
      "80/80 [==============================] - 0s 547us/step - loss: 0.4043 - accuracy: 0.8382\n",
      "Epoch 348/500\n",
      "80/80 [==============================] - 0s 521us/step - loss: 0.4043 - accuracy: 0.8378\n",
      "Epoch 349/500\n",
      "80/80 [==============================] - 0s 521us/step - loss: 0.4043 - accuracy: 0.8386\n",
      "Epoch 350/500\n",
      "80/80 [==============================] - 0s 535us/step - loss: 0.4044 - accuracy: 0.8381\n",
      "Epoch 351/500\n",
      "80/80 [==============================] - 0s 519us/step - loss: 0.4043 - accuracy: 0.8378\n",
      "Epoch 352/500\n",
      "80/80 [==============================] - 0s 522us/step - loss: 0.4043 - accuracy: 0.8385\n",
      "Epoch 353/500\n",
      "80/80 [==============================] - 0s 519us/step - loss: 0.4043 - accuracy: 0.8380\n",
      "Epoch 354/500\n",
      "80/80 [==============================] - 0s 545us/step - loss: 0.4043 - accuracy: 0.8386\n",
      "Epoch 355/500\n",
      "80/80 [==============================] - 0s 548us/step - loss: 0.4043 - accuracy: 0.8378\n",
      "Epoch 356/500\n",
      "80/80 [==============================] - 0s 533us/step - loss: 0.4043 - accuracy: 0.8382\n",
      "Epoch 357/500\n",
      "80/80 [==============================] - 0s 531us/step - loss: 0.4043 - accuracy: 0.8378\n",
      "Epoch 358/500\n",
      "80/80 [==============================] - 0s 536us/step - loss: 0.4044 - accuracy: 0.8376\n",
      "Epoch 359/500\n",
      "80/80 [==============================] - 0s 527us/step - loss: 0.4043 - accuracy: 0.8382\n",
      "Epoch 360/500\n",
      "80/80 [==============================] - 0s 517us/step - loss: 0.4043 - accuracy: 0.8384\n",
      "Epoch 361/500\n",
      "80/80 [==============================] - 0s 531us/step - loss: 0.4042 - accuracy: 0.8384\n",
      "Epoch 362/500\n",
      "80/80 [==============================] - 0s 519us/step - loss: 0.4043 - accuracy: 0.8382\n",
      "Epoch 363/500\n",
      "80/80 [==============================] - 0s 536us/step - loss: 0.4042 - accuracy: 0.8380\n",
      "Epoch 364/500\n",
      "80/80 [==============================] - 0s 546us/step - loss: 0.4043 - accuracy: 0.8381\n",
      "Epoch 365/500\n",
      "80/80 [==============================] - 0s 544us/step - loss: 0.4043 - accuracy: 0.8380\n",
      "Epoch 366/500\n",
      "80/80 [==============================] - 0s 537us/step - loss: 0.4042 - accuracy: 0.8386\n",
      "Epoch 367/500\n",
      "80/80 [==============================] - 0s 518us/step - loss: 0.4042 - accuracy: 0.8380\n",
      "Epoch 368/500\n",
      "80/80 [==============================] - 0s 543us/step - loss: 0.4042 - accuracy: 0.8375\n",
      "Epoch 369/500\n",
      "80/80 [==============================] - 0s 537us/step - loss: 0.4043 - accuracy: 0.8378\n",
      "Epoch 370/500\n",
      "80/80 [==============================] - 0s 532us/step - loss: 0.4042 - accuracy: 0.8380\n",
      "Epoch 371/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.4043 - accuracy: 0.8384\n",
      "Epoch 372/500\n",
      "80/80 [==============================] - 0s 536us/step - loss: 0.4042 - accuracy: 0.8378\n",
      "Epoch 373/500\n",
      "80/80 [==============================] - 0s 531us/step - loss: 0.4042 - accuracy: 0.8375\n",
      "Epoch 374/500\n",
      "80/80 [==============================] - 0s 545us/step - loss: 0.4042 - accuracy: 0.8382\n",
      "Epoch 375/500\n",
      "80/80 [==============================] - 0s 535us/step - loss: 0.4042 - accuracy: 0.8381\n",
      "Epoch 376/500\n",
      "80/80 [==============================] - 0s 522us/step - loss: 0.4042 - accuracy: 0.8385\n",
      "Epoch 377/500\n",
      "80/80 [==============================] - 0s 522us/step - loss: 0.4042 - accuracy: 0.8378\n",
      "Epoch 378/500\n",
      "80/80 [==============================] - 0s 520us/step - loss: 0.4042 - accuracy: 0.8385\n",
      "Epoch 379/500\n",
      "80/80 [==============================] - 0s 532us/step - loss: 0.4042 - accuracy: 0.8375\n",
      "Epoch 380/500\n",
      "80/80 [==============================] - 0s 524us/step - loss: 0.4042 - accuracy: 0.8381\n",
      "Epoch 381/500\n",
      "80/80 [==============================] - 0s 493us/step - loss: 0.4042 - accuracy: 0.8382\n",
      "Epoch 382/500\n",
      "80/80 [==============================] - 0s 537us/step - loss: 0.4042 - accuracy: 0.8375\n",
      "Epoch 383/500\n",
      "80/80 [==============================] - 0s 526us/step - loss: 0.4042 - accuracy: 0.8378\n",
      "Epoch 384/500\n",
      "80/80 [==============================] - 0s 525us/step - loss: 0.4042 - accuracy: 0.8384\n",
      "Epoch 385/500\n",
      "80/80 [==============================] - 0s 536us/step - loss: 0.4042 - accuracy: 0.8378\n",
      "Epoch 386/500\n",
      "80/80 [==============================] - 0s 532us/step - loss: 0.4042 - accuracy: 0.8380\n",
      "Epoch 387/500\n",
      "80/80 [==============================] - 0s 531us/step - loss: 0.4042 - accuracy: 0.8380\n",
      "Epoch 388/500\n",
      "80/80 [==============================] - 0s 537us/step - loss: 0.4042 - accuracy: 0.8380\n",
      "Epoch 389/500\n",
      "80/80 [==============================] - 0s 519us/step - loss: 0.4042 - accuracy: 0.8378\n",
      "Epoch 390/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.4041 - accuracy: 0.8380\n",
      "Epoch 391/500\n",
      "80/80 [==============================] - 0s 549us/step - loss: 0.4041 - accuracy: 0.8388\n",
      "Epoch 392/500\n",
      "80/80 [==============================] - 0s 506us/step - loss: 0.4042 - accuracy: 0.8380\n",
      "Epoch 393/500\n",
      "80/80 [==============================] - 0s 536us/step - loss: 0.4041 - accuracy: 0.8375\n",
      "Epoch 394/500\n",
      "80/80 [==============================] - 0s 545us/step - loss: 0.4041 - accuracy: 0.8381\n",
      "Epoch 395/500\n",
      "80/80 [==============================] - 0s 532us/step - loss: 0.4042 - accuracy: 0.8375\n",
      "Epoch 396/500\n",
      "80/80 [==============================] - 0s 524us/step - loss: 0.4042 - accuracy: 0.8384\n",
      "Epoch 397/500\n",
      "80/80 [==============================] - 0s 544us/step - loss: 0.4041 - accuracy: 0.8379\n",
      "Epoch 398/500\n",
      "80/80 [==============================] - 0s 532us/step - loss: 0.4041 - accuracy: 0.8381\n",
      "Epoch 399/500\n",
      "80/80 [==============================] - 0s 526us/step - loss: 0.4041 - accuracy: 0.8379\n",
      "Epoch 400/500\n",
      "80/80 [==============================] - 0s 517us/step - loss: 0.4041 - accuracy: 0.8379\n",
      "Epoch 401/500\n",
      "80/80 [==============================] - 0s 535us/step - loss: 0.4041 - accuracy: 0.8378\n",
      "Epoch 402/500\n",
      "80/80 [==============================] - 0s 545us/step - loss: 0.4041 - accuracy: 0.8380\n",
      "Epoch 403/500\n",
      "80/80 [==============================] - 0s 533us/step - loss: 0.4041 - accuracy: 0.8374\n",
      "Epoch 404/500\n",
      "80/80 [==============================] - 0s 523us/step - loss: 0.4041 - accuracy: 0.8376\n",
      "Epoch 405/500\n",
      "80/80 [==============================] - 0s 520us/step - loss: 0.4041 - accuracy: 0.8380\n",
      "Epoch 406/500\n",
      "80/80 [==============================] - 0s 532us/step - loss: 0.4041 - accuracy: 0.8378\n",
      "Epoch 407/500\n",
      "80/80 [==============================] - 0s 536us/step - loss: 0.4041 - accuracy: 0.8380\n",
      "Epoch 408/500\n",
      "80/80 [==============================] - 0s 519us/step - loss: 0.4041 - accuracy: 0.8382\n",
      "Epoch 409/500\n",
      "80/80 [==============================] - 0s 536us/step - loss: 0.4041 - accuracy: 0.8380\n",
      "Epoch 410/500\n",
      "80/80 [==============================] - 0s 533us/step - loss: 0.4041 - accuracy: 0.8378\n",
      "Epoch 411/500\n",
      "80/80 [==============================] - 0s 519us/step - loss: 0.4041 - accuracy: 0.8378\n",
      "Epoch 412/500\n",
      "80/80 [==============================] - 0s 548us/step - loss: 0.4041 - accuracy: 0.8379\n",
      "Epoch 413/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.4041 - accuracy: 0.8379\n",
      "Epoch 414/500\n",
      "80/80 [==============================] - 0s 519us/step - loss: 0.4042 - accuracy: 0.8379\n",
      "Epoch 415/500\n",
      "80/80 [==============================] - 0s 536us/step - loss: 0.4041 - accuracy: 0.8380\n",
      "Epoch 416/500\n",
      "80/80 [==============================] - 0s 532us/step - loss: 0.4041 - accuracy: 0.8376\n",
      "Epoch 417/500\n",
      "80/80 [==============================] - 0s 520us/step - loss: 0.4041 - accuracy: 0.8374\n",
      "Epoch 418/500\n",
      "80/80 [==============================] - 0s 523us/step - loss: 0.4041 - accuracy: 0.8380\n",
      "Epoch 419/500\n",
      "80/80 [==============================] - 0s 532us/step - loss: 0.4041 - accuracy: 0.8376\n",
      "Epoch 420/500\n",
      "80/80 [==============================] - 0s 544us/step - loss: 0.4040 - accuracy: 0.8381\n",
      "Epoch 421/500\n",
      "80/80 [==============================] - 0s 548us/step - loss: 0.4041 - accuracy: 0.8376\n",
      "Epoch 422/500\n",
      "80/80 [==============================] - 0s 520us/step - loss: 0.4040 - accuracy: 0.8375\n",
      "Epoch 423/500\n",
      "80/80 [==============================] - 0s 532us/step - loss: 0.4040 - accuracy: 0.8376\n",
      "Epoch 424/500\n",
      "80/80 [==============================] - 0s 548us/step - loss: 0.4040 - accuracy: 0.8376\n",
      "Epoch 425/500\n",
      "80/80 [==============================] - 0s 520us/step - loss: 0.4040 - accuracy: 0.8378\n",
      "Epoch 426/500\n",
      "80/80 [==============================] - 0s 532us/step - loss: 0.4040 - accuracy: 0.8376\n",
      "Epoch 427/500\n",
      "80/80 [==============================] - 0s 536us/step - loss: 0.4041 - accuracy: 0.8379\n",
      "Epoch 428/500\n",
      "80/80 [==============================] - 0s 532us/step - loss: 0.4040 - accuracy: 0.8378\n",
      "Epoch 429/500\n",
      "80/80 [==============================] - 0s 524us/step - loss: 0.4040 - accuracy: 0.8378\n",
      "Epoch 430/500\n",
      "80/80 [==============================] - 0s 519us/step - loss: 0.4041 - accuracy: 0.8378\n",
      "Epoch 431/500\n",
      "80/80 [==============================] - 0s 545us/step - loss: 0.4040 - accuracy: 0.8379\n",
      "Epoch 432/500\n",
      "80/80 [==============================] - 0s 535us/step - loss: 0.4040 - accuracy: 0.8384\n",
      "Epoch 433/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.4040 - accuracy: 0.8380\n",
      "Epoch 434/500\n",
      "80/80 [==============================] - 0s 544us/step - loss: 0.4040 - accuracy: 0.8375\n",
      "Epoch 435/500\n",
      "80/80 [==============================] - 0s 524us/step - loss: 0.4040 - accuracy: 0.8380\n",
      "Epoch 436/500\n",
      "80/80 [==============================] - 0s 533us/step - loss: 0.4040 - accuracy: 0.8378\n",
      "Epoch 437/500\n",
      "80/80 [==============================] - 0s 531us/step - loss: 0.4040 - accuracy: 0.8379\n",
      "Epoch 438/500\n",
      "80/80 [==============================] - 0s 535us/step - loss: 0.4040 - accuracy: 0.8379\n",
      "Epoch 439/500\n",
      "80/80 [==============================] - 0s 520us/step - loss: 0.4040 - accuracy: 0.8382\n",
      "Epoch 440/500\n",
      "80/80 [==============================] - 0s 523us/step - loss: 0.4040 - accuracy: 0.8374\n",
      "Epoch 441/500\n",
      "80/80 [==============================] - 0s 506us/step - loss: 0.4040 - accuracy: 0.8372\n",
      "Epoch 442/500\n",
      "80/80 [==============================] - 0s 519us/step - loss: 0.4040 - accuracy: 0.8378\n",
      "Epoch 443/500\n",
      "80/80 [==============================] - 0s 535us/step - loss: 0.4040 - accuracy: 0.8379\n",
      "Epoch 444/500\n",
      "80/80 [==============================] - 0s 532us/step - loss: 0.4040 - accuracy: 0.8378\n",
      "Epoch 445/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.4040 - accuracy: 0.8380\n",
      "Epoch 446/500\n",
      "80/80 [==============================] - 0s 757us/step - loss: 0.4040 - accuracy: 0.8378\n",
      "Epoch 447/500\n",
      "80/80 [==============================] - 0s 651us/step - loss: 0.4040 - accuracy: 0.8378\n",
      "Epoch 448/500\n",
      "80/80 [==============================] - 0s 561us/step - loss: 0.4040 - accuracy: 0.8381\n",
      "Epoch 449/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.4040 - accuracy: 0.8375\n",
      "Epoch 450/500\n",
      "80/80 [==============================] - 0s 571us/step - loss: 0.4040 - accuracy: 0.8375\n",
      "Epoch 451/500\n",
      "80/80 [==============================] - 0s 582us/step - loss: 0.4039 - accuracy: 0.8378\n",
      "Epoch 452/500\n",
      "80/80 [==============================] - 0s 582us/step - loss: 0.4040 - accuracy: 0.8380\n",
      "Epoch 453/500\n",
      "80/80 [==============================] - 0s 593us/step - loss: 0.4040 - accuracy: 0.8378\n",
      "Epoch 454/500\n",
      "80/80 [==============================] - 0s 562us/step - loss: 0.4040 - accuracy: 0.8380\n",
      "Epoch 455/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.4039 - accuracy: 0.8380\n",
      "Epoch 456/500\n",
      "80/80 [==============================] - 0s 572us/step - loss: 0.4040 - accuracy: 0.8381\n",
      "Epoch 457/500\n",
      "80/80 [==============================] - 0s 594us/step - loss: 0.4039 - accuracy: 0.8381\n",
      "Epoch 458/500\n",
      "80/80 [==============================] - 0s 569us/step - loss: 0.4039 - accuracy: 0.8378\n",
      "Epoch 459/500\n",
      "80/80 [==============================] - 0s 609us/step - loss: 0.4040 - accuracy: 0.8372\n",
      "Epoch 460/500\n",
      "80/80 [==============================] - 0s 581us/step - loss: 0.4039 - accuracy: 0.8379\n",
      "Epoch 461/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.4039 - accuracy: 0.8379\n",
      "Epoch 462/500\n",
      "80/80 [==============================] - 0s 586us/step - loss: 0.4040 - accuracy: 0.8376\n",
      "Epoch 463/500\n",
      "80/80 [==============================] - 0s 569us/step - loss: 0.4040 - accuracy: 0.8376\n",
      "Epoch 464/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.4039 - accuracy: 0.8382\n",
      "Epoch 465/500\n",
      "80/80 [==============================] - 0s 594us/step - loss: 0.4040 - accuracy: 0.8376\n",
      "Epoch 466/500\n",
      "80/80 [==============================] - 0s 572us/step - loss: 0.4039 - accuracy: 0.8378\n",
      "Epoch 467/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.4040 - accuracy: 0.8381\n",
      "Epoch 468/500\n",
      "80/80 [==============================] - 0s 530us/step - loss: 0.4039 - accuracy: 0.8380\n",
      "Epoch 469/500\n",
      "80/80 [==============================] - 0s 562us/step - loss: 0.4039 - accuracy: 0.8381\n",
      "Epoch 470/500\n",
      "80/80 [==============================] - 0s 531us/step - loss: 0.4039 - accuracy: 0.8379\n",
      "Epoch 471/500\n",
      "80/80 [==============================] - 0s 544us/step - loss: 0.4039 - accuracy: 0.8380\n",
      "Epoch 472/500\n",
      "80/80 [==============================] - 0s 523us/step - loss: 0.4039 - accuracy: 0.8374\n",
      "Epoch 473/500\n",
      "80/80 [==============================] - 0s 533us/step - loss: 0.4039 - accuracy: 0.8381\n",
      "Epoch 474/500\n",
      "80/80 [==============================] - 0s 518us/step - loss: 0.4039 - accuracy: 0.8380\n",
      "Epoch 475/500\n",
      "80/80 [==============================] - 0s 524us/step - loss: 0.4039 - accuracy: 0.8376\n",
      "Epoch 476/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 544us/step - loss: 0.4039 - accuracy: 0.8378\n",
      "Epoch 477/500\n",
      "80/80 [==============================] - 0s 510us/step - loss: 0.4039 - accuracy: 0.8376\n",
      "Epoch 478/500\n",
      "80/80 [==============================] - 0s 520us/step - loss: 0.4039 - accuracy: 0.8380\n",
      "Epoch 479/500\n",
      "80/80 [==============================] - 0s 535us/step - loss: 0.4039 - accuracy: 0.8379\n",
      "Epoch 480/500\n",
      "80/80 [==============================] - 0s 532us/step - loss: 0.4039 - accuracy: 0.8375\n",
      "Epoch 481/500\n",
      "80/80 [==============================] - 0s 532us/step - loss: 0.4039 - accuracy: 0.8382\n",
      "Epoch 482/500\n",
      "80/80 [==============================] - 0s 548us/step - loss: 0.4039 - accuracy: 0.8376\n",
      "Epoch 483/500\n",
      "80/80 [==============================] - 0s 532us/step - loss: 0.4039 - accuracy: 0.8376\n",
      "Epoch 484/500\n",
      "80/80 [==============================] - 0s 544us/step - loss: 0.4039 - accuracy: 0.8380\n",
      "Epoch 485/500\n",
      "80/80 [==============================] - 0s 537us/step - loss: 0.4039 - accuracy: 0.8381\n",
      "Epoch 486/500\n",
      "80/80 [==============================] - 0s 518us/step - loss: 0.4040 - accuracy: 0.8378\n",
      "Epoch 487/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.4039 - accuracy: 0.8381\n",
      "Epoch 488/500\n",
      "80/80 [==============================] - 0s 536us/step - loss: 0.4039 - accuracy: 0.8379\n",
      "Epoch 489/500\n",
      "80/80 [==============================] - 0s 544us/step - loss: 0.4039 - accuracy: 0.8376\n",
      "Epoch 490/500\n",
      "80/80 [==============================] - 0s 545us/step - loss: 0.4039 - accuracy: 0.8378\n",
      "Epoch 491/500\n",
      "80/80 [==============================] - 0s 532us/step - loss: 0.4039 - accuracy: 0.8381\n",
      "Epoch 492/500\n",
      "80/80 [==============================] - 0s 535us/step - loss: 0.4039 - accuracy: 0.8374\n",
      "Epoch 493/500\n",
      "80/80 [==============================] - 0s 507us/step - loss: 0.4039 - accuracy: 0.8378\n",
      "Epoch 494/500\n",
      "80/80 [==============================] - 0s 523us/step - loss: 0.4039 - accuracy: 0.8378\n",
      "Epoch 495/500\n",
      "80/80 [==============================] - 0s 519us/step - loss: 0.4039 - accuracy: 0.8375\n",
      "Epoch 496/500\n",
      "80/80 [==============================] - 0s 524us/step - loss: 0.4038 - accuracy: 0.8378\n",
      "Epoch 497/500\n",
      "80/80 [==============================] - 0s 520us/step - loss: 0.4038 - accuracy: 0.8378\n",
      "Epoch 498/500\n",
      "80/80 [==============================] - 0s 520us/step - loss: 0.4039 - accuracy: 0.8380\n",
      "Epoch 499/500\n",
      "80/80 [==============================] - 0s 536us/step - loss: 0.4038 - accuracy: 0.8376\n",
      "Epoch 500/500\n",
      "80/80 [==============================] - 0s 506us/step - loss: 0.4039 - accuracy: 0.8380\n"
     ]
    }
   ],
   "source": [
    "#### Last step in creation of NNmodel NNmodel is trained on the training set here with Tensor-Keras .fit based on Compiler\n",
    "#Fitting NNmodel\n",
    "history=NNmodel.fit(X_train,Y_train,batch_size=100,epochs = 500)\n",
    "### Note that tf.keras.models.Sequential() by default uses glorot initializer -- drawing intial weights from a uniform \n",
    "### distribution -- see other possibilities in https://keras.io/api/layers/initializers/\n",
    "### Or you could try own customized wts inputs using\n",
    "### for layer in model.layers:\n",
    "###    init_layer_weight = [] # the weights yourself in this layer\n",
    "###    layer.set_weights(init_layer_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9a74ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (100, 2)                  26        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (100, 1)                  3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 29\n",
      "Trainable params: 29\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NNmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fbefe2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.20274999737739563, 0.20412500202655792, 0.22849999368190765, 0.3266249895095825, 0.4698750078678131, 0.6190000176429749, 0.7325000166893005, 0.7921249866485596, 0.8004999756813049, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.8022500276565552, 0.8066250085830688, 0.809249997138977, 0.8119999766349792, 0.8158749938011169, 0.8182500004768372, 0.8197500109672546, 0.8202499747276306, 0.8220000267028809, 0.8236250281333923, 0.8232499957084656, 0.8236250281333923, 0.8241249918937683, 0.8236250281333923, 0.8241249918937683, 0.8243749737739563, 0.8258749842643738, 0.8256250023841858, 0.8262500166893005, 0.827875018119812, 0.8293750286102295, 0.8291249871253967, 0.8298749923706055, 0.8302500247955322, 0.831125020980835, 0.8303750157356262, 0.831125020980835, 0.8316249847412109, 0.8320000171661377, 0.8314999938011169, 0.8314999938011169, 0.8317499756813049, 0.8322499990463257, 0.8326249718666077, 0.8324999809265137, 0.8327500224113464, 0.8323749899864197, 0.8327500224113464, 0.8328750133514404, 0.8331249952316284, 0.8331249952316284, 0.8335000276565552, 0.8331249952316284, 0.8333749771118164, 0.8333749771118164, 0.8335000276565552, 0.8336250185966492, 0.8339999914169312, 0.8336250185966492, 0.8332499861717224, 0.8335000276565552, 0.8333749771118164, 0.8339999914169312, 0.8338750004768372, 0.8336250185966492, 0.8339999914169312, 0.8341249823570251, 0.8338750004768372, 0.8342499732971191, 0.8342499732971191, 0.8343750238418579, 0.8345000147819519, 0.8351250290870667, 0.8351250290870667, 0.8351250290870667, 0.8349999785423279, 0.8342499732971191, 0.8353750109672546, 0.8361250162124634, 0.8356249928474426, 0.8358749747276306, 0.8355000019073486, 0.8357499837875366, 0.8358749747276306, 0.8361250162124634, 0.8358749747276306, 0.8363749980926514, 0.8357499837875366, 0.8357499837875366, 0.8356249928474426, 0.8358749747276306, 0.8357499837875366, 0.8353750109672546, 0.8358749747276306, 0.8355000019073486, 0.8362500071525574, 0.8357499837875366, 0.8356249928474426, 0.8357499837875366, 0.8358749747276306, 0.8355000019073486, 0.8356249928474426, 0.8352500200271606, 0.8361250162124634, 0.8357499837875366, 0.8360000252723694, 0.8361250162124634, 0.8356249928474426, 0.8355000019073486, 0.8363749980926514, 0.8361250162124634, 0.8356249928474426, 0.8361250162124634, 0.8364999890327454, 0.8361250162124634, 0.8366249799728394, 0.8367499709129333, 0.8366249799728394, 0.8361250162124634, 0.8367499709129333, 0.8368750214576721, 0.8368750214576721, 0.8367499709129333, 0.8360000252723694, 0.8363749980926514, 0.8368750214576721, 0.8368750214576721, 0.8363749980926514, 0.8367499709129333, 0.8372499942779541, 0.8366249799728394, 0.8368750214576721, 0.8364999890327454, 0.8373749852180481, 0.8371250033378601, 0.8368750214576721, 0.8371250033378601, 0.8372499942779541, 0.8368750214576721, 0.8371250033378601, 0.8370000123977661, 0.8370000123977661, 0.8371250033378601, 0.8373749852180481, 0.8368750214576721, 0.8366249799728394, 0.8368750214576721, 0.8373749852180481, 0.8370000123977661, 0.8368750214576721, 0.8370000123977661, 0.8373749852180481, 0.8370000123977661, 0.8372499942779541, 0.8372499942779541, 0.8370000123977661, 0.8373749852180481, 0.8371250033378601, 0.8366249799728394, 0.8372499942779541, 0.8376250267028809, 0.8371250033378601, 0.8374999761581421, 0.8371250033378601, 0.8374999761581421, 0.8367499709129333, 0.8371250033378601, 0.8368750214576721, 0.8376250267028809, 0.8368750214576721, 0.8374999761581421, 0.8373749852180481, 0.8368750214576721, 0.8367499709129333, 0.8363749980926514, 0.8368750214576721, 0.8370000123977661, 0.8371250033378601, 0.8373749852180481, 0.8372499942779541, 0.8372499942779541, 0.8367499709129333, 0.8368750214576721, 0.8370000123977661, 0.8371250033378601, 0.8371250033378601, 0.8373749852180481, 0.8367499709129333, 0.8373749852180481, 0.8372499942779541, 0.8371250033378601, 0.8374999761581421, 0.8371250033378601, 0.8373749852180481, 0.8373749852180481, 0.8370000123977661, 0.8376250267028809, 0.8374999761581421, 0.8374999761581421, 0.8373749852180481, 0.8372499942779541, 0.8373749852180481, 0.8374999761581421, 0.8371250033378601, 0.8372499942779541, 0.8373749852180481, 0.8371250033378601, 0.8376250267028809, 0.8373749852180481, 0.8372499942779541, 0.8368750214576721, 0.8376250267028809, 0.8367499709129333, 0.8367499709129333, 0.8372499942779541, 0.8370000123977661, 0.8371250033378601, 0.8372499942779541, 0.8370000123977661, 0.8374999761581421, 0.8373749852180481, 0.8376250267028809, 0.8371250033378601, 0.8374999761581421, 0.8374999761581421, 0.8376250267028809, 0.8371250033378601, 0.8370000123977661, 0.8377500176429749, 0.8381249904632568, 0.8374999761581421, 0.8377500176429749, 0.8372499942779541, 0.8374999761581421, 0.8371250033378601, 0.8371250033378601, 0.8374999761581421, 0.8376250267028809, 0.8377500176429749, 0.8376250267028809, 0.8374999761581421, 0.8373749852180481, 0.8377500176429749, 0.8370000123977661, 0.8387500047683716, 0.8379999995231628, 0.8382499814033508, 0.8374999761581421, 0.8378750085830688, 0.8379999995231628, 0.8381249904632568, 0.8381249904632568, 0.8382499814033508, 0.8373749852180481, 0.8379999995231628, 0.8374999761581421, 0.8381249904632568, 0.8376250267028809, 0.8386250138282776, 0.8378750085830688, 0.8381249904632568, 0.8379999995231628, 0.8382499814033508, 0.8382499814033508, 0.8381249904632568, 0.8372499942779541, 0.8381249904632568, 0.8381249904632568, 0.8383749723434448, 0.8381249904632568, 0.8381249904632568, 0.8382499814033508, 0.8385000228881836, 0.8383749723434448, 0.8372499942779541, 0.8382499814033508, 0.8382499814033508, 0.8377500176429749, 0.8379999995231628, 0.8381249904632568, 0.8383749723434448, 0.8386250138282776, 0.8381249904632568, 0.8381249904632568, 0.8381249904632568, 0.8383749723434448, 0.8387500047683716, 0.8376250267028809, 0.8383749723434448, 0.8383749723434448, 0.8386250138282776, 0.8371250033378601, 0.8382499814033508, 0.8382499814033508, 0.8377500176429749, 0.8386250138282776, 0.8381249904632568, 0.8377500176429749, 0.8385000228881836, 0.8379999995231628, 0.8386250138282776, 0.8377500176429749, 0.8382499814033508, 0.8377500176429749, 0.8376250267028809, 0.8382499814033508, 0.8383749723434448, 0.8383749723434448, 0.8382499814033508, 0.8379999995231628, 0.8381249904632568, 0.8379999995231628, 0.8386250138282776, 0.8379999995231628, 0.8374999761581421, 0.8377500176429749, 0.8379999995231628, 0.8383749723434448, 0.8377500176429749, 0.8374999761581421, 0.8382499814033508, 0.8381249904632568, 0.8385000228881836, 0.8377500176429749, 0.8385000228881836, 0.8374999761581421, 0.8381249904632568, 0.8382499814033508, 0.8374999761581421, 0.8377500176429749, 0.8383749723434448, 0.8377500176429749, 0.8379999995231628, 0.8379999995231628, 0.8379999995231628, 0.8377500176429749, 0.8379999995231628, 0.8387500047683716, 0.8379999995231628, 0.8374999761581421, 0.8381249904632568, 0.8374999761581421, 0.8383749723434448, 0.8378750085830688, 0.8381249904632568, 0.8378750085830688, 0.8378750085830688, 0.8377500176429749, 0.8379999995231628, 0.8373749852180481, 0.8376250267028809, 0.8379999995231628, 0.8377500176429749, 0.8379999995231628, 0.8382499814033508, 0.8379999995231628, 0.8377500176429749, 0.8377500176429749, 0.8378750085830688, 0.8378750085830688, 0.8378750085830688, 0.8379999995231628, 0.8376250267028809, 0.8373749852180481, 0.8379999995231628, 0.8376250267028809, 0.8381249904632568, 0.8376250267028809, 0.8374999761581421, 0.8376250267028809, 0.8376250267028809, 0.8377500176429749, 0.8376250267028809, 0.8378750085830688, 0.8377500176429749, 0.8377500176429749, 0.8377500176429749, 0.8378750085830688, 0.8383749723434448, 0.8379999995231628, 0.8374999761581421, 0.8379999995231628, 0.8377500176429749, 0.8378750085830688, 0.8378750085830688, 0.8382499814033508, 0.8373749852180481, 0.8372499942779541, 0.8377500176429749, 0.8378750085830688, 0.8377500176429749, 0.8379999995231628, 0.8377500176429749, 0.8377500176429749, 0.8381249904632568, 0.8374999761581421, 0.8374999761581421, 0.8377500176429749, 0.8379999995231628, 0.8377500176429749, 0.8379999995231628, 0.8379999995231628, 0.8381249904632568, 0.8381249904632568, 0.8377500176429749, 0.8372499942779541, 0.8378750085830688, 0.8378750085830688, 0.8376250267028809, 0.8376250267028809, 0.8382499814033508, 0.8376250267028809, 0.8377500176429749, 0.8381249904632568, 0.8379999995231628, 0.8381249904632568, 0.8378750085830688, 0.8379999995231628, 0.8373749852180481, 0.8381249904632568, 0.8379999995231628, 0.8376250267028809, 0.8377500176429749, 0.8376250267028809, 0.8379999995231628, 0.8378750085830688, 0.8374999761581421, 0.8382499814033508, 0.8376250267028809, 0.8376250267028809, 0.8379999995231628, 0.8381249904632568, 0.8377500176429749, 0.8381249904632568, 0.8378750085830688, 0.8376250267028809, 0.8377500176429749, 0.8381249904632568, 0.8373749852180481, 0.8377500176429749, 0.8377500176429749, 0.8374999761581421, 0.8377500176429749, 0.8377500176429749, 0.8379999995231628, 0.8376250267028809, 0.8379999995231628]\n",
      "[0.9664573073387146, 0.8834163546562195, 0.8147804737091064, 0.7583789825439453, 0.7121374011039734, 0.6743095517158508, 0.6433326601982117, 0.6180149912834167, 0.5972847938537598, 0.5803542137145996, 0.5664627552032471, 0.5550926327705383, 0.5457271337509155, 0.537981390953064, 0.5315383076667786, 0.5261492729187012, 0.5216004848480225, 0.5176578164100647, 0.5142293572425842, 0.511148989200592, 0.508350670337677, 0.5057458877563477, 0.5032765865325928, 0.5009046792984009, 0.49858081340789795, 0.49628815054893494, 0.4940042793750763, 0.49172621965408325, 0.48944205045700073, 0.48714545369148254, 0.48483431339263916, 0.4825003147125244, 0.48014339804649353, 0.47773563861846924, 0.475281298160553, 0.4727253019809723, 0.4700261652469635, 0.4671160578727722, 0.4640021324157715, 0.4606627821922302, 0.45726990699768066, 0.45387977361679077, 0.4506211578845978, 0.44761449098587036, 0.4448416233062744, 0.4423775374889374, 0.4401668608188629, 0.43818721175193787, 0.4364931285381317, 0.4349875748157501, 0.4336899518966675, 0.4325675666332245, 0.43157726526260376, 0.43072012066841125, 0.42996615171432495, 0.42929571866989136, 0.4287007451057434, 0.4281453490257263, 0.42760950326919556, 0.42713621258735657, 0.42664802074432373, 0.42615148425102234, 0.42565232515335083, 0.42512038350105286, 0.4245702624320984, 0.42402124404907227, 0.4233991205692291, 0.42276623845100403, 0.42213165760040283, 0.42150789499282837, 0.42084160447120667, 0.42017731070518494, 0.41955751180648804, 0.41894710063934326, 0.41838741302490234, 0.41782400012016296, 0.417277455329895, 0.41675540804862976, 0.41631463170051575, 0.4158654510974884, 0.4154451787471771, 0.4150572717189789, 0.41468173265457153, 0.41434627771377563, 0.41401875019073486, 0.41367796063423157, 0.4133654832839966, 0.41309869289398193, 0.4127945899963379, 0.41253501176834106, 0.41229382157325745, 0.412048876285553, 0.411862850189209, 0.4116130471229553, 0.41140398383140564, 0.41118305921554565, 0.4110397398471832, 0.4108404815196991, 0.41068151593208313, 0.410529226064682, 0.41031166911125183, 0.41015827655792236, 0.4100300967693329, 0.40987035632133484, 0.40973934531211853, 0.40963515639305115, 0.4094909727573395, 0.40939149260520935, 0.40924710035324097, 0.40914198756217957, 0.409040629863739, 0.40892329812049866, 0.40883252024650574, 0.40877315402030945, 0.4086056649684906, 0.40855732560157776, 0.4084646701812744, 0.40837496519088745, 0.40827885270118713, 0.4082193672657013, 0.4081047475337982, 0.40804529190063477, 0.40795063972473145, 0.4078962802886963, 0.40784478187561035, 0.40775537490844727, 0.40769198536872864, 0.4076208770275116, 0.407564640045166, 0.4075213372707367, 0.40746352076530457, 0.4074021577835083, 0.4073484539985657, 0.4073002338409424, 0.40726521611213684, 0.40721216797828674, 0.40715187788009644, 0.4070667624473572, 0.40703538060188293, 0.40701860189437866, 0.40697479248046875, 0.4069055914878845, 0.40687066316604614, 0.406867653131485, 0.4067932963371277, 0.4067528545856476, 0.40671592950820923, 0.4067115783691406, 0.4066309332847595, 0.40660783648490906, 0.4065716862678528, 0.4065212607383728, 0.40652769804000854, 0.4064703583717346, 0.4064468443393707, 0.4064279794692993, 0.4064120054244995, 0.4063745141029358, 0.40631571412086487, 0.4063643217086792, 0.40627190470695496, 0.4062584340572357, 0.40623176097869873, 0.4062325358390808, 0.4061751365661621, 0.40613362193107605, 0.4061460793018341, 0.406110942363739, 0.4060797095298767, 0.4060590863227844, 0.4060624837875366, 0.4060039222240448, 0.40599191188812256, 0.40603455901145935, 0.40598195791244507, 0.4059002697467804, 0.40592387318611145, 0.4059142768383026, 0.4058583676815033, 0.4058528542518616, 0.40584319829940796, 0.40584197640419006, 0.40583565831184387, 0.40578457713127136, 0.4057498574256897, 0.4057410955429077, 0.40573734045028687, 0.4057237505912781, 0.40569308400154114, 0.4056352972984314, 0.4056744873523712, 0.4056425094604492, 0.4056567847728729, 0.40560445189476013, 0.40561506152153015, 0.4056006073951721, 0.4055657982826233, 0.40554624795913696, 0.40557870268821716, 0.4054951071739197, 0.4054897427558899, 0.40546277165412903, 0.4054717719554901, 0.405516654253006, 0.405452698469162, 0.4054759442806244, 0.40538787841796875, 0.40542611479759216, 0.4053996205329895, 0.4053744673728943, 0.40536877512931824, 0.40535756945610046, 0.4053323566913605, 0.40531715750694275, 0.40532922744750977, 0.40530771017074585, 0.4052805006504059, 0.4053201377391815, 0.40526852011680603, 0.4052215814590454, 0.4052523672580719, 0.40521174669265747, 0.4052126109600067, 0.40518009662628174, 0.4052101969718933, 0.405173659324646, 0.4051528871059418, 0.4052034616470337, 0.40516412258148193, 0.40514039993286133, 0.40511277318000793, 0.405100554227829, 0.4051186740398407, 0.4050711691379547, 0.40506356954574585, 0.4051041007041931, 0.405082106590271, 0.4050699770450592, 0.4050653874874115, 0.4050327241420746, 0.4050196409225464, 0.40508002042770386, 0.4050315022468567, 0.40498101711273193, 0.40502020716667175, 0.4049595296382904, 0.40496566891670227, 0.4049570858478546, 0.40497955679893494, 0.40492257475852966, 0.40497928857803345, 0.40489086508750916, 0.40491366386413574, 0.40492159128189087, 0.4048973023891449, 0.4049277901649475, 0.4048960506916046, 0.4048636257648468, 0.4048813581466675, 0.40492773056030273, 0.40485879778862, 0.4048353433609009, 0.4048531651496887, 0.40486523509025574, 0.4048529267311096, 0.4048044681549072, 0.40484419465065, 0.4048231542110443, 0.4047941267490387, 0.40482574701309204, 0.4047453701496124, 0.4047732353210449, 0.40475210547447205, 0.4047849178314209, 0.404718279838562, 0.4047324061393738, 0.40473541617393494, 0.4047037959098816, 0.40472611784935, 0.4046904444694519, 0.4046807587146759, 0.40476194024086, 0.4046771228313446, 0.40466031432151794, 0.40471720695495605, 0.40469902753829956, 0.4046752452850342, 0.40467867255210876, 0.4046587646007538, 0.40465039014816284, 0.40465453267097473, 0.4046085774898529, 0.4046138823032379, 0.4046240746974945, 0.40462440252304077, 0.4045964181423187, 0.40467706322669983, 0.40461164712905884, 0.40462726354599, 0.40455833077430725, 0.4046474099159241, 0.40458759665489197, 0.40456637740135193, 0.4045637547969818, 0.4045732021331787, 0.40454116463661194, 0.4045359194278717, 0.4045592248439789, 0.4045582711696625, 0.4045127332210541, 0.404496431350708, 0.404508501291275, 0.4044903516769409, 0.4044595956802368, 0.40448054671287537, 0.404491126537323, 0.4044840633869171, 0.4045056402683258, 0.40450385212898254, 0.4044862389564514, 0.4044910967350006, 0.40446242690086365, 0.4044579863548279, 0.4044318199157715, 0.4044381380081177, 0.40445592999458313, 0.40441933274269104, 0.40448346734046936, 0.4044046103954315, 0.4044225513935089, 0.40438392758369446, 0.4044578969478607, 0.4044041037559509, 0.40446552634239197, 0.40440958738327026, 0.4044644236564636, 0.4043785035610199, 0.4043598473072052, 0.40439721941947937, 0.40436628460884094, 0.404384970664978, 0.40437108278274536, 0.4043925404548645, 0.40436944365501404, 0.4043586850166321, 0.4043654799461365, 0.4043498933315277, 0.40433669090270996, 0.40432414412498474, 0.404374897480011, 0.4042957127094269, 0.4043201804161072, 0.4042980968952179, 0.40430864691734314, 0.40427300333976746, 0.4043184816837311, 0.40427449345588684, 0.40435194969177246, 0.40427759289741516, 0.4042566418647766, 0.40424516797065735, 0.4042731821537018, 0.40421855449676514, 0.4042789340019226, 0.40425652265548706, 0.40424150228500366, 0.4042346775531769, 0.4042070209980011, 0.40427833795547485, 0.4042290449142456, 0.4042697846889496, 0.4042045772075653, 0.4041997194290161, 0.4042246639728546, 0.4042026996612549, 0.40419110655784607, 0.4041694104671478, 0.4041963815689087, 0.40419164299964905, 0.4042060375213623, 0.40421149134635925, 0.40419819951057434, 0.40420040488243103, 0.4042128622531891, 0.4041610062122345, 0.40421822667121887, 0.4041789472103119, 0.4041667878627777, 0.40418484807014465, 0.4041287899017334, 0.4041268229484558, 0.40418481826782227, 0.404144823551178, 0.40412864089012146, 0.4041835069656372, 0.4041930139064789, 0.40411219000816345, 0.40412476658821106, 0.40410834550857544, 0.40410730242729187, 0.4041489362716675, 0.40414705872535706, 0.4041188955307007, 0.4041065573692322, 0.4041266143321991, 0.4041486978530884, 0.4041077792644501, 0.40406540036201477, 0.40411376953125, 0.4040643274784088, 0.40406012535095215, 0.40407702326774597, 0.40406233072280884, 0.40415021777153015, 0.40408065915107727, 0.4040765166282654, 0.40408071875572205, 0.40407595038414, 0.40408286452293396, 0.404049813747406, 0.40407076478004456, 0.4040481448173523, 0.40402674674987793, 0.40403130650520325, 0.4040432274341583, 0.40404146909713745, 0.40406671166419983, 0.40402019023895264, 0.4039909541606903, 0.4040553867816925, 0.4040319323539734, 0.4040207266807556, 0.4040224254131317, 0.40401536226272583, 0.40397438406944275, 0.4040031433105469, 0.40404099225997925, 0.40398526191711426, 0.4040021300315857, 0.4039970338344574, 0.4040101170539856, 0.4039982557296753, 0.40397024154663086, 0.40401366353034973, 0.4040270745754242, 0.40396761894226074, 0.4040040671825409, 0.40398067235946655, 0.4039578437805176, 0.40395671129226685, 0.4039459824562073, 0.4039503037929535, 0.4039977788925171, 0.4039855897426605, 0.40391406416893005, 0.40395352244377136, 0.40392550826072693, 0.40394026041030884, 0.40402108430862427, 0.40393346548080444, 0.40389999747276306, 0.40396133065223694, 0.40397924184799194, 0.403914213180542, 0.40396153926849365, 0.4039147198200226, 0.4039577543735504, 0.403931587934494, 0.4039289653301239, 0.4039165675640106, 0.40393033623695374, 0.4039398431777954, 0.4039165675640106, 0.4039117097854614, 0.40393292903900146, 0.40392133593559265, 0.40393075346946716, 0.40387770533561707, 0.4039030373096466, 0.40388110280036926, 0.40392017364501953, 0.4039132595062256, 0.4039190113544464, 0.40389689803123474, 0.4038912355899811, 0.4039560854434967, 0.4038752317428589, 0.4038637578487396, 0.40391796827316284, 0.40390411019325256, 0.4038597643375397, 0.40389007329940796, 0.4038793742656708, 0.40390339493751526, 0.40391451120376587, 0.4038366973400116, 0.40384289622306824, 0.4038991630077362, 0.4038347601890564, 0.40391677618026733]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(history.history['accuracy'])\n",
    "print(history.history['loss'])\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy and loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['accuracy', 'loss'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c57e988",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Below are the weights in the final iteration\n",
    "first_layer_weights = NNmodel.layers[0].get_weights()[0]\n",
    "first_layer_biases  = NNmodel.layers[0].get_weights()[1]\n",
    "second_layer_weights = NNmodel.layers[1].get_weights()[0]\n",
    "second_layer_biases  = NNmodel.layers[1].get_weights()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0f6f156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.12957409 -0.23395298]\n",
      " [-0.8011603  -0.14569512]\n",
      " [ 0.03298669 -0.14439125]\n",
      " [-0.06800221 -0.13349134]\n",
      " [ 0.47778416 -0.13762003]\n",
      " [-0.08550332  2.7046294 ]\n",
      " [ 0.01181852 -0.04670924]\n",
      " [-0.32131016  0.05471463]\n",
      " [ 0.16288659  0.01178513]\n",
      " [-0.04200388 -0.01793073]\n",
      " [ 2.2061067   1.3493848 ]\n",
      " [-0.06204733  0.02079621]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(12, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(first_layer_weights)\n",
    "first_layer_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f38be53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.630937   -0.09523977]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(first_layer_biases)\n",
    "first_layer_biases.shape  ### (2,) here basically means 2 elements in a 1-dim array. .T has no effect on 1d array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "306e8073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.1982512]\n",
      " [ 3.2680798]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(second_layer_weights)\n",
    "second_layer_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5587208e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.3811104]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(second_layer_biases)\n",
    "second_layer_biases.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a4a041b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.00150113 -0.58312392 -0.57273139 -1.55489968  0.91509065  0.10629772\n",
      "  -0.70174202 -0.26396987  0.80225696  0.64376017  0.97725852 -0.00249134]]\n"
     ]
    }
   ],
   "source": [
    "### Now we use the trained weights and biases to try to predict based on a new case\n",
    "tr=sc.transform([[1, 0, 0, 500, 1, 40, 3, 60000, 2, 1, 1, 100000]])\n",
    "print(tr)  ### tr.shape is (1,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86520ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 48ms/step\n",
      "[[0.1377649]]\n"
     ]
    }
   ],
   "source": [
    "### Example\n",
    "### Predicting result for Single Observation\n",
    "print(NNmodel.predict(tr))\n",
    "### note in each recompute -- this no. will change slightly because of the random initiation of the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e89bf0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.44816566, 1.63737071]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### now we compute the predicted prob of 1, manually\n",
    "tr.dot(first_layer_weights)  ### gives a 1 x 2 matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "223820b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.07910264 1.54213094]]\n"
     ]
    }
   ],
   "source": [
    "Flayerneurons_sum=tr.dot(first_layer_weights) + first_layer_biases\n",
    "print(Flayerneurons_sum)  ### 1 x 2 matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d7512768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.98335897 0.82377429]]\n"
     ]
    }
   ],
   "source": [
    "Flayerneurons_act=1/(1+np.exp(-Flayerneurons_sum))\n",
    "print(Flayerneurons_act)  ### 1 x 2 matrix -- output of neurons in hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6f7f5c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.83397939]]\n"
     ]
    }
   ],
   "source": [
    "Slayerneurons_sum=Flayerneurons_act.dot (second_layer_weights)+second_layer_biases\n",
    "print(Slayerneurons_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7960c6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1377649]]\n"
     ]
    }
   ],
   "source": [
    "predprob=1/(1+np.exp(-Slayerneurons_sum))\n",
    "print(predprob) ### Note this is the same output as NNmodel.predict(tr)\n",
    "### This manual computation of the forward pass should have output same as in NNmodel.predict(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "34a5ac4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 478us/step - loss: 0.4037 - accuracy: 0.8380\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.40366193652153015, 0.8379999995231628]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Now we use the trained NNmodel to predict output in X_train sample\n",
    "NNmodel.evaluate(X_train,Y_train)  ### evaluates the loss and accuracy as specified in the Compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5ba646c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 398us/step\n"
     ]
    }
   ],
   "source": [
    "### Now we use the trained NNmodel to predict output in X_train sample -- computing manually via .predict\n",
    "TE=NNmodel.predict(X_train)  ### note X_train has 8000 data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "82ce3372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TE.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f49c856f",
   "metadata": {},
   "outputs": [],
   "source": [
    "h=(TE > 0.5).astype(int) ### Convert TE>0.5 == true ==> 1, False to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bb651f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8000, 1)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(h)\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4a697768",
   "metadata": {},
   "outputs": [],
   "source": [
    "### replace all elements in numpy array of value 0 with value -1\n",
    "h[h==0]=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c9d41502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1]\n",
      " [-1]\n",
      " [-1]\n",
      " ...\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8000, 1)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(h)\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e8b950e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### replace all elements in numpy array of value 0 with value -1\n",
    "Y_train1=Y_train\n",
    "Y_train1[Y_train1==0]=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "53b50eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1 -1  1 ...  1 -1  1]\n"
     ]
    }
   ],
   "source": [
    "print(Y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ec6af287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000,)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fe3de8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6704 0.838\n"
     ]
    }
   ],
   "source": [
    "J=np.multiply(Y_train1.T,h.T)  ### element by element multiplication\n",
    "c=np.count_nonzero(J > 0) \n",
    "print(c,c/8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6bfab32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 704us/step - loss: 0.4014 - accuracy: 0.8330\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.40139150619506836, 0.8330000042915344]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Now we use the trained NNmodel to predict output in X_test sample\n",
    "NNmodel.evaluate(X_test,Y_test)  ### evaluates the loss and accuracy as specified in the Compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f34f4bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 591us/step\n"
     ]
    }
   ],
   "source": [
    "### Now we use the trained NNmodel to predict output in X_test sample -- computing manually via .predict\n",
    "TE1=NNmodel.predict(X_test)  ### note X_test has 2000 data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3d186532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1666 0.833\n"
     ]
    }
   ],
   "source": [
    "h1=(TE1 > 0.5).astype(int) ### Convert TE1>0.5 == true ==> 1, False to 0\n",
    "h1[h1==0]=-1\n",
    "Y_test1=Y_test\n",
    "Y_test1[Y_test1==0]=-1\n",
    "J1=np.multiply(Y_test1.T,h1.T)  ### element by element multiplication\n",
    "c1=np.count_nonzero(J1 > 0) \n",
    "print(c1,c1/2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38db4fcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
