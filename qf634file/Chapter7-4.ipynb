{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1597b18f",
   "metadata": {},
   "source": [
    "This model follows Chapter7-3.ipynb except it uses SGD (stochastic descent method) instead of Adam in code line [12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "617ee135",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c6deff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Dataset\n",
    "data = pd.read_csv(\"Churn_Modelling.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a80540e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CreditScore Geography  Gender  Age  Tenure    Balance  NumOfProducts  \\\n",
       "0          619    France  Female   42       2       0.00              1   \n",
       "1          608     Spain  Female   41       1   83807.86              1   \n",
       "2          502    France  Female   42       8  159660.80              3   \n",
       "3          699    France  Female   39       1       0.00              2   \n",
       "4          850     Spain  Female   43       2  125510.82              1   \n",
       "\n",
       "   HasCrCard  IsActiveMember  EstimatedSalary  \n",
       "0          1               1        101348.88  \n",
       "1          0               1        112542.58  \n",
       "2          1               0        113931.57  \n",
       "3          0               0         93826.63  \n",
       "4          1               1         79084.10  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generating Dependent Variable Vectors\n",
    "Y = data.iloc[:,-1].values\n",
    "X = data.iloc[:,3:13]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a98c381c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0\n",
      "1       0\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "       ..\n",
      "9995    1\n",
      "9996    1\n",
      "9997    0\n",
      "9998    1\n",
      "9999    0\n",
      "Name: Gender, Length: 10000, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Generating Dependent Variable Vectors\n",
    "Y = data.iloc[:,-1].values\n",
    "X = data.iloc[:,3:13]\n",
    "X['Gender']=X['Gender'].map({'Female':0,'Male':1})\n",
    "### above is used instead of a more complicated package involving -- from sklearn.preprocessing import LabelEncoder\n",
    "### converts Female -- 0, Male -- 1, i.e. hot-encoding categorical variables\n",
    "print (X['Gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e436f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding Categorical variable Geography\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ct =ColumnTransformer(transformers=[('encoder',OneHotEncoder(),[1])],remainder=\"passthrough\")\n",
    "X = np.array(ct.fit_transform(X))\n",
    "### Geography is transformed into France -- 1,0,0; Spain -- 0,0,1; Germany -- 0,1,0.\n",
    "### Moreover -- this encoded vector of ones-zeros is now put in first 3 cols. Credit Score pushed to 4th col."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3166a509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>619.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101348.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>608.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>112542.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>502.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>113931.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>699.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93826.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>850.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>79084.10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0    1    2      3    4     5    6          7    8    9    10         11\n",
       "0  1.0  0.0  0.0  619.0  0.0  42.0  2.0       0.00  1.0  1.0  1.0  101348.88\n",
       "1  0.0  0.0  1.0  608.0  0.0  41.0  1.0   83807.86  1.0  0.0  1.0  112542.58\n",
       "2  1.0  0.0  0.0  502.0  0.0  42.0  8.0  159660.80  3.0  1.0  0.0  113931.57\n",
       "3  1.0  0.0  0.0  699.0  0.0  39.0  1.0       0.00  2.0  0.0  0.0   93826.63\n",
       "4  0.0  0.0  1.0  850.0  0.0  43.0  2.0  125510.82  1.0  1.0  1.0   79084.10"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### convert X to dataframe X1\n",
    "X1 = pd.DataFrame(X)\n",
    "X1.head()\n",
    "### Note there are 12 features including onehotencoder for the Geography feature-- \n",
    "### The features are encoded using a one-hot (aka ‘one-of-K’ or ‘dummy’) encoding scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ef915ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting dataset into training and testing dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "740ede9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9850b84b",
   "metadata": {},
   "source": [
    "We call fit_transform() method on our training data and transform() method on our test data. Each feature in the training\n",
    "set is scaled to mean 0, variance 1. In sklearn.preprocessing.StandardScaler(), centering and scaling happens independently on each feature. The fit method is calculating the mean and variance of each of the features present in the data. The transform method is transforming all the features using the respective feature's mean and variance that are calculated in the statement\n",
    "before on X_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f43f1db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### This is the very first step while creating NNmodel. Here we are going to create our ann object by using a certain class of Keras \n",
    "### named Sequential. As a part of tensorflow 2.0, Keras is now integrated with tensorflow and is now considered as a \n",
    "### sub-library of tensorflow. The Sequential class is a part of the models module of Keras library which is a part of the \n",
    "### tensorflow library now. \n",
    "### It used to be \"import tensorflow as tf; from tensorflow import keras; from tensorflow.keras import layers\"\n",
    "### See documentation at https://keras.io/guides/sequential_model/\n",
    "\n",
    "#Initialising the NN model name -- NNmodel\n",
    "NNmodel = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34730cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating a network that has 1 hidden layer together with 1 input layer and 1 output layer. \n",
    "#Adding First Hidden Layer\n",
    "NNmodel.add(tf.keras.layers.Dense(units=2,activation=\"sigmoid\"))\n",
    "### units = 2 refer to 2 neurons in hidden layer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab12edcf",
   "metadata": {},
   "source": [
    "Above -- first hidden layer is created using the Dense class which is part of the layers module. This class accepts 2 inputs:-\n",
    "(1) units:- number of neurons that will be present in the respective layer (2) activation:- specify which activation function to be used. This example uses first input as 2. There is no correct answer which is the right number of neurons in the layer -- trial and error. Not too large to be computationally impractical or redundant; not too small to be ineffective.\n",
    "For the second input, we try the sigmoid or logistic function as an activation function for hidden layers. We can also try “relu”[rectified linear unit]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "303997d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### now we create the output layer\n",
    "#Adding Output Layer\n",
    "NNmodel.add(tf.keras.layers.Dense(units=1,activation=\"sigmoid\"))\n",
    "### Only 1 output neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b4a623",
   "metadata": {},
   "source": [
    "For a binary classification problem as above, actual case output is 1 or 0. Hence we require only one neuron to output layer - output could be estimated probability of case actual output = 1. For multiclass classification problem, if the output contains m categories then we need to create m different neurons, one for each category. In the binary output case, the suitable activation function is the sigmoid function. For multiclass classification problem, the activation function is typically softmax. The softmax function predicts a multinomial probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70baaa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "### After creating the layers -- require compiling the NNmodel. Compiling allows the computer to run and understand the program \n",
    "### without the need of more fundamental steps in the programming. Compiling adds other elements or linking other libraries, and optimization,\n",
    "### such that after compiling the results are readily computed e.g. in a binary executable program as an output. \n",
    "#Compiling NNmodel\n",
    "NNmodel.compile(optimizer=\"SGD\",loss=\"binary_crossentropy\",metrics=['accuracy'])\n",
    "### Note optimizer here is a more sophisticated version of the Mean Square loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f9d245",
   "metadata": {},
   "source": [
    "Compile method above accepts inputs: (1) optimizer:- specifies which optimizer to be used in order to perform stochastic gradient descent (2) error/loss function, e.g., 'binary_crossentropy' here. For multiclass classification, it should be categorical_crossentropy, (3) metrics - the performance metrics to use in order to compute performance. 'accuracy' is one such  performance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51697cc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 12)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36a93b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "80/80 [==============================] - 0s 693us/step - loss: 0.5776 - accuracy: 0.7969\n",
      "Epoch 2/500\n",
      "80/80 [==============================] - 0s 560us/step - loss: 0.5523 - accuracy: 0.7972\n",
      "Epoch 3/500\n",
      "80/80 [==============================] - 0s 597us/step - loss: 0.5369 - accuracy: 0.7972\n",
      "Epoch 4/500\n",
      "80/80 [==============================] - 0s 530us/step - loss: 0.5270 - accuracy: 0.7972\n",
      "Epoch 5/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.5204 - accuracy: 0.7972\n",
      "Epoch 6/500\n",
      "80/80 [==============================] - 0s 545us/step - loss: 0.5157 - accuracy: 0.7972\n",
      "Epoch 7/500\n",
      "80/80 [==============================] - 0s 510us/step - loss: 0.5123 - accuracy: 0.7972\n",
      "Epoch 8/500\n",
      "80/80 [==============================] - 0s 507us/step - loss: 0.5095 - accuracy: 0.7972\n",
      "Epoch 9/500\n",
      "80/80 [==============================] - 0s 509us/step - loss: 0.5073 - accuracy: 0.7972\n",
      "Epoch 10/500\n",
      "80/80 [==============================] - 0s 507us/step - loss: 0.5053 - accuracy: 0.7972\n",
      "Epoch 11/500\n",
      "80/80 [==============================] - 0s 516us/step - loss: 0.5036 - accuracy: 0.7972\n",
      "Epoch 12/500\n",
      "80/80 [==============================] - 0s 501us/step - loss: 0.5021 - accuracy: 0.7972\n",
      "Epoch 13/500\n",
      "80/80 [==============================] - 0s 498us/step - loss: 0.5006 - accuracy: 0.7972\n",
      "Epoch 14/500\n",
      "80/80 [==============================] - 0s 507us/step - loss: 0.4993 - accuracy: 0.7972\n",
      "Epoch 15/500\n",
      "80/80 [==============================] - 0s 499us/step - loss: 0.4979 - accuracy: 0.7972\n",
      "Epoch 16/500\n",
      "80/80 [==============================] - 0s 506us/step - loss: 0.4967 - accuracy: 0.7972\n",
      "Epoch 17/500\n",
      "80/80 [==============================] - 0s 498us/step - loss: 0.4955 - accuracy: 0.7972\n",
      "Epoch 18/500\n",
      "80/80 [==============================] - 0s 520us/step - loss: 0.4943 - accuracy: 0.7972\n",
      "Epoch 19/500\n",
      "80/80 [==============================] - 0s 505us/step - loss: 0.4932 - accuracy: 0.7972\n",
      "Epoch 20/500\n",
      "80/80 [==============================] - 0s 513us/step - loss: 0.4920 - accuracy: 0.7972\n",
      "Epoch 21/500\n",
      "80/80 [==============================] - 0s 506us/step - loss: 0.4909 - accuracy: 0.7972\n",
      "Epoch 22/500\n",
      "80/80 [==============================] - 0s 493us/step - loss: 0.4899 - accuracy: 0.7972\n",
      "Epoch 23/500\n",
      "80/80 [==============================] - 0s 512us/step - loss: 0.4888 - accuracy: 0.7972\n",
      "Epoch 24/500\n",
      "80/80 [==============================] - 0s 498us/step - loss: 0.4878 - accuracy: 0.7972\n",
      "Epoch 25/500\n",
      "80/80 [==============================] - 0s 507us/step - loss: 0.4868 - accuracy: 0.7972\n",
      "Epoch 26/500\n",
      "80/80 [==============================] - 0s 497us/step - loss: 0.4858 - accuracy: 0.7972\n",
      "Epoch 27/500\n",
      "80/80 [==============================] - 0s 521us/step - loss: 0.4848 - accuracy: 0.7972\n",
      "Epoch 28/500\n",
      "80/80 [==============================] - 0s 509us/step - loss: 0.4839 - accuracy: 0.7972\n",
      "Epoch 29/500\n",
      "80/80 [==============================] - 0s 520us/step - loss: 0.4829 - accuracy: 0.7972\n",
      "Epoch 30/500\n",
      "80/80 [==============================] - 0s 498us/step - loss: 0.4820 - accuracy: 0.7972\n",
      "Epoch 31/500\n",
      "80/80 [==============================] - 0s 505us/step - loss: 0.4811 - accuracy: 0.7972\n",
      "Epoch 32/500\n",
      "80/80 [==============================] - 0s 486us/step - loss: 0.4802 - accuracy: 0.7972\n",
      "Epoch 33/500\n",
      "80/80 [==============================] - 0s 559us/step - loss: 0.4793 - accuracy: 0.7972\n",
      "Epoch 34/500\n",
      "80/80 [==============================] - 0s 658us/step - loss: 0.4784 - accuracy: 0.7972\n",
      "Epoch 35/500\n",
      "80/80 [==============================] - 0s 569us/step - loss: 0.4776 - accuracy: 0.7972\n",
      "Epoch 36/500\n",
      "80/80 [==============================] - 0s 545us/step - loss: 0.4767 - accuracy: 0.7972\n",
      "Epoch 37/500\n",
      "80/80 [==============================] - 0s 548us/step - loss: 0.4759 - accuracy: 0.7972\n",
      "Epoch 38/500\n",
      "80/80 [==============================] - 0s 532us/step - loss: 0.4751 - accuracy: 0.7972\n",
      "Epoch 39/500\n",
      "80/80 [==============================] - 0s 532us/step - loss: 0.4742 - accuracy: 0.7972\n",
      "Epoch 40/500\n",
      "80/80 [==============================] - 0s 544us/step - loss: 0.4734 - accuracy: 0.7972\n",
      "Epoch 41/500\n",
      "80/80 [==============================] - 0s 573us/step - loss: 0.4726 - accuracy: 0.7972\n",
      "Epoch 42/500\n",
      "80/80 [==============================] - 0s 608us/step - loss: 0.4719 - accuracy: 0.7972\n",
      "Epoch 43/500\n",
      "80/80 [==============================] - 0s 559us/step - loss: 0.4711 - accuracy: 0.7972\n",
      "Epoch 44/500\n",
      "80/80 [==============================] - 0s 544us/step - loss: 0.4703 - accuracy: 0.7972\n",
      "Epoch 45/500\n",
      "80/80 [==============================] - 0s 545us/step - loss: 0.4696 - accuracy: 0.7972\n",
      "Epoch 46/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.4688 - accuracy: 0.7972\n",
      "Epoch 47/500\n",
      "80/80 [==============================] - 0s 534us/step - loss: 0.4681 - accuracy: 0.7972\n",
      "Epoch 48/500\n",
      "80/80 [==============================] - 0s 582us/step - loss: 0.4674 - accuracy: 0.7972\n",
      "Epoch 49/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.4666 - accuracy: 0.7972\n",
      "Epoch 50/500\n",
      "80/80 [==============================] - 0s 582us/step - loss: 0.4659 - accuracy: 0.7972\n",
      "Epoch 51/500\n",
      "80/80 [==============================] - 0s 595us/step - loss: 0.4652 - accuracy: 0.7972\n",
      "Epoch 52/500\n",
      "80/80 [==============================] - 0s 545us/step - loss: 0.4645 - accuracy: 0.7972\n",
      "Epoch 53/500\n",
      "80/80 [==============================] - 0s 535us/step - loss: 0.4639 - accuracy: 0.7972\n",
      "Epoch 54/500\n",
      "80/80 [==============================] - 0s 546us/step - loss: 0.4632 - accuracy: 0.7972\n",
      "Epoch 55/500\n",
      "80/80 [==============================] - 0s 544us/step - loss: 0.4625 - accuracy: 0.7972\n",
      "Epoch 56/500\n",
      "80/80 [==============================] - 0s 555us/step - loss: 0.4619 - accuracy: 0.7972\n",
      "Epoch 57/500\n",
      "80/80 [==============================] - 0s 537us/step - loss: 0.4612 - accuracy: 0.7972\n",
      "Epoch 58/500\n",
      "80/80 [==============================] - 0s 544us/step - loss: 0.4606 - accuracy: 0.7972\n",
      "Epoch 59/500\n",
      "80/80 [==============================] - 0s 577us/step - loss: 0.4599 - accuracy: 0.7972\n",
      "Epoch 60/500\n",
      "80/80 [==============================] - 0s 553us/step - loss: 0.4593 - accuracy: 0.7972\n",
      "Epoch 61/500\n",
      "80/80 [==============================] - 0s 561us/step - loss: 0.4587 - accuracy: 0.7972\n",
      "Epoch 62/500\n",
      "80/80 [==============================] - 0s 504us/step - loss: 0.4581 - accuracy: 0.7972\n",
      "Epoch 63/500\n",
      "80/80 [==============================] - 0s 531us/step - loss: 0.4575 - accuracy: 0.7972\n",
      "Epoch 64/500\n",
      "80/80 [==============================] - 0s 523us/step - loss: 0.4569 - accuracy: 0.7972\n",
      "Epoch 65/500\n",
      "80/80 [==============================] - 0s 519us/step - loss: 0.4563 - accuracy: 0.7972\n",
      "Epoch 66/500\n",
      "80/80 [==============================] - 0s 498us/step - loss: 0.4557 - accuracy: 0.7972\n",
      "Epoch 67/500\n",
      "80/80 [==============================] - 0s 506us/step - loss: 0.4552 - accuracy: 0.7972\n",
      "Epoch 68/500\n",
      "80/80 [==============================] - 0s 510us/step - loss: 0.4546 - accuracy: 0.7972\n",
      "Epoch 69/500\n",
      "80/80 [==============================] - 0s 495us/step - loss: 0.4541 - accuracy: 0.7972\n",
      "Epoch 70/500\n",
      "80/80 [==============================] - 0s 524us/step - loss: 0.4535 - accuracy: 0.7972\n",
      "Epoch 71/500\n",
      "80/80 [==============================] - 0s 519us/step - loss: 0.4530 - accuracy: 0.7972\n",
      "Epoch 72/500\n",
      "80/80 [==============================] - 0s 493us/step - loss: 0.4525 - accuracy: 0.7972\n",
      "Epoch 73/500\n",
      "80/80 [==============================] - 0s 511us/step - loss: 0.4519 - accuracy: 0.7972\n",
      "Epoch 74/500\n",
      "80/80 [==============================] - 0s 494us/step - loss: 0.4514 - accuracy: 0.7972\n",
      "Epoch 75/500\n",
      "80/80 [==============================] - 0s 511us/step - loss: 0.4509 - accuracy: 0.7972\n",
      "Epoch 76/500\n",
      "80/80 [==============================] - 0s 507us/step - loss: 0.4504 - accuracy: 0.7972\n",
      "Epoch 77/500\n",
      "80/80 [==============================] - 0s 500us/step - loss: 0.4499 - accuracy: 0.7972\n",
      "Epoch 78/500\n",
      "80/80 [==============================] - 0s 504us/step - loss: 0.4495 - accuracy: 0.7972\n",
      "Epoch 79/500\n",
      "80/80 [==============================] - 0s 512us/step - loss: 0.4490 - accuracy: 0.7972\n",
      "Epoch 80/500\n",
      "80/80 [==============================] - 0s 505us/step - loss: 0.4485 - accuracy: 0.7972\n",
      "Epoch 81/500\n",
      "80/80 [==============================] - 0s 510us/step - loss: 0.4481 - accuracy: 0.7972\n",
      "Epoch 82/500\n",
      "80/80 [==============================] - 0s 494us/step - loss: 0.4476 - accuracy: 0.7972\n",
      "Epoch 83/500\n",
      "80/80 [==============================] - 0s 499us/step - loss: 0.4472 - accuracy: 0.7972\n",
      "Epoch 84/500\n",
      "80/80 [==============================] - 0s 497us/step - loss: 0.4468 - accuracy: 0.7972\n",
      "Epoch 85/500\n",
      "80/80 [==============================] - 0s 495us/step - loss: 0.4463 - accuracy: 0.7972\n",
      "Epoch 86/500\n",
      "80/80 [==============================] - 0s 497us/step - loss: 0.4459 - accuracy: 0.7972\n",
      "Epoch 87/500\n",
      "80/80 [==============================] - 0s 520us/step - loss: 0.4455 - accuracy: 0.7972\n",
      "Epoch 88/500\n",
      "80/80 [==============================] - 0s 507us/step - loss: 0.4451 - accuracy: 0.7972\n",
      "Epoch 89/500\n",
      "80/80 [==============================] - 0s 498us/step - loss: 0.4447 - accuracy: 0.7972\n",
      "Epoch 90/500\n",
      "80/80 [==============================] - 0s 506us/step - loss: 0.4443 - accuracy: 0.7972\n",
      "Epoch 91/500\n",
      "80/80 [==============================] - 0s 512us/step - loss: 0.4439 - accuracy: 0.7972\n",
      "Epoch 92/500\n",
      "80/80 [==============================] - 0s 569us/step - loss: 0.4436 - accuracy: 0.7972\n",
      "Epoch 93/500\n",
      "80/80 [==============================] - 0s 498us/step - loss: 0.4432 - accuracy: 0.7972\n",
      "Epoch 94/500\n",
      "80/80 [==============================] - 0s 519us/step - loss: 0.4428 - accuracy: 0.7972\n",
      "Epoch 95/500\n",
      "80/80 [==============================] - 0s 523us/step - loss: 0.4425 - accuracy: 0.7972\n",
      "Epoch 96/500\n",
      "80/80 [==============================] - 0s 507us/step - loss: 0.4421 - accuracy: 0.7972\n",
      "Epoch 97/500\n",
      "80/80 [==============================] - 0s 520us/step - loss: 0.4418 - accuracy: 0.7972\n",
      "Epoch 98/500\n",
      "80/80 [==============================] - 0s 511us/step - loss: 0.4415 - accuracy: 0.7972\n",
      "Epoch 99/500\n",
      "80/80 [==============================] - 0s 507us/step - loss: 0.4411 - accuracy: 0.7972\n",
      "Epoch 100/500\n",
      "80/80 [==============================] - 0s 509us/step - loss: 0.4408 - accuracy: 0.7972\n",
      "Epoch 101/500\n",
      "80/80 [==============================] - 0s 494us/step - loss: 0.4405 - accuracy: 0.7972\n",
      "Epoch 102/500\n",
      "80/80 [==============================] - 0s 510us/step - loss: 0.4402 - accuracy: 0.7972\n",
      "Epoch 103/500\n",
      "80/80 [==============================] - 0s 520us/step - loss: 0.4399 - accuracy: 0.7972\n",
      "Epoch 104/500\n",
      "80/80 [==============================] - 0s 499us/step - loss: 0.4396 - accuracy: 0.7972\n",
      "Epoch 105/500\n",
      "80/80 [==============================] - 0s 506us/step - loss: 0.4393 - accuracy: 0.7972\n",
      "Epoch 106/500\n",
      "80/80 [==============================] - 0s 498us/step - loss: 0.4390 - accuracy: 0.7972\n",
      "Epoch 107/500\n",
      "80/80 [==============================] - 0s 506us/step - loss: 0.4388 - accuracy: 0.7972\n",
      "Epoch 108/500\n",
      "80/80 [==============================] - 0s 510us/step - loss: 0.4385 - accuracy: 0.7972\n",
      "Epoch 109/500\n",
      "80/80 [==============================] - 0s 507us/step - loss: 0.4382 - accuracy: 0.7972\n",
      "Epoch 110/500\n",
      "80/80 [==============================] - 0s 530us/step - loss: 0.4380 - accuracy: 0.7972\n",
      "Epoch 111/500\n",
      "80/80 [==============================] - 0s 512us/step - loss: 0.4377 - accuracy: 0.7972\n",
      "Epoch 112/500\n",
      "80/80 [==============================] - 0s 510us/step - loss: 0.4375 - accuracy: 0.7972\n",
      "Epoch 113/500\n",
      "80/80 [==============================] - 0s 493us/step - loss: 0.4372 - accuracy: 0.7972\n",
      "Epoch 114/500\n",
      "80/80 [==============================] - 0s 531us/step - loss: 0.4370 - accuracy: 0.7972\n",
      "Epoch 115/500\n",
      "80/80 [==============================] - 0s 487us/step - loss: 0.4367 - accuracy: 0.7972\n",
      "Epoch 116/500\n",
      "80/80 [==============================] - 0s 494us/step - loss: 0.4365 - accuracy: 0.7972\n",
      "Epoch 117/500\n",
      "80/80 [==============================] - 0s 498us/step - loss: 0.4363 - accuracy: 0.7972\n",
      "Epoch 118/500\n",
      "80/80 [==============================] - 0s 522us/step - loss: 0.4361 - accuracy: 0.7972\n",
      "Epoch 119/500\n",
      "80/80 [==============================] - 0s 519us/step - loss: 0.4359 - accuracy: 0.7972\n",
      "Epoch 120/500\n",
      "80/80 [==============================] - 0s 508us/step - loss: 0.4356 - accuracy: 0.7972\n",
      "Epoch 121/500\n",
      "80/80 [==============================] - 0s 510us/step - loss: 0.4354 - accuracy: 0.7972\n",
      "Epoch 122/500\n",
      "80/80 [==============================] - 0s 507us/step - loss: 0.4352 - accuracy: 0.7972\n",
      "Epoch 123/500\n",
      "80/80 [==============================] - 0s 510us/step - loss: 0.4350 - accuracy: 0.7972\n",
      "Epoch 124/500\n",
      "80/80 [==============================] - 0s 506us/step - loss: 0.4348 - accuracy: 0.7972\n",
      "Epoch 125/500\n",
      "80/80 [==============================] - 0s 511us/step - loss: 0.4347 - accuracy: 0.7972\n",
      "Epoch 126/500\n",
      "80/80 [==============================] - 0s 520us/step - loss: 0.4345 - accuracy: 0.7972\n",
      "Epoch 127/500\n",
      "80/80 [==============================] - 0s 510us/step - loss: 0.4343 - accuracy: 0.7972\n",
      "Epoch 128/500\n",
      "80/80 [==============================] - 0s 506us/step - loss: 0.4341 - accuracy: 0.7972\n",
      "Epoch 129/500\n",
      "80/80 [==============================] - 0s 486us/step - loss: 0.4339 - accuracy: 0.7972\n",
      "Epoch 130/500\n",
      "80/80 [==============================] - 0s 546us/step - loss: 0.4338 - accuracy: 0.7972\n",
      "Epoch 131/500\n",
      "80/80 [==============================] - 0s 582us/step - loss: 0.4336 - accuracy: 0.7972\n",
      "Epoch 132/500\n",
      "80/80 [==============================] - 0s 510us/step - loss: 0.4334 - accuracy: 0.7972\n",
      "Epoch 133/500\n",
      "80/80 [==============================] - 0s 507us/step - loss: 0.4333 - accuracy: 0.7972\n",
      "Epoch 134/500\n",
      "80/80 [==============================] - 0s 532us/step - loss: 0.4331 - accuracy: 0.7972\n",
      "Epoch 135/500\n",
      "80/80 [==============================] - 0s 510us/step - loss: 0.4330 - accuracy: 0.7972\n",
      "Epoch 136/500\n",
      "80/80 [==============================] - 0s 506us/step - loss: 0.4328 - accuracy: 0.7972\n",
      "Epoch 137/500\n",
      "80/80 [==============================] - 0s 500us/step - loss: 0.4327 - accuracy: 0.7972\n",
      "Epoch 138/500\n",
      "80/80 [==============================] - 0s 505us/step - loss: 0.4325 - accuracy: 0.7972\n",
      "Epoch 139/500\n",
      "80/80 [==============================] - 0s 498us/step - loss: 0.4324 - accuracy: 0.7972\n",
      "Epoch 140/500\n",
      "80/80 [==============================] - 0s 506us/step - loss: 0.4323 - accuracy: 0.7972\n",
      "Epoch 141/500\n",
      "80/80 [==============================] - 0s 498us/step - loss: 0.4321 - accuracy: 0.7972\n",
      "Epoch 142/500\n",
      "80/80 [==============================] - 0s 508us/step - loss: 0.4320 - accuracy: 0.7972\n",
      "Epoch 143/500\n",
      "80/80 [==============================] - 0s 497us/step - loss: 0.4319 - accuracy: 0.7972\n",
      "Epoch 144/500\n",
      "80/80 [==============================] - 0s 506us/step - loss: 0.4317 - accuracy: 0.7972\n",
      "Epoch 145/500\n",
      "80/80 [==============================] - 0s 497us/step - loss: 0.4316 - accuracy: 0.7972\n",
      "Epoch 146/500\n",
      "80/80 [==============================] - 0s 508us/step - loss: 0.4315 - accuracy: 0.7972\n",
      "Epoch 147/500\n",
      "80/80 [==============================] - 0s 498us/step - loss: 0.4314 - accuracy: 0.7972\n",
      "Epoch 148/500\n",
      "80/80 [==============================] - 0s 520us/step - loss: 0.4313 - accuracy: 0.7972\n",
      "Epoch 149/500\n",
      "80/80 [==============================] - 0s 523us/step - loss: 0.4311 - accuracy: 0.7972\n",
      "Epoch 150/500\n",
      "80/80 [==============================] - 0s 519us/step - loss: 0.4310 - accuracy: 0.7972\n",
      "Epoch 151/500\n",
      "80/80 [==============================] - 0s 506us/step - loss: 0.4309 - accuracy: 0.7972\n",
      "Epoch 152/500\n",
      "80/80 [==============================] - 0s 498us/step - loss: 0.4308 - accuracy: 0.7972\n",
      "Epoch 153/500\n",
      "80/80 [==============================] - 0s 494us/step - loss: 0.4307 - accuracy: 0.7972\n",
      "Epoch 154/500\n",
      "80/80 [==============================] - 0s 510us/step - loss: 0.4306 - accuracy: 0.7972\n",
      "Epoch 155/500\n",
      "80/80 [==============================] - 0s 507us/step - loss: 0.4305 - accuracy: 0.7972\n",
      "Epoch 156/500\n",
      "80/80 [==============================] - 0s 498us/step - loss: 0.4304 - accuracy: 0.7972\n",
      "Epoch 157/500\n",
      "80/80 [==============================] - 0s 518us/step - loss: 0.4303 - accuracy: 0.7972\n",
      "Epoch 158/500\n",
      "80/80 [==============================] - 0s 512us/step - loss: 0.4302 - accuracy: 0.7972\n",
      "Epoch 159/500\n",
      "80/80 [==============================] - 0s 506us/step - loss: 0.4301 - accuracy: 0.7972\n",
      "Epoch 160/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 510us/step - loss: 0.4300 - accuracy: 0.7972\n",
      "Epoch 161/500\n",
      "80/80 [==============================] - 0s 507us/step - loss: 0.4299 - accuracy: 0.7972\n",
      "Epoch 162/500\n",
      "80/80 [==============================] - 0s 497us/step - loss: 0.4299 - accuracy: 0.7972\n",
      "Epoch 163/500\n",
      "80/80 [==============================] - 0s 507us/step - loss: 0.4298 - accuracy: 0.7972\n",
      "Epoch 164/500\n",
      "80/80 [==============================] - 0s 498us/step - loss: 0.4297 - accuracy: 0.7972\n",
      "Epoch 165/500\n",
      "80/80 [==============================] - 0s 519us/step - loss: 0.4296 - accuracy: 0.7972\n",
      "Epoch 166/500\n",
      "80/80 [==============================] - 0s 510us/step - loss: 0.4295 - accuracy: 0.7972\n",
      "Epoch 167/500\n",
      "80/80 [==============================] - 0s 508us/step - loss: 0.4295 - accuracy: 0.7972\n",
      "Epoch 168/500\n",
      "80/80 [==============================] - 0s 497us/step - loss: 0.4294 - accuracy: 0.7972\n",
      "Epoch 169/500\n",
      "80/80 [==============================] - 0s 519us/step - loss: 0.4293 - accuracy: 0.7972\n",
      "Epoch 170/500\n",
      "80/80 [==============================] - 0s 523us/step - loss: 0.4292 - accuracy: 0.7972\n",
      "Epoch 171/500\n",
      "80/80 [==============================] - 0s 531us/step - loss: 0.4292 - accuracy: 0.7972\n",
      "Epoch 172/500\n",
      "80/80 [==============================] - 0s 429us/step - loss: 0.4291 - accuracy: 0.7972\n",
      "Epoch 173/500\n",
      "80/80 [==============================] - 0s 600us/step - loss: 0.4290 - accuracy: 0.7972\n",
      "Epoch 174/500\n",
      "80/80 [==============================] - 0s 535us/step - loss: 0.4289 - accuracy: 0.7972\n",
      "Epoch 175/500\n",
      "80/80 [==============================] - 0s 510us/step - loss: 0.4289 - accuracy: 0.7972\n",
      "Epoch 176/500\n",
      "80/80 [==============================] - 0s 506us/step - loss: 0.4288 - accuracy: 0.7972\n",
      "Epoch 177/500\n",
      "80/80 [==============================] - 0s 510us/step - loss: 0.4287 - accuracy: 0.7972\n",
      "Epoch 178/500\n",
      "80/80 [==============================] - 0s 495us/step - loss: 0.4287 - accuracy: 0.7972\n",
      "Epoch 179/500\n",
      "80/80 [==============================] - 0s 522us/step - loss: 0.4286 - accuracy: 0.7972\n",
      "Epoch 180/500\n",
      "80/80 [==============================] - 0s 507us/step - loss: 0.4286 - accuracy: 0.7972\n",
      "Epoch 181/500\n",
      "80/80 [==============================] - 0s 505us/step - loss: 0.4285 - accuracy: 0.7972\n",
      "Epoch 182/500\n",
      "80/80 [==============================] - 0s 562us/step - loss: 0.4284 - accuracy: 0.7972\n",
      "Epoch 183/500\n",
      "80/80 [==============================] - 0s 495us/step - loss: 0.4284 - accuracy: 0.7972\n",
      "Epoch 184/500\n",
      "80/80 [==============================] - 0s 497us/step - loss: 0.4283 - accuracy: 0.7972\n",
      "Epoch 185/500\n",
      "80/80 [==============================] - 0s 495us/step - loss: 0.4283 - accuracy: 0.7972\n",
      "Epoch 186/500\n",
      "80/80 [==============================] - 0s 497us/step - loss: 0.4282 - accuracy: 0.7972\n",
      "Epoch 187/500\n",
      "80/80 [==============================] - 0s 498us/step - loss: 0.4281 - accuracy: 0.7972\n",
      "Epoch 188/500\n",
      "80/80 [==============================] - 0s 508us/step - loss: 0.4281 - accuracy: 0.7972\n",
      "Epoch 189/500\n",
      "80/80 [==============================] - 0s 519us/step - loss: 0.4280 - accuracy: 0.7972\n",
      "Epoch 190/500\n",
      "80/80 [==============================] - 0s 510us/step - loss: 0.4280 - accuracy: 0.7976\n",
      "Epoch 191/500\n",
      "80/80 [==============================] - 0s 507us/step - loss: 0.4279 - accuracy: 0.7976\n",
      "Epoch 192/500\n",
      "80/80 [==============================] - 0s 498us/step - loss: 0.4279 - accuracy: 0.7976\n",
      "Epoch 193/500\n",
      "80/80 [==============================] - 0s 520us/step - loss: 0.4278 - accuracy: 0.7977\n",
      "Epoch 194/500\n",
      "80/80 [==============================] - 0s 498us/step - loss: 0.4278 - accuracy: 0.7980\n",
      "Epoch 195/500\n",
      "80/80 [==============================] - 0s 584us/step - loss: 0.4277 - accuracy: 0.7980\n",
      "Epoch 196/500\n",
      "80/80 [==============================] - 0s 759us/step - loss: 0.4277 - accuracy: 0.7983\n",
      "Epoch 197/500\n",
      "80/80 [==============================] - 0s 532us/step - loss: 0.4276 - accuracy: 0.7986\n",
      "Epoch 198/500\n",
      "80/80 [==============================] - 0s 531us/step - loss: 0.4276 - accuracy: 0.7987\n",
      "Epoch 199/500\n",
      "80/80 [==============================] - 0s 545us/step - loss: 0.4276 - accuracy: 0.7986\n",
      "Epoch 200/500\n",
      "80/80 [==============================] - 0s 561us/step - loss: 0.4275 - accuracy: 0.7989\n",
      "Epoch 201/500\n",
      "80/80 [==============================] - 0s 521us/step - loss: 0.4275 - accuracy: 0.7989\n",
      "Epoch 202/500\n",
      "80/80 [==============================] - 0s 519us/step - loss: 0.4274 - accuracy: 0.7990\n",
      "Epoch 203/500\n",
      "80/80 [==============================] - 0s 498us/step - loss: 0.4274 - accuracy: 0.7994\n",
      "Epoch 204/500\n",
      "80/80 [==============================] - 0s 519us/step - loss: 0.4273 - accuracy: 0.7995\n",
      "Epoch 205/500\n",
      "80/80 [==============================] - 0s 506us/step - loss: 0.4273 - accuracy: 0.8002\n",
      "Epoch 206/500\n",
      "80/80 [==============================] - 0s 511us/step - loss: 0.4273 - accuracy: 0.8002\n",
      "Epoch 207/500\n",
      "80/80 [==============================] - 0s 519us/step - loss: 0.4272 - accuracy: 0.8002\n",
      "Epoch 208/500\n",
      "80/80 [==============================] - 0s 498us/step - loss: 0.4272 - accuracy: 0.8008\n",
      "Epoch 209/500\n",
      "80/80 [==============================] - 0s 508us/step - loss: 0.4271 - accuracy: 0.8010\n",
      "Epoch 210/500\n",
      "80/80 [==============================] - 0s 536us/step - loss: 0.4271 - accuracy: 0.8010\n",
      "Epoch 211/500\n",
      "80/80 [==============================] - 0s 506us/step - loss: 0.4271 - accuracy: 0.8012\n",
      "Epoch 212/500\n",
      "80/80 [==============================] - 0s 523us/step - loss: 0.4270 - accuracy: 0.8014\n",
      "Epoch 213/500\n",
      "80/80 [==============================] - 0s 506us/step - loss: 0.4270 - accuracy: 0.8016\n",
      "Epoch 214/500\n",
      "80/80 [==============================] - 0s 507us/step - loss: 0.4270 - accuracy: 0.8018\n",
      "Epoch 215/500\n",
      "80/80 [==============================] - 0s 510us/step - loss: 0.4269 - accuracy: 0.8024\n",
      "Epoch 216/500\n",
      "80/80 [==============================] - 0s 489us/step - loss: 0.4269 - accuracy: 0.8024\n",
      "Epoch 217/500\n",
      "80/80 [==============================] - 0s 541us/step - loss: 0.4268 - accuracy: 0.8030\n",
      "Epoch 218/500\n",
      "80/80 [==============================] - 0s 488us/step - loss: 0.4268 - accuracy: 0.8040\n",
      "Epoch 219/500\n",
      "80/80 [==============================] - 0s 424us/step - loss: 0.4268 - accuracy: 0.8039\n",
      "Epoch 220/500\n",
      "80/80 [==============================] - 0s 625us/step - loss: 0.4267 - accuracy: 0.8045\n",
      "Epoch 221/500\n",
      "80/80 [==============================] - 0s 547us/step - loss: 0.4267 - accuracy: 0.8043\n",
      "Epoch 222/500\n",
      "80/80 [==============================] - 0s 495us/step - loss: 0.4267 - accuracy: 0.8045\n",
      "Epoch 223/500\n",
      "80/80 [==============================] - 0s 499us/step - loss: 0.4266 - accuracy: 0.8045\n",
      "Epoch 224/500\n",
      "80/80 [==============================] - 0s 510us/step - loss: 0.4266 - accuracy: 0.8051\n",
      "Epoch 225/500\n",
      "80/80 [==============================] - 0s 507us/step - loss: 0.4266 - accuracy: 0.8056\n",
      "Epoch 226/500\n",
      "80/80 [==============================] - 0s 499us/step - loss: 0.4265 - accuracy: 0.8061\n",
      "Epoch 227/500\n",
      "80/80 [==============================] - 0s 506us/step - loss: 0.4265 - accuracy: 0.8064\n",
      "Epoch 228/500\n",
      "80/80 [==============================] - 0s 498us/step - loss: 0.4265 - accuracy: 0.8064\n",
      "Epoch 229/500\n",
      "80/80 [==============================] - 0s 494us/step - loss: 0.4265 - accuracy: 0.8071\n",
      "Epoch 230/500\n",
      "80/80 [==============================] - 0s 510us/step - loss: 0.4264 - accuracy: 0.8077\n",
      "Epoch 231/500\n",
      "80/80 [==============================] - 0s 533us/step - loss: 0.4264 - accuracy: 0.8074\n",
      "Epoch 232/500\n",
      "80/80 [==============================] - 0s 496us/step - loss: 0.4264 - accuracy: 0.8083\n",
      "Epoch 233/500\n",
      "80/80 [==============================] - 0s 494us/step - loss: 0.4263 - accuracy: 0.8085\n",
      "Epoch 234/500\n",
      "80/80 [==============================] - 0s 498us/step - loss: 0.4263 - accuracy: 0.8090\n",
      "Epoch 235/500\n",
      "80/80 [==============================] - 0s 495us/step - loss: 0.4263 - accuracy: 0.8091\n",
      "Epoch 236/500\n",
      "80/80 [==============================] - 0s 523us/step - loss: 0.4262 - accuracy: 0.8092\n",
      "Epoch 237/500\n",
      "80/80 [==============================] - 0s 494us/step - loss: 0.4262 - accuracy: 0.8094\n",
      "Epoch 238/500\n",
      "80/80 [==============================] - 0s 510us/step - loss: 0.4262 - accuracy: 0.8104\n",
      "Epoch 239/500\n",
      "80/80 [==============================] - 0s 508us/step - loss: 0.4262 - accuracy: 0.8098\n",
      "Epoch 240/500\n",
      "80/80 [==============================] - 0s 506us/step - loss: 0.4261 - accuracy: 0.8112\n",
      "Epoch 241/500\n",
      "80/80 [==============================] - 0s 498us/step - loss: 0.4261 - accuracy: 0.8108\n",
      "Epoch 242/500\n",
      "80/80 [==============================] - 0s 498us/step - loss: 0.4261 - accuracy: 0.8110\n",
      "Epoch 243/500\n",
      "80/80 [==============================] - 0s 507us/step - loss: 0.4261 - accuracy: 0.8110\n",
      "Epoch 244/500\n",
      "80/80 [==============================] - 0s 510us/step - loss: 0.4260 - accuracy: 0.8108\n",
      "Epoch 245/500\n",
      "80/80 [==============================] - 0s 507us/step - loss: 0.4260 - accuracy: 0.8110\n",
      "Epoch 246/500\n",
      "80/80 [==============================] - 0s 510us/step - loss: 0.4260 - accuracy: 0.8112\n",
      "Epoch 247/500\n",
      "80/80 [==============================] - 0s 495us/step - loss: 0.4259 - accuracy: 0.8111\n",
      "Epoch 248/500\n",
      "80/80 [==============================] - 0s 510us/step - loss: 0.4259 - accuracy: 0.8115\n",
      "Epoch 249/500\n",
      "80/80 [==============================] - 0s 494us/step - loss: 0.4259 - accuracy: 0.8114\n",
      "Epoch 250/500\n",
      "80/80 [==============================] - 0s 511us/step - loss: 0.4259 - accuracy: 0.8127\n",
      "Epoch 251/500\n",
      "80/80 [==============================] - 0s 495us/step - loss: 0.4258 - accuracy: 0.8123\n",
      "Epoch 252/500\n",
      "80/80 [==============================] - 0s 510us/step - loss: 0.4258 - accuracy: 0.8126\n",
      "Epoch 253/500\n",
      "80/80 [==============================] - 0s 506us/step - loss: 0.4258 - accuracy: 0.8126\n",
      "Epoch 254/500\n",
      "80/80 [==============================] - 0s 524us/step - loss: 0.4258 - accuracy: 0.8129\n",
      "Epoch 255/500\n",
      "80/80 [==============================] - 0s 494us/step - loss: 0.4257 - accuracy: 0.8130\n",
      "Epoch 256/500\n",
      "80/80 [==============================] - 0s 507us/step - loss: 0.4257 - accuracy: 0.8131\n",
      "Epoch 257/500\n",
      "80/80 [==============================] - 0s 498us/step - loss: 0.4257 - accuracy: 0.8136\n",
      "Epoch 258/500\n",
      "80/80 [==============================] - 0s 498us/step - loss: 0.4257 - accuracy: 0.8138\n",
      "Epoch 259/500\n",
      "80/80 [==============================] - 0s 507us/step - loss: 0.4256 - accuracy: 0.8140\n",
      "Epoch 260/500\n",
      "80/80 [==============================] - 0s 535us/step - loss: 0.4256 - accuracy: 0.8140\n",
      "Epoch 261/500\n",
      "80/80 [==============================] - 0s 495us/step - loss: 0.4256 - accuracy: 0.8145\n",
      "Epoch 262/500\n",
      "80/80 [==============================] - 0s 532us/step - loss: 0.4256 - accuracy: 0.8142\n",
      "Epoch 263/500\n",
      "80/80 [==============================] - 0s 511us/step - loss: 0.4255 - accuracy: 0.8142\n",
      "Epoch 264/500\n",
      "80/80 [==============================] - 0s 494us/step - loss: 0.4255 - accuracy: 0.8150\n",
      "Epoch 265/500\n",
      "80/80 [==============================] - 0s 498us/step - loss: 0.4255 - accuracy: 0.8146\n",
      "Epoch 266/500\n",
      "80/80 [==============================] - 0s 506us/step - loss: 0.4255 - accuracy: 0.8151\n",
      "Epoch 267/500\n",
      "80/80 [==============================] - 0s 510us/step - loss: 0.4254 - accuracy: 0.8151\n",
      "Epoch 268/500\n",
      "80/80 [==============================] - 0s 519us/step - loss: 0.4254 - accuracy: 0.8150\n",
      "Epoch 269/500\n",
      "80/80 [==============================] - 0s 510us/step - loss: 0.4254 - accuracy: 0.8151\n",
      "Epoch 270/500\n",
      "80/80 [==============================] - 0s 507us/step - loss: 0.4254 - accuracy: 0.8148\n",
      "Epoch 271/500\n",
      "80/80 [==============================] - 0s 473us/step - loss: 0.4254 - accuracy: 0.8151\n",
      "Epoch 272/500\n",
      "80/80 [==============================] - 0s 482us/step - loss: 0.4253 - accuracy: 0.8152\n",
      "Epoch 273/500\n",
      "80/80 [==============================] - 0s 497us/step - loss: 0.4253 - accuracy: 0.8151\n",
      "Epoch 274/500\n",
      "80/80 [==============================] - 0s 494us/step - loss: 0.4253 - accuracy: 0.8146\n",
      "Epoch 275/500\n",
      "80/80 [==============================] - 0s 498us/step - loss: 0.4253 - accuracy: 0.8154\n",
      "Epoch 276/500\n",
      "80/80 [==============================] - 0s 531us/step - loss: 0.4252 - accuracy: 0.8146\n",
      "Epoch 277/500\n",
      "80/80 [==============================] - 0s 510us/step - loss: 0.4252 - accuracy: 0.8145\n",
      "Epoch 278/500\n",
      "80/80 [==============================] - 0s 495us/step - loss: 0.4252 - accuracy: 0.8149\n",
      "Epoch 279/500\n",
      "80/80 [==============================] - 0s 510us/step - loss: 0.4252 - accuracy: 0.8141\n",
      "Epoch 280/500\n",
      "80/80 [==============================] - 0s 507us/step - loss: 0.4251 - accuracy: 0.8142\n",
      "Epoch 281/500\n",
      "80/80 [==============================] - 0s 510us/step - loss: 0.4251 - accuracy: 0.8141\n",
      "Epoch 282/500\n",
      "80/80 [==============================] - 0s 507us/step - loss: 0.4251 - accuracy: 0.8145\n",
      "Epoch 283/500\n",
      "80/80 [==============================] - 0s 499us/step - loss: 0.4251 - accuracy: 0.8146\n",
      "Epoch 284/500\n",
      "80/80 [==============================] - 0s 507us/step - loss: 0.4251 - accuracy: 0.8148\n",
      "Epoch 285/500\n",
      "80/80 [==============================] - 0s 510us/step - loss: 0.4250 - accuracy: 0.8148\n",
      "Epoch 286/500\n",
      "80/80 [==============================] - 0s 507us/step - loss: 0.4250 - accuracy: 0.8150\n",
      "Epoch 287/500\n",
      "80/80 [==============================] - 0s 497us/step - loss: 0.4250 - accuracy: 0.8155\n",
      "Epoch 288/500\n",
      "80/80 [==============================] - 0s 506us/step - loss: 0.4250 - accuracy: 0.8154\n",
      "Epoch 289/500\n",
      "80/80 [==============================] - 0s 511us/step - loss: 0.4249 - accuracy: 0.8154\n",
      "Epoch 290/500\n",
      "80/80 [==============================] - 0s 493us/step - loss: 0.4249 - accuracy: 0.8155\n",
      "Epoch 291/500\n",
      "80/80 [==============================] - 0s 494us/step - loss: 0.4249 - accuracy: 0.8155\n",
      "Epoch 292/500\n",
      "80/80 [==============================] - 0s 512us/step - loss: 0.4249 - accuracy: 0.8158\n",
      "Epoch 293/500\n",
      "80/80 [==============================] - 0s 497us/step - loss: 0.4249 - accuracy: 0.8154\n",
      "Epoch 294/500\n",
      "80/80 [==============================] - 0s 507us/step - loss: 0.4248 - accuracy: 0.8158\n",
      "Epoch 295/500\n",
      "80/80 [==============================] - 0s 510us/step - loss: 0.4248 - accuracy: 0.8156\n",
      "Epoch 296/500\n",
      "80/80 [==============================] - 0s 507us/step - loss: 0.4248 - accuracy: 0.8160\n",
      "Epoch 297/500\n",
      "80/80 [==============================] - 0s 511us/step - loss: 0.4248 - accuracy: 0.8159\n",
      "Epoch 298/500\n",
      "80/80 [==============================] - 0s 506us/step - loss: 0.4247 - accuracy: 0.8161\n",
      "Epoch 299/500\n",
      "80/80 [==============================] - 0s 497us/step - loss: 0.4247 - accuracy: 0.8164\n",
      "Epoch 300/500\n",
      "80/80 [==============================] - 0s 520us/step - loss: 0.4247 - accuracy: 0.8159\n",
      "Epoch 301/500\n",
      "80/80 [==============================] - 0s 506us/step - loss: 0.4247 - accuracy: 0.8161\n",
      "Epoch 302/500\n",
      "80/80 [==============================] - 0s 505us/step - loss: 0.4246 - accuracy: 0.8163\n",
      "Epoch 303/500\n",
      "80/80 [==============================] - 0s 512us/step - loss: 0.4246 - accuracy: 0.8167\n",
      "Epoch 304/500\n",
      "80/80 [==============================] - 0s 498us/step - loss: 0.4246 - accuracy: 0.8170\n",
      "Epoch 305/500\n",
      "80/80 [==============================] - 0s 506us/step - loss: 0.4246 - accuracy: 0.8170\n",
      "Epoch 306/500\n",
      "80/80 [==============================] - 0s 497us/step - loss: 0.4246 - accuracy: 0.8167\n",
      "Epoch 307/500\n",
      "80/80 [==============================] - 0s 507us/step - loss: 0.4245 - accuracy: 0.8165\n",
      "Epoch 308/500\n",
      "80/80 [==============================] - 0s 518us/step - loss: 0.4245 - accuracy: 0.8173\n",
      "Epoch 309/500\n",
      "80/80 [==============================] - 0s 536us/step - loss: 0.4245 - accuracy: 0.8167\n",
      "Epoch 310/500\n",
      "80/80 [==============================] - 0s 508us/step - loss: 0.4245 - accuracy: 0.8167\n",
      "Epoch 311/500\n",
      "80/80 [==============================] - 0s 509us/step - loss: 0.4244 - accuracy: 0.8167\n",
      "Epoch 312/500\n",
      "80/80 [==============================] - 0s 511us/step - loss: 0.4244 - accuracy: 0.8167\n",
      "Epoch 313/500\n",
      "80/80 [==============================] - 0s 507us/step - loss: 0.4244 - accuracy: 0.8165\n",
      "Epoch 314/500\n",
      "80/80 [==============================] - 0s 520us/step - loss: 0.4244 - accuracy: 0.8166\n",
      "Epoch 315/500\n",
      "80/80 [==============================] - 0s 497us/step - loss: 0.4244 - accuracy: 0.8166\n",
      "Epoch 316/500\n",
      "80/80 [==============================] - 0s 545us/step - loss: 0.4243 - accuracy: 0.8166\n",
      "Epoch 317/500\n",
      "80/80 [==============================] - 0s 485us/step - loss: 0.4243 - accuracy: 0.8169\n",
      "Epoch 318/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 494us/step - loss: 0.4243 - accuracy: 0.8164\n",
      "Epoch 319/500\n",
      "80/80 [==============================] - 0s 499us/step - loss: 0.4243 - accuracy: 0.8164\n",
      "Epoch 320/500\n",
      "80/80 [==============================] - 0s 525us/step - loss: 0.4242 - accuracy: 0.8164\n",
      "Epoch 321/500\n",
      "80/80 [==============================] - 0s 503us/step - loss: 0.4242 - accuracy: 0.8164\n",
      "Epoch 322/500\n",
      "80/80 [==============================] - 0s 508us/step - loss: 0.4242 - accuracy: 0.8161\n",
      "Epoch 323/500\n",
      "80/80 [==============================] - 0s 510us/step - loss: 0.4242 - accuracy: 0.8164\n",
      "Epoch 324/500\n",
      "80/80 [==============================] - 0s 508us/step - loss: 0.4242 - accuracy: 0.8161\n",
      "Epoch 325/500\n",
      "80/80 [==============================] - 0s 523us/step - loss: 0.4241 - accuracy: 0.8163\n",
      "Epoch 326/500\n",
      "80/80 [==============================] - 0s 507us/step - loss: 0.4241 - accuracy: 0.8161\n",
      "Epoch 327/500\n",
      "80/80 [==============================] - 0s 498us/step - loss: 0.4241 - accuracy: 0.8161\n",
      "Epoch 328/500\n",
      "80/80 [==============================] - 0s 508us/step - loss: 0.4241 - accuracy: 0.8156\n",
      "Epoch 329/500\n",
      "80/80 [==============================] - 0s 494us/step - loss: 0.4240 - accuracy: 0.8159\n",
      "Epoch 330/500\n",
      "80/80 [==============================] - 0s 537us/step - loss: 0.4240 - accuracy: 0.8158\n",
      "Epoch 331/500\n",
      "80/80 [==============================] - 0s 506us/step - loss: 0.4240 - accuracy: 0.8158\n",
      "Epoch 332/500\n",
      "80/80 [==============================] - 0s 510us/step - loss: 0.4240 - accuracy: 0.8155\n",
      "Epoch 333/500\n",
      "80/80 [==============================] - 0s 507us/step - loss: 0.4239 - accuracy: 0.8158\n",
      "Epoch 334/500\n",
      "80/80 [==============================] - 0s 504us/step - loss: 0.4239 - accuracy: 0.8159\n",
      "Epoch 335/500\n",
      "80/80 [==============================] - 0s 501us/step - loss: 0.4239 - accuracy: 0.8156\n",
      "Epoch 336/500\n",
      "80/80 [==============================] - 0s 510us/step - loss: 0.4239 - accuracy: 0.8161\n",
      "Epoch 337/500\n",
      "80/80 [==============================] - 0s 493us/step - loss: 0.4239 - accuracy: 0.8158\n",
      "Epoch 338/500\n",
      "80/80 [==============================] - 0s 500us/step - loss: 0.4238 - accuracy: 0.8154\n",
      "Epoch 339/500\n",
      "80/80 [==============================] - 0s 519us/step - loss: 0.4238 - accuracy: 0.8159\n",
      "Epoch 340/500\n",
      "80/80 [==============================] - 0s 508us/step - loss: 0.4238 - accuracy: 0.8155\n",
      "Epoch 341/500\n",
      "80/80 [==============================] - 0s 496us/step - loss: 0.4238 - accuracy: 0.8159\n",
      "Epoch 342/500\n",
      "80/80 [==============================] - 0s 497us/step - loss: 0.4237 - accuracy: 0.8158\n",
      "Epoch 343/500\n",
      "80/80 [==============================] - 0s 507us/step - loss: 0.4237 - accuracy: 0.8155\n",
      "Epoch 344/500\n",
      "80/80 [==============================] - 0s 537us/step - loss: 0.4237 - accuracy: 0.8159\n",
      "Epoch 345/500\n",
      "80/80 [==============================] - 0s 506us/step - loss: 0.4237 - accuracy: 0.8159\n",
      "Epoch 346/500\n",
      "80/80 [==============================] - 0s 511us/step - loss: 0.4237 - accuracy: 0.8154\n",
      "Epoch 347/500\n",
      "80/80 [==============================] - 0s 494us/step - loss: 0.4236 - accuracy: 0.8159\n",
      "Epoch 348/500\n",
      "80/80 [==============================] - 0s 510us/step - loss: 0.4236 - accuracy: 0.8156\n",
      "Epoch 349/500\n",
      "80/80 [==============================] - 0s 506us/step - loss: 0.4236 - accuracy: 0.8154\n",
      "Epoch 350/500\n",
      "80/80 [==============================] - 0s 511us/step - loss: 0.4236 - accuracy: 0.8156\n",
      "Epoch 351/500\n",
      "80/80 [==============================] - 0s 519us/step - loss: 0.4235 - accuracy: 0.8150\n",
      "Epoch 352/500\n",
      "80/80 [==============================] - 0s 506us/step - loss: 0.4235 - accuracy: 0.8158\n",
      "Epoch 353/500\n",
      "80/80 [==============================] - 0s 487us/step - loss: 0.4235 - accuracy: 0.8158\n",
      "Epoch 354/500\n",
      "80/80 [==============================] - 0s 494us/step - loss: 0.4235 - accuracy: 0.8158\n",
      "Epoch 355/500\n",
      "80/80 [==============================] - 0s 521us/step - loss: 0.4234 - accuracy: 0.8159\n",
      "Epoch 356/500\n",
      "80/80 [==============================] - 0s 520us/step - loss: 0.4234 - accuracy: 0.8155\n",
      "Epoch 357/500\n",
      "80/80 [==============================] - 0s 510us/step - loss: 0.4234 - accuracy: 0.8159\n",
      "Epoch 358/500\n",
      "80/80 [==============================] - 0s 507us/step - loss: 0.4234 - accuracy: 0.8161\n",
      "Epoch 359/500\n",
      "80/80 [==============================] - 0s 505us/step - loss: 0.4233 - accuracy: 0.8156\n",
      "Epoch 360/500\n",
      "80/80 [==============================] - 0s 500us/step - loss: 0.4233 - accuracy: 0.8158\n",
      "Epoch 361/500\n",
      "80/80 [==============================] - 0s 510us/step - loss: 0.4233 - accuracy: 0.8161\n",
      "Epoch 362/500\n",
      "80/80 [==============================] - 0s 520us/step - loss: 0.4233 - accuracy: 0.8159\n",
      "Epoch 363/500\n",
      "80/80 [==============================] - 0s 528us/step - loss: 0.4233 - accuracy: 0.8160\n",
      "Epoch 364/500\n",
      "80/80 [==============================] - 0s 498us/step - loss: 0.4232 - accuracy: 0.8155\n",
      "Epoch 365/500\n",
      "80/80 [==============================] - 0s 500us/step - loss: 0.4232 - accuracy: 0.8156\n",
      "Epoch 366/500\n",
      "80/80 [==============================] - 0s 510us/step - loss: 0.4232 - accuracy: 0.8159\n",
      "Epoch 367/500\n",
      "80/80 [==============================] - 0s 494us/step - loss: 0.4232 - accuracy: 0.8159\n",
      "Epoch 368/500\n",
      "80/80 [==============================] - 0s 506us/step - loss: 0.4231 - accuracy: 0.8161\n",
      "Epoch 369/500\n",
      "80/80 [==============================] - 0s 511us/step - loss: 0.4231 - accuracy: 0.8159\n",
      "Epoch 370/500\n",
      "80/80 [==============================] - 0s 494us/step - loss: 0.4231 - accuracy: 0.8160\n",
      "Epoch 371/500\n",
      "80/80 [==============================] - 0s 510us/step - loss: 0.4231 - accuracy: 0.8163\n",
      "Epoch 372/500\n",
      "80/80 [==============================] - 0s 495us/step - loss: 0.4230 - accuracy: 0.8163\n",
      "Epoch 373/500\n",
      "80/80 [==============================] - 0s 510us/step - loss: 0.4230 - accuracy: 0.8164\n",
      "Epoch 374/500\n",
      "80/80 [==============================] - 0s 494us/step - loss: 0.4230 - accuracy: 0.8164\n",
      "Epoch 375/500\n",
      "80/80 [==============================] - 0s 520us/step - loss: 0.4230 - accuracy: 0.8167\n",
      "Epoch 376/500\n",
      "80/80 [==============================] - 0s 498us/step - loss: 0.4229 - accuracy: 0.8164\n",
      "Epoch 377/500\n",
      "80/80 [==============================] - 0s 506us/step - loss: 0.4229 - accuracy: 0.8165\n",
      "Epoch 378/500\n",
      "80/80 [==============================] - 0s 498us/step - loss: 0.4229 - accuracy: 0.8167\n",
      "Epoch 379/500\n",
      "80/80 [==============================] - 0s 520us/step - loss: 0.4229 - accuracy: 0.8164\n",
      "Epoch 380/500\n",
      "80/80 [==============================] - 0s 511us/step - loss: 0.4228 - accuracy: 0.8164\n",
      "Epoch 381/500\n",
      "80/80 [==============================] - 0s 493us/step - loss: 0.4228 - accuracy: 0.8164\n",
      "Epoch 382/500\n",
      "80/80 [==============================] - 0s 510us/step - loss: 0.4228 - accuracy: 0.8169\n",
      "Epoch 383/500\n",
      "80/80 [==============================] - 0s 522us/step - loss: 0.4228 - accuracy: 0.8165\n",
      "Epoch 384/500\n",
      "80/80 [==============================] - 0s 509us/step - loss: 0.4227 - accuracy: 0.8161\n",
      "Epoch 385/500\n",
      "80/80 [==============================] - 0s 505us/step - loss: 0.4227 - accuracy: 0.8163\n",
      "Epoch 386/500\n",
      "80/80 [==============================] - 0s 524us/step - loss: 0.4227 - accuracy: 0.8163\n",
      "Epoch 387/500\n",
      "80/80 [==============================] - 0s 507us/step - loss: 0.4227 - accuracy: 0.8163\n",
      "Epoch 388/500\n",
      "80/80 [==============================] - 0s 518us/step - loss: 0.4226 - accuracy: 0.8163\n",
      "Epoch 389/500\n",
      "80/80 [==============================] - 0s 510us/step - loss: 0.4226 - accuracy: 0.8163\n",
      "Epoch 390/500\n",
      "80/80 [==============================] - 0s 507us/step - loss: 0.4226 - accuracy: 0.8165\n",
      "Epoch 391/500\n",
      "80/80 [==============================] - 0s 510us/step - loss: 0.4226 - accuracy: 0.8164\n",
      "Epoch 392/500\n",
      "80/80 [==============================] - 0s 495us/step - loss: 0.4225 - accuracy: 0.8163\n",
      "Epoch 393/500\n",
      "80/80 [==============================] - 0s 502us/step - loss: 0.4225 - accuracy: 0.8164\n",
      "Epoch 394/500\n",
      "80/80 [==============================] - 0s 515us/step - loss: 0.4225 - accuracy: 0.8159\n",
      "Epoch 395/500\n",
      "80/80 [==============================] - 0s 511us/step - loss: 0.4225 - accuracy: 0.8161\n",
      "Epoch 396/500\n",
      "80/80 [==============================] - 0s 506us/step - loss: 0.4224 - accuracy: 0.8160\n",
      "Epoch 397/500\n",
      "80/80 [==============================] - 0s 497us/step - loss: 0.4224 - accuracy: 0.8160\n",
      "Epoch 398/500\n",
      "80/80 [==============================] - 0s 507us/step - loss: 0.4224 - accuracy: 0.8161\n",
      "Epoch 399/500\n",
      "80/80 [==============================] - 0s 511us/step - loss: 0.4224 - accuracy: 0.8160\n",
      "Epoch 400/500\n",
      "80/80 [==============================] - 0s 494us/step - loss: 0.4223 - accuracy: 0.8160\n",
      "Epoch 401/500\n",
      "80/80 [==============================] - 0s 510us/step - loss: 0.4223 - accuracy: 0.8160\n",
      "Epoch 402/500\n",
      "80/80 [==============================] - 0s 507us/step - loss: 0.4223 - accuracy: 0.8160\n",
      "Epoch 403/500\n",
      "80/80 [==============================] - 0s 535us/step - loss: 0.4223 - accuracy: 0.8163\n",
      "Epoch 404/500\n",
      "80/80 [==============================] - 0s 508us/step - loss: 0.4222 - accuracy: 0.8160\n",
      "Epoch 405/500\n",
      "80/80 [==============================] - 0s 512us/step - loss: 0.4222 - accuracy: 0.8160\n",
      "Epoch 406/500\n",
      "80/80 [==============================] - 0s 519us/step - loss: 0.4222 - accuracy: 0.8163\n",
      "Epoch 407/500\n",
      "80/80 [==============================] - 0s 519us/step - loss: 0.4221 - accuracy: 0.8161\n",
      "Epoch 408/500\n",
      "80/80 [==============================] - 0s 524us/step - loss: 0.4221 - accuracy: 0.8161\n",
      "Epoch 409/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.4221 - accuracy: 0.8165\n",
      "Epoch 410/500\n",
      "80/80 [==============================] - 0s 658us/step - loss: 0.4221 - accuracy: 0.8164\n",
      "Epoch 411/500\n",
      "80/80 [==============================] - 0s 532us/step - loss: 0.4220 - accuracy: 0.8163\n",
      "Epoch 412/500\n",
      "80/80 [==============================] - 0s 533us/step - loss: 0.4220 - accuracy: 0.8165\n",
      "Epoch 413/500\n",
      "80/80 [==============================] - 0s 550us/step - loss: 0.4220 - accuracy: 0.8159\n",
      "Epoch 414/500\n",
      "80/80 [==============================] - 0s 532us/step - loss: 0.4220 - accuracy: 0.8161\n",
      "Epoch 415/500\n",
      "80/80 [==============================] - 0s 520us/step - loss: 0.4219 - accuracy: 0.8163\n",
      "Epoch 416/500\n",
      "80/80 [==============================] - 0s 560us/step - loss: 0.4219 - accuracy: 0.8161\n",
      "Epoch 417/500\n",
      "80/80 [==============================] - 0s 544us/step - loss: 0.4219 - accuracy: 0.8161\n",
      "Epoch 418/500\n",
      "80/80 [==============================] - 0s 532us/step - loss: 0.4219 - accuracy: 0.8161\n",
      "Epoch 419/500\n",
      "80/80 [==============================] - 0s 596us/step - loss: 0.4218 - accuracy: 0.8161\n",
      "Epoch 420/500\n",
      "80/80 [==============================] - 0s 548us/step - loss: 0.4218 - accuracy: 0.8163\n",
      "Epoch 421/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.4218 - accuracy: 0.8161\n",
      "Epoch 422/500\n",
      "80/80 [==============================] - 0s 531us/step - loss: 0.4217 - accuracy: 0.8163\n",
      "Epoch 423/500\n",
      "80/80 [==============================] - 0s 533us/step - loss: 0.4217 - accuracy: 0.8163\n",
      "Epoch 424/500\n",
      "80/80 [==============================] - 0s 573us/step - loss: 0.4217 - accuracy: 0.8163\n",
      "Epoch 425/500\n",
      "80/80 [==============================] - 0s 532us/step - loss: 0.4217 - accuracy: 0.8161\n",
      "Epoch 426/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.4216 - accuracy: 0.8165\n",
      "Epoch 427/500\n",
      "80/80 [==============================] - 0s 532us/step - loss: 0.4216 - accuracy: 0.8161\n",
      "Epoch 428/500\n",
      "80/80 [==============================] - 0s 549us/step - loss: 0.4216 - accuracy: 0.8164\n",
      "Epoch 429/500\n",
      "80/80 [==============================] - 0s 532us/step - loss: 0.4216 - accuracy: 0.8158\n",
      "Epoch 430/500\n",
      "80/80 [==============================] - 0s 530us/step - loss: 0.4215 - accuracy: 0.8164\n",
      "Epoch 431/500\n",
      "80/80 [==============================] - 0s 549us/step - loss: 0.4215 - accuracy: 0.8164\n",
      "Epoch 432/500\n",
      "80/80 [==============================] - 0s 518us/step - loss: 0.4215 - accuracy: 0.8163\n",
      "Epoch 433/500\n",
      "80/80 [==============================] - 0s 507us/step - loss: 0.4214 - accuracy: 0.8160\n",
      "Epoch 434/500\n",
      "80/80 [==============================] - 0s 524us/step - loss: 0.4214 - accuracy: 0.8160\n",
      "Epoch 435/500\n",
      "80/80 [==============================] - 0s 494us/step - loss: 0.4214 - accuracy: 0.8164\n",
      "Epoch 436/500\n",
      "80/80 [==============================] - 0s 498us/step - loss: 0.4214 - accuracy: 0.8159\n",
      "Epoch 437/500\n",
      "80/80 [==============================] - 0s 495us/step - loss: 0.4213 - accuracy: 0.8160\n",
      "Epoch 438/500\n",
      "80/80 [==============================] - 0s 498us/step - loss: 0.4213 - accuracy: 0.8159\n",
      "Epoch 439/500\n",
      "80/80 [==============================] - 0s 494us/step - loss: 0.4213 - accuracy: 0.8160\n",
      "Epoch 440/500\n",
      "80/80 [==============================] - 0s 510us/step - loss: 0.4212 - accuracy: 0.8161\n",
      "Epoch 441/500\n",
      "80/80 [==============================] - 0s 507us/step - loss: 0.4212 - accuracy: 0.8164\n",
      "Epoch 442/500\n",
      "80/80 [==============================] - 0s 510us/step - loss: 0.4212 - accuracy: 0.8160\n",
      "Epoch 443/500\n",
      "80/80 [==============================] - 0s 494us/step - loss: 0.4212 - accuracy: 0.8164\n",
      "Epoch 444/500\n",
      "80/80 [==============================] - 0s 498us/step - loss: 0.4211 - accuracy: 0.8159\n",
      "Epoch 445/500\n",
      "80/80 [==============================] - 0s 533us/step - loss: 0.4211 - accuracy: 0.8163\n",
      "Epoch 446/500\n",
      "80/80 [==============================] - 0s 505us/step - loss: 0.4211 - accuracy: 0.8163\n",
      "Epoch 447/500\n",
      "80/80 [==============================] - 0s 536us/step - loss: 0.4210 - accuracy: 0.8165\n",
      "Epoch 448/500\n",
      "80/80 [==============================] - 0s 519us/step - loss: 0.4210 - accuracy: 0.8163\n",
      "Epoch 449/500\n",
      "80/80 [==============================] - 0s 511us/step - loss: 0.4210 - accuracy: 0.8165\n",
      "Epoch 450/500\n",
      "80/80 [==============================] - 0s 506us/step - loss: 0.4210 - accuracy: 0.8161\n",
      "Epoch 451/500\n",
      "80/80 [==============================] - 0s 498us/step - loss: 0.4209 - accuracy: 0.8165\n",
      "Epoch 452/500\n",
      "80/80 [==============================] - 0s 519us/step - loss: 0.4209 - accuracy: 0.8164\n",
      "Epoch 453/500\n",
      "80/80 [==============================] - 0s 498us/step - loss: 0.4209 - accuracy: 0.8164\n",
      "Epoch 454/500\n",
      "80/80 [==============================] - 0s 506us/step - loss: 0.4208 - accuracy: 0.8165\n",
      "Epoch 455/500\n",
      "80/80 [==============================] - 0s 522us/step - loss: 0.4208 - accuracy: 0.8161\n",
      "Epoch 456/500\n",
      "80/80 [==============================] - 0s 507us/step - loss: 0.4208 - accuracy: 0.8165\n",
      "Epoch 457/500\n",
      "80/80 [==============================] - 0s 508us/step - loss: 0.4207 - accuracy: 0.8160\n",
      "Epoch 458/500\n",
      "80/80 [==============================] - 0s 508us/step - loss: 0.4207 - accuracy: 0.8165\n",
      "Epoch 459/500\n",
      "80/80 [==============================] - 0s 499us/step - loss: 0.4207 - accuracy: 0.8163\n",
      "Epoch 460/500\n",
      "80/80 [==============================] - 0s 507us/step - loss: 0.4207 - accuracy: 0.8164\n",
      "Epoch 461/500\n",
      "80/80 [==============================] - 0s 519us/step - loss: 0.4206 - accuracy: 0.8158\n",
      "Epoch 462/500\n",
      "80/80 [==============================] - 0s 498us/step - loss: 0.4206 - accuracy: 0.8159\n",
      "Epoch 463/500\n",
      "80/80 [==============================] - 0s 520us/step - loss: 0.4206 - accuracy: 0.8165\n",
      "Epoch 464/500\n",
      "80/80 [==============================] - 0s 499us/step - loss: 0.4205 - accuracy: 0.8159\n",
      "Epoch 465/500\n",
      "80/80 [==============================] - 0s 518us/step - loss: 0.4205 - accuracy: 0.8160\n",
      "Epoch 466/500\n",
      "80/80 [==============================] - 0s 523us/step - loss: 0.4205 - accuracy: 0.8161\n",
      "Epoch 467/500\n",
      "80/80 [==============================] - 0s 507us/step - loss: 0.4204 - accuracy: 0.8163\n",
      "Epoch 468/500\n",
      "80/80 [==============================] - 0s 510us/step - loss: 0.4204 - accuracy: 0.8163\n",
      "Epoch 469/500\n",
      "80/80 [==============================] - 0s 507us/step - loss: 0.4204 - accuracy: 0.8160\n",
      "Epoch 470/500\n",
      "80/80 [==============================] - 0s 508us/step - loss: 0.4203 - accuracy: 0.8164\n",
      "Epoch 471/500\n",
      "80/80 [==============================] - 0s 510us/step - loss: 0.4203 - accuracy: 0.8164\n",
      "Epoch 472/500\n",
      "80/80 [==============================] - 0s 506us/step - loss: 0.4203 - accuracy: 0.8164\n",
      "Epoch 473/500\n",
      "80/80 [==============================] - 0s 512us/step - loss: 0.4203 - accuracy: 0.8163\n",
      "Epoch 474/500\n",
      "80/80 [==============================] - 0s 506us/step - loss: 0.4202 - accuracy: 0.8164\n",
      "Epoch 475/500\n",
      "80/80 [==============================] - 0s 499us/step - loss: 0.4202 - accuracy: 0.8163\n",
      "Epoch 476/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 531us/step - loss: 0.4202 - accuracy: 0.8163\n",
      "Epoch 477/500\n",
      "80/80 [==============================] - 0s 519us/step - loss: 0.4201 - accuracy: 0.8165\n",
      "Epoch 478/500\n",
      "80/80 [==============================] - 0s 499us/step - loss: 0.4201 - accuracy: 0.8164\n",
      "Epoch 479/500\n",
      "80/80 [==============================] - 0s 523us/step - loss: 0.4201 - accuracy: 0.8165\n",
      "Epoch 480/500\n",
      "80/80 [==============================] - 0s 494us/step - loss: 0.4200 - accuracy: 0.8161\n",
      "Epoch 481/500\n",
      "80/80 [==============================] - 0s 519us/step - loss: 0.4200 - accuracy: 0.8165\n",
      "Epoch 482/500\n",
      "80/80 [==============================] - 0s 512us/step - loss: 0.4200 - accuracy: 0.8163\n",
      "Epoch 483/500\n",
      "80/80 [==============================] - 0s 518us/step - loss: 0.4199 - accuracy: 0.8163\n",
      "Epoch 484/500\n",
      "80/80 [==============================] - 0s 511us/step - loss: 0.4199 - accuracy: 0.8164\n",
      "Epoch 485/500\n",
      "80/80 [==============================] - 0s 519us/step - loss: 0.4199 - accuracy: 0.8166\n",
      "Epoch 486/500\n",
      "80/80 [==============================] - 0s 498us/step - loss: 0.4198 - accuracy: 0.8164\n",
      "Epoch 487/500\n",
      "80/80 [==============================] - 0s 519us/step - loss: 0.4198 - accuracy: 0.8167\n",
      "Epoch 488/500\n",
      "80/80 [==============================] - 0s 498us/step - loss: 0.4198 - accuracy: 0.8166\n",
      "Epoch 489/500\n",
      "80/80 [==============================] - 0s 520us/step - loss: 0.4197 - accuracy: 0.8167\n",
      "Epoch 490/500\n",
      "80/80 [==============================] - 0s 511us/step - loss: 0.4197 - accuracy: 0.8170\n",
      "Epoch 491/500\n",
      "80/80 [==============================] - 0s 494us/step - loss: 0.4197 - accuracy: 0.8167\n",
      "Epoch 492/500\n",
      "80/80 [==============================] - 0s 498us/step - loss: 0.4196 - accuracy: 0.8167\n",
      "Epoch 493/500\n",
      "80/80 [==============================] - 0s 508us/step - loss: 0.4196 - accuracy: 0.8166\n",
      "Epoch 494/500\n",
      "80/80 [==============================] - 0s 498us/step - loss: 0.4196 - accuracy: 0.8167\n",
      "Epoch 495/500\n",
      "80/80 [==============================] - 0s 519us/step - loss: 0.4195 - accuracy: 0.8166\n",
      "Epoch 496/500\n",
      "80/80 [==============================] - 0s 510us/step - loss: 0.4195 - accuracy: 0.8167\n",
      "Epoch 497/500\n",
      "80/80 [==============================] - 0s 507us/step - loss: 0.4195 - accuracy: 0.8169\n",
      "Epoch 498/500\n",
      "80/80 [==============================] - 0s 498us/step - loss: 0.4194 - accuracy: 0.8167\n",
      "Epoch 499/500\n",
      "80/80 [==============================] - 0s 499us/step - loss: 0.4194 - accuracy: 0.8169\n",
      "Epoch 500/500\n",
      "80/80 [==============================] - 0s 492us/step - loss: 0.4194 - accuracy: 0.8169\n"
     ]
    }
   ],
   "source": [
    "#### Last step in creation of NNmodel NNmodel is trained on the training set here with Tensor-Keras .fit based on Compiler\n",
    "#Fitting NNmodel\n",
    "history=NNmodel.fit(X_train,Y_train,batch_size=100,epochs = 500)\n",
    "### Note that tf.keras.models.Sequential() by default uses glorot initializer -- drawing intial weights from a uniform \n",
    "### distribution -- see other possibilities in https://keras.io/api/layers/initializers/\n",
    "### Or you could try own customized wts inputs using\n",
    "### for layer in model.layers:\n",
    "###    init_layer_weight = [] # the weights yourself in this layer\n",
    "###    layer.set_weights(init_layer_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9a74ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (100, 2)                  26        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (100, 1)                  3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 29\n",
      "Trainable params: 29\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NNmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a2c5691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.796875, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.7976250052452087, 0.7976250052452087, 0.7976250052452087, 0.7977499961853027, 0.7979999780654907, 0.7979999780654907, 0.7982500195503235, 0.7986249923706055, 0.7987499833106995, 0.7986249923706055, 0.7988749742507935, 0.7988749742507935, 0.7990000247955322, 0.7993749976158142, 0.7994999885559082, 0.8002499938011169, 0.8002499938011169, 0.8002499938011169, 0.8007500171661377, 0.8009999990463257, 0.8009999990463257, 0.8012499809265137, 0.8013749718666077, 0.8016250133514404, 0.8017500042915344, 0.8023750185966492, 0.8023750185966492, 0.8029999732971191, 0.8040000200271606, 0.8038750290870667, 0.8044999837875366, 0.8042500019073486, 0.8044999837875366, 0.8044999837875366, 0.8051249980926514, 0.8056250214576721, 0.8061249852180481, 0.8063750267028809, 0.8063750267028809, 0.8071249723434448, 0.8077499866485596, 0.8073750138282776, 0.8082500100135803, 0.8084999918937683, 0.8090000152587891, 0.8091250061988831, 0.809249997138977, 0.809374988079071, 0.8103749752044678, 0.8097500205039978, 0.8112499713897705, 0.8107500076293945, 0.8109999895095825, 0.8109999895095825, 0.8107500076293945, 0.8109999895095825, 0.8112499713897705, 0.8111249804496765, 0.8115000128746033, 0.8113750219345093, 0.812749981880188, 0.812250018119812, 0.812624990940094, 0.812624990940094, 0.812874972820282, 0.8130000233650208, 0.8131250143051147, 0.8136249780654907, 0.8137500286102295, 0.8140000104904175, 0.8140000104904175, 0.8144999742507935, 0.8142499923706055, 0.8142499923706055, 0.8149999976158142, 0.8146250247955322, 0.8151249885559082, 0.8151249885559082, 0.8149999976158142, 0.8151249885559082, 0.8147500157356262, 0.8151249885559082, 0.8152499794960022, 0.8151249885559082, 0.8146250247955322, 0.8153749704360962, 0.8146250247955322, 0.8144999742507935, 0.8148750066757202, 0.8141250014305115, 0.8142499923706055, 0.8141250014305115, 0.8144999742507935, 0.8146250247955322, 0.8147500157356262, 0.8147500157356262, 0.8149999976158142, 0.815500020980835, 0.8153749704360962, 0.8153749704360962, 0.815500020980835, 0.815500020980835, 0.815750002861023, 0.8153749704360962, 0.815750002861023, 0.815625011920929, 0.8159999847412109, 0.8158749938011169, 0.8161249756813049, 0.8163750171661377, 0.8158749938011169, 0.8161249756813049, 0.8162500262260437, 0.8167499899864197, 0.8169999718666077, 0.8169999718666077, 0.8167499899864197, 0.8165000081062317, 0.8172500133514404, 0.8167499899864197, 0.8167499899864197, 0.8167499899864197, 0.8167499899864197, 0.8165000081062317, 0.8166249990463257, 0.8166249990463257, 0.8166249990463257, 0.8168749809265137, 0.8163750171661377, 0.8163750171661377, 0.8163750171661377, 0.8163750171661377, 0.8161249756813049, 0.8163750171661377, 0.8161249756813049, 0.8162500262260437, 0.8161249756813049, 0.8161249756813049, 0.815625011920929, 0.8158749938011169, 0.815750002861023, 0.815750002861023, 0.815500020980835, 0.815750002861023, 0.8158749938011169, 0.815625011920929, 0.8161249756813049, 0.815750002861023, 0.8153749704360962, 0.8158749938011169, 0.815500020980835, 0.8158749938011169, 0.815750002861023, 0.815500020980835, 0.8158749938011169, 0.8158749938011169, 0.8153749704360962, 0.8158749938011169, 0.815625011920929, 0.8153749704360962, 0.815625011920929, 0.8149999976158142, 0.815750002861023, 0.815750002861023, 0.815750002861023, 0.8158749938011169, 0.815500020980835, 0.8158749938011169, 0.8161249756813049, 0.815625011920929, 0.815750002861023, 0.8161249756813049, 0.8158749938011169, 0.8159999847412109, 0.815500020980835, 0.815625011920929, 0.8158749938011169, 0.8158749938011169, 0.8161249756813049, 0.8158749938011169, 0.8159999847412109, 0.8162500262260437, 0.8162500262260437, 0.8163750171661377, 0.8163750171661377, 0.8167499899864197, 0.8163750171661377, 0.8165000081062317, 0.8167499899864197, 0.8163750171661377, 0.8163750171661377, 0.8163750171661377, 0.8168749809265137, 0.8165000081062317, 0.8161249756813049, 0.8162500262260437, 0.8162500262260437, 0.8162500262260437, 0.8162500262260437, 0.8162500262260437, 0.8165000081062317, 0.8163750171661377, 0.8162500262260437, 0.8163750171661377, 0.8158749938011169, 0.8161249756813049, 0.8159999847412109, 0.8159999847412109, 0.8161249756813049, 0.8159999847412109, 0.8159999847412109, 0.8159999847412109, 0.8159999847412109, 0.8162500262260437, 0.8159999847412109, 0.8159999847412109, 0.8162500262260437, 0.8161249756813049, 0.8161249756813049, 0.8165000081062317, 0.8163750171661377, 0.8162500262260437, 0.8165000081062317, 0.8158749938011169, 0.8161249756813049, 0.8162500262260437, 0.8161249756813049, 0.8161249756813049, 0.8161249756813049, 0.8161249756813049, 0.8162500262260437, 0.8161249756813049, 0.8162500262260437, 0.8162500262260437, 0.8162500262260437, 0.8161249756813049, 0.8165000081062317, 0.8161249756813049, 0.8163750171661377, 0.815750002861023, 0.8163750171661377, 0.8163750171661377, 0.8162500262260437, 0.8159999847412109, 0.8159999847412109, 0.8163750171661377, 0.8158749938011169, 0.8159999847412109, 0.8158749938011169, 0.8159999847412109, 0.8161249756813049, 0.8163750171661377, 0.8159999847412109, 0.8163750171661377, 0.8158749938011169, 0.8162500262260437, 0.8162500262260437, 0.8165000081062317, 0.8162500262260437, 0.8165000081062317, 0.8161249756813049, 0.8165000081062317, 0.8163750171661377, 0.8163750171661377, 0.8165000081062317, 0.8161249756813049, 0.8165000081062317, 0.8159999847412109, 0.8165000081062317, 0.8162500262260437, 0.8163750171661377, 0.815750002861023, 0.8158749938011169, 0.8165000081062317, 0.8158749938011169, 0.8159999847412109, 0.8161249756813049, 0.8162500262260437, 0.8162500262260437, 0.8159999847412109, 0.8163750171661377, 0.8163750171661377, 0.8163750171661377, 0.8162500262260437, 0.8163750171661377, 0.8162500262260437, 0.8162500262260437, 0.8165000081062317, 0.8163750171661377, 0.8165000081062317, 0.8161249756813049, 0.8165000081062317, 0.8162500262260437, 0.8162500262260437, 0.8163750171661377, 0.8166249990463257, 0.8163750171661377, 0.8167499899864197, 0.8166249990463257, 0.8167499899864197, 0.8169999718666077, 0.8167499899864197, 0.8167499899864197, 0.8166249990463257, 0.8167499899864197, 0.8166249990463257, 0.8167499899864197, 0.8168749809265137, 0.8167499899864197, 0.8168749809265137, 0.8168749809265137]\n",
      "[0.5776256322860718, 0.5523287057876587, 0.5368965268135071, 0.5270374417304993, 0.5204228162765503, 0.5157498121261597, 0.51225745677948, 0.5095179080963135, 0.5072646141052246, 0.505336582660675, 0.5036258697509766, 0.5020678043365479, 0.5006174445152283, 0.4992528259754181, 0.49794769287109375, 0.4966929852962494, 0.4954785704612732, 0.4942985475063324, 0.4931514263153076, 0.4920303523540497, 0.4909351170063019, 0.4898637533187866, 0.4888156056404114, 0.48778659105300903, 0.48678040504455566, 0.4857872426509857, 0.4848126769065857, 0.48385509848594666, 0.48291322588920593, 0.48198971152305603, 0.48107707500457764, 0.4801819920539856, 0.47929832339286804, 0.47842520475387573, 0.477565735578537, 0.47672268748283386, 0.4758865535259247, 0.4750617444515228, 0.4742441177368164, 0.4734440743923187, 0.4726478159427643, 0.47187018394470215, 0.47109219431877136, 0.47033289074897766, 0.4695762097835541, 0.4688282310962677, 0.46809640526771545, 0.46736612915992737, 0.46664896607398987, 0.4659421145915985, 0.46524080634117126, 0.46454790234565735, 0.4638616740703583, 0.4631887376308441, 0.46251970529556274, 0.4618620276451111, 0.46121135354042053, 0.46057024598121643, 0.4599370062351227, 0.45931103825569153, 0.45869478583335876, 0.4580879509449005, 0.457485169172287, 0.45689657330513, 0.4563085436820984, 0.45573458075523376, 0.4551660418510437, 0.4546065032482147, 0.45405468344688416, 0.4535113275051117, 0.45298126339912415, 0.452452152967453, 0.4519343972206116, 0.451423317193985, 0.4509211480617523, 0.4504244327545166, 0.44993844628334045, 0.4494602680206299, 0.44898706674575806, 0.448524534702301, 0.44807296991348267, 0.4476237893104553, 0.4471844434738159, 0.4467518627643585, 0.4463283121585846, 0.44591110944747925, 0.4455006718635559, 0.4451017379760742, 0.44470468163490295, 0.4443173408508301, 0.4439384639263153, 0.4435637593269348, 0.44320082664489746, 0.44283804297447205, 0.44248881936073303, 0.4421423375606537, 0.441802054643631, 0.44146978855133057, 0.44114574790000916, 0.44082769751548767, 0.4405166804790497, 0.4402042329311371, 0.4399086534976959, 0.43961402773857117, 0.4393254220485687, 0.4390416741371155, 0.43876442313194275, 0.43849703669548035, 0.43822938203811646, 0.4379701614379883, 0.4377145767211914, 0.43746277689933777, 0.4372195601463318, 0.43698012828826904, 0.4367476999759674, 0.43652015924453735, 0.43629369139671326, 0.4360734522342682, 0.4358600974082947, 0.43564942479133606, 0.43544116616249084, 0.43524086475372314, 0.43504568934440613, 0.4348492920398712, 0.4346576929092407, 0.43447640538215637, 0.4342952370643616, 0.4341188669204712, 0.43394526839256287, 0.43377310037612915, 0.4336077570915222, 0.4334443211555481, 0.4332851767539978, 0.4331301748752594, 0.4329754412174225, 0.4328286349773407, 0.43268057703971863, 0.432537317276001, 0.432394802570343, 0.4322599172592163, 0.432125449180603, 0.4319933354854584, 0.4318634569644928, 0.431738942861557, 0.4316183924674988, 0.4314946234226227, 0.4313770830631256, 0.4312584400177002, 0.4311465322971344, 0.43103858828544617, 0.4309275448322296, 0.4308266043663025, 0.4307160973548889, 0.4306107759475708, 0.43051332235336304, 0.43041521310806274, 0.43031853437423706, 0.4302237927913666, 0.43013256788253784, 0.4300404191017151, 0.4299490749835968, 0.4298667311668396, 0.42977797985076904, 0.4296944737434387, 0.42961442470550537, 0.4295279383659363, 0.4294513463973999, 0.4293719530105591, 0.429301381111145, 0.4292260706424713, 0.42915457487106323, 0.4290803074836731, 0.42901116609573364, 0.428941547870636, 0.4288736581802368, 0.42880523204803467, 0.4287380874156952, 0.4286747872829437, 0.4286108911037445, 0.42855504155158997, 0.42849162220954895, 0.42842966318130493, 0.4283704161643982, 0.4283127784729004, 0.4282556176185608, 0.4281995892524719, 0.428146094083786, 0.4280914068222046, 0.4280385971069336, 0.4279862940311432, 0.42793747782707214, 0.42788591980934143, 0.4278377890586853, 0.42778798937797546, 0.4277397096157074, 0.4276915192604065, 0.4276447594165802, 0.4275990426540375, 0.4275532066822052, 0.42751169204711914, 0.4274664521217346, 0.42742639780044556, 0.4273785948753357, 0.4273407459259033, 0.4272936284542084, 0.42725813388824463, 0.4272138774394989, 0.42717689275741577, 0.42713820934295654, 0.42710331082344055, 0.4270646274089813, 0.4270232319831848, 0.42698684334754944, 0.4269540011882782, 0.42691439390182495, 0.4268783926963806, 0.42684343457221985, 0.42680874466896057, 0.42677512764930725, 0.42674168944358826, 0.42670807242393494, 0.4266722500324249, 0.42663875222206116, 0.42661139369010925, 0.4265786111354828, 0.4265454411506653, 0.4265158474445343, 0.4264827370643616, 0.42645013332366943, 0.42642319202423096, 0.4263887107372284, 0.42636197805404663, 0.4263289272785187, 0.42630085349082947, 0.4262748062610626, 0.4262426495552063, 0.42621660232543945, 0.4261915981769562, 0.4261547029018402, 0.42613160610198975, 0.4261026680469513, 0.4260749816894531, 0.42605164647102356, 0.42602214217185974, 0.42599397897720337, 0.42596903443336487, 0.42594316601753235, 0.4259214699268341, 0.4258924424648285, 0.42586520314216614, 0.4258405864238739, 0.4258156716823578, 0.4257862865924835, 0.42576345801353455, 0.42573583126068115, 0.42571157217025757, 0.4256862699985504, 0.4256612956523895, 0.4256344735622406, 0.4256111979484558, 0.4255935847759247, 0.4255642890930176, 0.4255416691303253, 0.4255143702030182, 0.4254920482635498, 0.4254683256149292, 0.4254492223262787, 0.4254208207130432, 0.42539793252944946, 0.4253730773925781, 0.42535078525543213, 0.42532750964164734, 0.4253003001213074, 0.4252808690071106, 0.42525771260261536, 0.42523694038391113, 0.42520931363105774, 0.42518696188926697, 0.4251653254032135, 0.4251408278942108, 0.42512303590774536, 0.42510148882865906, 0.4250723123550415, 0.42505115270614624, 0.4250306189060211, 0.42500805854797363, 0.4249867796897888, 0.424961656332016, 0.42493802309036255, 0.4249182641506195, 0.42489543557167053, 0.42487627267837524, 0.4248502254486084, 0.424826443195343, 0.42480236291885376, 0.4247802495956421, 0.4247584939002991, 0.4247357249259949, 0.4247116148471832, 0.4246905744075775, 0.42466792464256287, 0.42464789748191833, 0.42462819814682007, 0.4246004521846771, 0.4245785176753998, 0.42455950379371643, 0.4245361089706421, 0.42451170086860657, 0.4244898855686188, 0.4244685173034668, 0.4244452118873596, 0.42442429065704346, 0.4244039058685303, 0.4243796765804291, 0.42435333132743835, 0.4243321716785431, 0.42430952191352844, 0.4242922067642212, 0.42427024245262146, 0.4242442548274994, 0.4242236614227295, 0.42419886589050293, 0.42417848110198975, 0.4241570234298706, 0.42413195967674255, 0.4241146147251129, 0.42408621311187744, 0.4240674376487732, 0.4240398705005646, 0.42402225732803345, 0.4239966571331024, 0.42397335171699524, 0.4239466190338135, 0.42392751574516296, 0.4239073693752289, 0.42387881875038147, 0.42385947704315186, 0.42383792996406555, 0.423814982175827, 0.4237869679927826, 0.4237690269947052, 0.42374518513679504, 0.4237206280231476, 0.42369505763053894, 0.42367613315582275, 0.4236537516117096, 0.42363056540489197, 0.42360788583755493, 0.42358577251434326, 0.42356252670288086, 0.4235355854034424, 0.4235144555568695, 0.4234873056411743, 0.4234662353992462, 0.4234437942504883, 0.42341580986976624, 0.4233943819999695, 0.4233691692352295, 0.4233473539352417, 0.42332136631011963, 0.42329925298690796, 0.42327338457107544, 0.42325663566589355, 0.4232289493083954, 0.4232003688812256, 0.42317935824394226, 0.4231584072113037, 0.42313069105148315, 0.42310839891433716, 0.42308416962623596, 0.4230608642101288, 0.4230325222015381, 0.42300692200660706, 0.42298370599746704, 0.4229598045349121, 0.42294254899024963, 0.4229087233543396, 0.4228832721710205, 0.42286279797554016, 0.42283791303634644, 0.42281270027160645, 0.42278802394866943, 0.4227653443813324, 0.4227345883846283, 0.4227082133293152, 0.4226875305175781, 0.4226617217063904, 0.4226367175579071, 0.42260974645614624, 0.42258667945861816, 0.42255914211273193, 0.42253217101097107, 0.42251044511795044, 0.42248204350471497, 0.4224609136581421, 0.42243310809135437, 0.4224061369895935, 0.42237767577171326, 0.42235609889030457, 0.4223348796367645, 0.4223070442676544, 0.42227667570114136, 0.42225396633148193, 0.4222284257411957, 0.4221980571746826, 0.42217350006103516, 0.42214518785476685, 0.42212191224098206, 0.42209863662719727, 0.42206713557243347, 0.42203858494758606, 0.4220163822174072, 0.42198610305786133, 0.42196106910705566, 0.42193514108657837, 0.4219048321247101, 0.421879380941391, 0.42185208201408386, 0.4218270480632782, 0.42179784178733826, 0.4217711389064789, 0.4217418432235718, 0.4217146337032318, 0.4216878116130829, 0.42165863513946533, 0.42163145542144775, 0.4216056764125824, 0.4215778708457947, 0.4215548038482666, 0.42152196168899536, 0.4214966595172882, 0.42146530747413635, 0.42144185304641724, 0.42140862345695496, 0.42138245701789856, 0.4213545620441437, 0.42133039236068726, 0.42129647731781006, 0.4212694466114044, 0.42123618721961975, 0.4212070405483246, 0.4211812913417816, 0.42115017771720886, 0.42112576961517334, 0.421097069978714, 0.4210674464702606, 0.42103761434555054, 0.421009361743927, 0.4209822118282318, 0.4209515452384949, 0.4209199547767639, 0.4208929240703583, 0.4208601415157318, 0.42083218693733215, 0.4208030104637146, 0.420772522687912, 0.42074304819107056, 0.42071405053138733, 0.42068296670913696, 0.4206554591655731, 0.4206256568431854, 0.42059096693992615, 0.4205622375011444, 0.4205315113067627, 0.4204968214035034, 0.42047134041786194, 0.4204431474208832, 0.42041292786598206, 0.4203825294971466, 0.42034733295440674, 0.4203190803527832, 0.4202892482280731, 0.4202592372894287, 0.42022380232810974, 0.4201948940753937, 0.4201574921607971, 0.4201277494430542, 0.42009788751602173, 0.42006605863571167, 0.42003560066223145, 0.4200038015842438, 0.4199681878089905, 0.41993820667266846, 0.4199051856994629, 0.4198751151561737, 0.4198417067527771, 0.41980913281440735, 0.4197748005390167, 0.4197455644607544, 0.41971197724342346, 0.41968512535095215, 0.4196479618549347, 0.41961178183555603, 0.41957855224609375, 0.419547975063324, 0.41951674222946167, 0.41948235034942627, 0.41944620013237, 0.4194137156009674, 0.419380784034729]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(history.history['accuracy'])\n",
    "print(history.history['loss'])\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy and loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['accuracy', 'loss'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c57e988",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Below are the weights in the final iteration\n",
    "first_layer_weights = NNmodel.layers[0].get_weights()[0]\n",
    "first_layer_biases  = NNmodel.layers[0].get_weights()[1]\n",
    "second_layer_weights = NNmodel.layers[1].get_weights()[0]\n",
    "second_layer_biases  = NNmodel.layers[1].get_weights()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0f6f156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.55136114  0.1693606 ]\n",
      " [ 0.61171705 -0.06692807]\n",
      " [ 0.24039747  0.37233958]\n",
      " [-0.0175344   0.06224259]\n",
      " [ 0.13579012  0.47630784]\n",
      " [-0.52162725 -2.2510974 ]\n",
      " [-0.09049977 -0.02694809]\n",
      " [ 0.26751447 -0.22092134]\n",
      " [ 0.20028448  0.09356061]\n",
      " [-0.34103987 -0.06724508]\n",
      " [-0.96340257  0.4022087 ]\n",
      " [-0.02751628 -0.03769129]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(12, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(first_layer_weights)\n",
    "first_layer_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f38be53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09173506 0.4673299 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(first_layer_biases)\n",
    "first_layer_biases.shape  ### (2,) here basically means 2 elements in a 1-dim array. .T has no effect on 1d array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "306e8073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.0791818]\n",
      " [-3.1662748]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(second_layer_weights)\n",
    "second_layer_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5587208e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.3442263]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(second_layer_biases)\n",
    "second_layer_biases.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a4a041b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.00150113 -0.58312392 -0.57273139 -1.55489968  0.91509065  0.10629772\n",
      "  -0.70174202 -0.26396987  0.80225696  0.64376017  0.97725852 -0.00249134]]\n"
     ]
    }
   ],
   "source": [
    "### Now we use the trained weights and biases to try to predict based on a new case\n",
    "tr=sc.transform([[1, 0, 0, 500, 1, 40, 3, 60000, 2, 1, 1, 100000]])\n",
    "print(tr)  ### tr.shape is (1,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86520ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 46ms/step\n",
      "[[0.07215706]]\n"
     ]
    }
   ],
   "source": [
    "### Example\n",
    "### Predicting result for Single Observation\n",
    "print(NNmodel.predict(tr))\n",
    "### note in each recompute -- this no. will change slightly because of the random initiation of the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e89bf0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.95790345,  0.59734185]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### now we compute the predicted prob of 1, manually\n",
    "tr.dot(first_layer_weights)  ### gives a 1 x 2 matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "223820b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.86616839  1.06467174]]\n"
     ]
    }
   ],
   "source": [
    "Flayerneurons_sum=tr.dot(first_layer_weights) + first_layer_biases\n",
    "print(Flayerneurons_sum)  ### 1 x 2 matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d7512768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.13398569 0.74358231]]\n"
     ]
    }
   ],
   "source": [
    "Flayerneurons_act=1/(1+np.exp(-Flayerneurons_sum))\n",
    "print(Flayerneurons_act)  ### 1 x 2 matrix -- output of neurons in hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6f7f5c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.55401729]]\n"
     ]
    }
   ],
   "source": [
    "Slayerneurons_sum=Flayerneurons_act.dot (second_layer_weights)+second_layer_biases\n",
    "print(Slayerneurons_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7960c6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.07215706]]\n"
     ]
    }
   ],
   "source": [
    "predprob=1/(1+np.exp(-Slayerneurons_sum))\n",
    "print(predprob) ### Note this is the same output as NNmodel.predict(tr)\n",
    "### This manual computation of the forward pass should have output same as in NNmodel.predict(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "34a5ac4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 540us/step - loss: 0.4193 - accuracy: 0.8169\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4193280339241028, 0.8168749809265137]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Now we use the trained NNmodel to predict output in X_train sample\n",
    "NNmodel.evaluate(X_train,Y_train)  ### evaluates the loss and accuracy as specified in the Compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5ba646c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 472us/step\n"
     ]
    }
   ],
   "source": [
    "### Now we use the trained NNmodel to predict output in X_train sample -- computing manually via .predict\n",
    "TE=NNmodel.predict(X_train)  ### note X_train has 8000 data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "82ce3372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TE.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f49c856f",
   "metadata": {},
   "outputs": [],
   "source": [
    "h=(TE > 0.5).astype(int) ### Convert TE>0.5 == true ==> 1, False to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bb651f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8000, 1)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(h)\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4a697768",
   "metadata": {},
   "outputs": [],
   "source": [
    "### replace all elements in numpy array of value 0 with value -1\n",
    "h[h==0]=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c9d41502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1]\n",
      " [-1]\n",
      " [-1]\n",
      " ...\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8000, 1)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(h)\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e8b950e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### replace all elements in numpy array of value 0 with value -1\n",
    "Y_train1=Y_train\n",
    "Y_train1[Y_train1==0]=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "53b50eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1 -1  1 ...  1 -1  1]\n"
     ]
    }
   ],
   "source": [
    "print(Y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ec6af287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000,)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fe3de8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6535 0.816875\n"
     ]
    }
   ],
   "source": [
    "J=np.multiply(Y_train1.T,h.T)  ### element by element multiplication\n",
    "c=np.count_nonzero(J > 0) \n",
    "print(c,c/8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6bfab32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 702us/step - loss: 0.4130 - accuracy: 0.8235\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.41300517320632935, 0.8234999775886536]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Now we use the trained NNmodel to predict output in X_test sample\n",
    "NNmodel.evaluate(X_test,Y_test)  ### evaluates the loss and accuracy as specified in the Compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f34f4bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 464us/step\n"
     ]
    }
   ],
   "source": [
    "### Now we use the trained NNmodel to predict output in X_test sample -- computing manually via .predict\n",
    "TE1=NNmodel.predict(X_test)  ### note X_test has 2000 data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3d186532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1647 0.8235\n"
     ]
    }
   ],
   "source": [
    "h1=(TE1 > 0.5).astype(int) ### Convert TE1>0.5 == true ==> 1, False to 0\n",
    "h1[h1==0]=-1\n",
    "Y_test1=Y_test\n",
    "Y_test1[Y_test1==0]=-1\n",
    "J1=np.multiply(Y_test1.T,h1.T)  ### element by element multiplication\n",
    "c1=np.count_nonzero(J1 > 0) \n",
    "print(c1,c1/2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2345cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
