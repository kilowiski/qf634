{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c65022cc",
   "metadata": {},
   "source": [
    "# In this model -- same adam, sigmoids, 12-6-6-6-1 structure\n",
    "# Total sample size 10,000\n",
    "# Training sample size 80% = 8,000, this is (whole) batch size\n",
    "# (Mini) batch size = 100, hence no. of batches per epoch = 80\n",
    "# Hence 80 iterations per epoch \n",
    "# (1 iteration = 1 forward pass + 1 backward pass)\n",
    "# No. of epochs = 500, Total iterations = 500 x 80 = 40,000\n",
    "# Prediction of Y=1 (exit from bank) or Y=0 (no exit): binary classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ad2255c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kglim\\Anaconda3\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras import initializers\n",
    "\n",
    "# Set the seed using keras.utils.set_random_seed. This will set:\n",
    "# 1) `numpy` seed\n",
    "# 2) backend random seed\n",
    "# 3) `python` random seed\n",
    "keras.utils.set_random_seed(5)\n",
    "\n",
    "# If using TensorFlow, this will make GPU ops as deterministic as possible,\n",
    "# but it will affect the overall performance, so be mindful of that.\n",
    "tf.config.experimental.enable_op_determinism()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "617ee135",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c6deff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Dataset\n",
    "data = pd.read_csv(\"Churn_Modelling.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a80540e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CreditScore Geography  Gender  Age  Tenure    Balance  NumOfProducts  \\\n",
       "0          619    France  Female   42       2       0.00              1   \n",
       "1          608     Spain  Female   41       1   83807.86              1   \n",
       "2          502    France  Female   42       8  159660.80              3   \n",
       "3          699    France  Female   39       1       0.00              2   \n",
       "4          850     Spain  Female   43       2  125510.82              1   \n",
       "\n",
       "   HasCrCard  IsActiveMember  EstimatedSalary  \n",
       "0          1               1        101348.88  \n",
       "1          0               1        112542.58  \n",
       "2          1               0        113931.57  \n",
       "3          0               0         93826.63  \n",
       "4          1               1         79084.10  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generating Dependent Variable Vectors\n",
    "Y = data.iloc[:,-1].values\n",
    "X = data.iloc[:,3:13]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a98c381c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 ... 1 1 0]\n",
      "0       0\n",
      "1       0\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "       ..\n",
      "9995    1\n",
      "9996    1\n",
      "9997    0\n",
      "9998    1\n",
      "9999    0\n",
      "Name: Gender, Length: 10000, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Generating Dependent Variable Vectors\n",
    "Y = data.iloc[:,-1].values\n",
    "X = data.iloc[:,3:13]\n",
    "X['Gender']=X['Gender'].map({'Female':0,'Male':1})\n",
    "### above is used instead of a more complicated package involving -- from sklearn.preprocessing import LabelEncoder\n",
    "### converts Female -- 0, Male -- 1, i.e. hot-encoding categorical variables\n",
    "print(Y) ### Y = 1 means exited bank, 0 means staying on with bank as customer\n",
    "print (X['Gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e436f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding Categorical variable Geography\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ct =ColumnTransformer(transformers=[('encoder',OneHotEncoder(),[1])],remainder=\"passthrough\")\n",
    "X = np.array(ct.fit_transform(X))\n",
    "### Geography is transformed into France -- 1,0,0; Spain -- 0,0,1; Germany -- 0,1,0.\n",
    "### Moreover -- this encoded vector of ones-zeros is now put in first 3 cols. Credit Score pushed to 4th col."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3166a509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>619.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101348.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>608.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>112542.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>502.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>113931.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>699.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93826.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>850.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>79084.10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0    1    2      3    4     5    6          7    8    9    10         11\n",
       "0  1.0  0.0  0.0  619.0  0.0  42.0  2.0       0.00  1.0  1.0  1.0  101348.88\n",
       "1  0.0  0.0  1.0  608.0  0.0  41.0  1.0   83807.86  1.0  0.0  1.0  112542.58\n",
       "2  1.0  0.0  0.0  502.0  0.0  42.0  8.0  159660.80  3.0  1.0  0.0  113931.57\n",
       "3  1.0  0.0  0.0  699.0  0.0  39.0  1.0       0.00  2.0  0.0  0.0   93826.63\n",
       "4  0.0  0.0  1.0  850.0  0.0  43.0  2.0  125510.82  1.0  1.0  1.0   79084.10"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### convert X to dataframe X1\n",
    "X1 = pd.DataFrame(X)\n",
    "X1.head()\n",
    "### Note there are 12 features including onehotencoder for the Geography feature-- \n",
    "### The features are encoded using a one-hot (aka ‘one-of-K’ or ‘dummy’) encoding scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ef915ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting dataset into training and testing dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb2461b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0\n",
      " 1 0 0 0 1 0 0 1 0 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(Y_test[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "740ede9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9850b84b",
   "metadata": {},
   "source": [
    "We call fit_transform() method on our training data and transform() method on our test data.\n",
    "In sklearn.preprocessing.StandardScaler(), centering and scaling happens independently on each feature. \n",
    "\n",
    "The fit method is calculating the mean and variance of each of the features present in the data. The transform method is transforming all the features using the respective feature's mean and variance that are calculated in the statement\n",
    "before on X_train.\n",
    "\n",
    "X_train = sc.fit_transform(X_train) -- each feature in the X_train is scaled to mean 0, variance 1. \n",
    "X_test = sc.transform(X_test) -- each feature in X_test is scaled using sample mean, var of X_train features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e9c3dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.89906987e-16 -2.86048962e-16 -5.30686606e-16 -2.17603713e-17\n",
      "  4.79061235e-17 -6.23376351e-16 -5.14518983e-17 -4.18663715e-15\n",
      " -4.83973972e-16  7.27695681e-16 -5.93469718e-16  3.58962859e-15]\n"
     ]
    }
   ],
   "source": [
    "### Check transformed data\n",
    "import sys\n",
    "#np.set_printoptions(threshold=sys.maxsize)\n",
    "#print(X_train)\n",
    "print(np.mean(X_train,axis=0)) ## print means of each feature, showing scaling working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d87c81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(np.var(X_train,axis=0)) ## print means of each feature, showing scaling working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33f93b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.02150002 -0.03274686  0.00811563  0.01112925  0.01455712  0.0165023\n",
      " -0.0211389   0.00179946 -0.01435279 -0.0164785   0.03600953 -0.00461947]\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(X_test,axis=0)) ## print means of each feature, showing scaling working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6acc33de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99960225 0.96186549 1.0094561  0.99757565 0.99720133 1.0179895\n",
      " 1.02168564 1.00410934 0.96758663 1.01471752 0.99704644 0.98892999]\n"
     ]
    }
   ],
   "source": [
    "print(np.var(X_test,axis=0)) ## print means of each feature, showing scaling working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f43f1db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### This is the very first step while creating NNmodel. Here we are going to create our ann object by using a certain class of Keras \n",
    "### named Sequential. As a part of tensorflow 2.0, Keras is now integrated with tensorflow and is now considered as a \n",
    "### sub-library of tensorflow. The Sequential class is a part of the models module of Keras library which is a part of the \n",
    "### tensorflow library now. \n",
    "### It used to be \"import tensorflow as tf; from tensorflow import keras; from tensorflow.keras import layers\"\n",
    "### See documentation at https://keras.io/guides/sequential_model/\n",
    "\n",
    "#Initialising the NN model name -- NNmodel\n",
    "NNmodel = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34730cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating a network that has 2 hidden layers together with 1 input layer and 1 output layer. \n",
    "#Adding First Hidden Layer\n",
    "NNmodel.add(tf.keras.layers.Dense(units=6,activation=\"sigmoid\"))\n",
    "### units = 6 refer to 6 neurons in hidden layer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab12edcf",
   "metadata": {},
   "source": [
    "Above -- first hidden layer is created using the Dense class which is part of the layers module. This class accepts 2 inputs:-\n",
    "(1) units:- number of neurons that will be present in the respective layer (2) activation:- specify which activation function to be used. This example uses first input as 8. There is no correct answer which is the right number of neurons in the layer -- trial and error. Not too large to be computationally impractical or redundant; not too small to be ineffective.\n",
    "For the second input, we try the sigmoid or logistic function as an activation function for hidden layers. We can also try “relu”[rectified linear unit]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e22a225d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating 2nd hidden layer \n",
    "#Adding Second Hidden Layer -- note this is added sequentially to the first hidden layer\n",
    "NNmodel.add(tf.keras.layers.Dense(units=6,activation=\"sigmoid\"))\n",
    "### units = 6 refer to 6 neurons in hidden layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b423a28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating 3rd hidden layer \n",
    "#Adding Second Hidden Layer -- note this is added sequentially to the first hidden layer\n",
    "NNmodel.add(tf.keras.layers.Dense(units=6,activation=\"sigmoid\"))\n",
    "### units = 6 refer to 6 neurons in hidden layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "303997d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### now we create the output layer -- this is added sequentially\n",
    "#Adding Output Layer\n",
    "NNmodel.add(tf.keras.layers.Dense(units=1,activation=\"sigmoid\"))\n",
    "### Only 1 output neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b4a623",
   "metadata": {},
   "source": [
    "For a binary classification problem as above, actual case output is 1 or 0. Hence we require only one neuron to output layer - output could be estimated probability of case actual output = 1. For multiclass classification problem, if the output contains m categories then we need to create m different neurons, one for each category. In the binary output case, the suitable activation function is the sigmoid function. For multiclass classification problem, the activation function is typically softmax. The softmax function predicts a multinomial probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70baaa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "### After creating the layers -- require compiling the NNmodel. Compiling allows the computer to run and understand the program \n",
    "### without the need of more fundamental steps in the programming. Compiling adds other elements or linking other libraries, and optimization,\n",
    "### such that after compiling the results are readily computed e.g. in a binary executable program as an output. \n",
    "#Compiling NNmodel\n",
    "NNmodel.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=['accuracy'])\n",
    "### Note optimizer here is a more sophisticated version of the Mean Square loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f9d245",
   "metadata": {},
   "source": [
    "Compile method above accepts inputs: (1) optimizer:- specifies which optimizer to be used in order to perform stochastic gradient descent (2) error/loss function, e.g., 'binary_crossentropy' here. For multiclass classification, it should be categorical_crossentropy, (3) metrics - the performance metrics to use in order to compute performance. 'accuracy' is one such  performance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "51697cc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 12)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36a93b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "80/80 [==============================] - 1s 1ms/step - loss: 0.5893 - accuracy: 0.7972\n",
      "Epoch 2/500\n",
      "80/80 [==============================] - 0s 996us/step - loss: 0.5327 - accuracy: 0.7972\n",
      "Epoch 3/500\n",
      "80/80 [==============================] - 0s 968us/step - loss: 0.5103 - accuracy: 0.7972\n",
      "Epoch 4/500\n",
      "80/80 [==============================] - 0s 1000us/step - loss: 0.5033 - accuracy: 0.7972\n",
      "Epoch 5/500\n",
      "80/80 [==============================] - 0s 976us/step - loss: 0.5007 - accuracy: 0.7972\n",
      "Epoch 6/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.4987 - accuracy: 0.7972\n",
      "Epoch 7/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.4964 - accuracy: 0.7972\n",
      "Epoch 8/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.4937 - accuracy: 0.7972\n",
      "Epoch 9/500\n",
      "80/80 [==============================] - 0s 953us/step - loss: 0.4902 - accuracy: 0.7972\n",
      "Epoch 10/500\n",
      "80/80 [==============================] - 0s 965us/step - loss: 0.4857 - accuracy: 0.7972\n",
      "Epoch 11/500\n",
      "80/80 [==============================] - 0s 975us/step - loss: 0.4801 - accuracy: 0.7972\n",
      "Epoch 12/500\n",
      "80/80 [==============================] - 0s 980us/step - loss: 0.4735 - accuracy: 0.7972\n",
      "Epoch 13/500\n",
      "80/80 [==============================] - 0s 929us/step - loss: 0.4663 - accuracy: 0.7972\n",
      "Epoch 14/500\n",
      "80/80 [==============================] - 0s 921us/step - loss: 0.4589 - accuracy: 0.7972\n",
      "Epoch 15/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.4521 - accuracy: 0.7972\n",
      "Epoch 16/500\n",
      "80/80 [==============================] - 0s 928us/step - loss: 0.4460 - accuracy: 0.7972\n",
      "Epoch 17/500\n",
      "80/80 [==============================] - 0s 906us/step - loss: 0.4409 - accuracy: 0.7972\n",
      "Epoch 18/500\n",
      "80/80 [==============================] - 0s 890us/step - loss: 0.4369 - accuracy: 0.7972\n",
      "Epoch 19/500\n",
      "80/80 [==============================] - 0s 908us/step - loss: 0.4338 - accuracy: 0.7972\n",
      "Epoch 20/500\n",
      "80/80 [==============================] - 0s 942us/step - loss: 0.4315 - accuracy: 0.7972\n",
      "Epoch 21/500\n",
      "80/80 [==============================] - 0s 931us/step - loss: 0.4297 - accuracy: 0.7972\n",
      "Epoch 22/500\n",
      "80/80 [==============================] - 0s 977us/step - loss: 0.4284 - accuracy: 0.7972\n",
      "Epoch 23/500\n",
      "80/80 [==============================] - 0s 990us/step - loss: 0.4274 - accuracy: 0.7972\n",
      "Epoch 24/500\n",
      "80/80 [==============================] - 0s 979us/step - loss: 0.4266 - accuracy: 0.7972\n",
      "Epoch 25/500\n",
      "80/80 [==============================] - 0s 993us/step - loss: 0.4256 - accuracy: 0.7972\n",
      "Epoch 26/500\n",
      "80/80 [==============================] - 0s 978us/step - loss: 0.4248 - accuracy: 0.7974\n",
      "Epoch 27/500\n",
      "80/80 [==============================] - 0s 956us/step - loss: 0.4238 - accuracy: 0.8015\n",
      "Epoch 28/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.4228 - accuracy: 0.8087\n",
      "Epoch 29/500\n",
      "80/80 [==============================] - 0s 962us/step - loss: 0.4220 - accuracy: 0.8154\n",
      "Epoch 30/500\n",
      "80/80 [==============================] - 0s 992us/step - loss: 0.4207 - accuracy: 0.8201\n",
      "Epoch 31/500\n",
      "80/80 [==============================] - 0s 979us/step - loss: 0.4195 - accuracy: 0.8211\n",
      "Epoch 32/500\n",
      "80/80 [==============================] - 0s 979us/step - loss: 0.4182 - accuracy: 0.8206\n",
      "Epoch 33/500\n",
      "80/80 [==============================] - 0s 996us/step - loss: 0.4168 - accuracy: 0.8229\n",
      "Epoch 34/500\n",
      "80/80 [==============================] - 0s 959us/step - loss: 0.4154 - accuracy: 0.8240\n",
      "Epoch 35/500\n",
      "80/80 [==============================] - 0s 970us/step - loss: 0.4139 - accuracy: 0.8248\n",
      "Epoch 36/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.4120 - accuracy: 0.8271\n",
      "Epoch 37/500\n",
      "80/80 [==============================] - 0s 926us/step - loss: 0.4102 - accuracy: 0.8270\n",
      "Epoch 38/500\n",
      "80/80 [==============================] - 0s 896us/step - loss: 0.4081 - accuracy: 0.8288\n",
      "Epoch 39/500\n",
      "80/80 [==============================] - 0s 902us/step - loss: 0.4061 - accuracy: 0.8285\n",
      "Epoch 40/500\n",
      "80/80 [==============================] - 0s 857us/step - loss: 0.4039 - accuracy: 0.8295\n",
      "Epoch 41/500\n",
      "80/80 [==============================] - 0s 881us/step - loss: 0.4019 - accuracy: 0.8286\n",
      "Epoch 42/500\n",
      "80/80 [==============================] - 0s 710us/step - loss: 0.3998 - accuracy: 0.8291\n",
      "Epoch 43/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3978 - accuracy: 0.8307\n",
      "Epoch 44/500\n",
      "80/80 [==============================] - 0s 843us/step - loss: 0.3958 - accuracy: 0.8300\n",
      "Epoch 45/500\n",
      "80/80 [==============================] - 0s 868us/step - loss: 0.3939 - accuracy: 0.8324\n",
      "Epoch 46/500\n",
      "80/80 [==============================] - 0s 702us/step - loss: 0.3920 - accuracy: 0.8335\n",
      "Epoch 47/500\n",
      "80/80 [==============================] - 0s 882us/step - loss: 0.3901 - accuracy: 0.8345\n",
      "Epoch 48/500\n",
      "80/80 [==============================] - 0s 858us/step - loss: 0.3883 - accuracy: 0.8369\n",
      "Epoch 49/500\n",
      "80/80 [==============================] - 0s 710us/step - loss: 0.3866 - accuracy: 0.8371\n",
      "Epoch 50/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3849 - accuracy: 0.8395\n",
      "Epoch 51/500\n",
      "80/80 [==============================] - 0s 867us/step - loss: 0.3832 - accuracy: 0.8413\n",
      "Epoch 52/500\n",
      "80/80 [==============================] - 0s 883us/step - loss: 0.3816 - accuracy: 0.8415\n",
      "Epoch 53/500\n",
      "80/80 [==============================] - 0s 736us/step - loss: 0.3800 - accuracy: 0.8424\n",
      "Epoch 54/500\n",
      "80/80 [==============================] - 0s 847us/step - loss: 0.3784 - accuracy: 0.8444\n",
      "Epoch 55/500\n",
      "80/80 [==============================] - 0s 843us/step - loss: 0.3771 - accuracy: 0.8453\n",
      "Epoch 56/500\n",
      "80/80 [==============================] - 0s 847us/step - loss: 0.3757 - accuracy: 0.8460\n",
      "Epoch 57/500\n",
      "80/80 [==============================] - 0s 981us/step - loss: 0.3743 - accuracy: 0.8475\n",
      "Epoch 58/500\n",
      "80/80 [==============================] - 0s 856us/step - loss: 0.3734 - accuracy: 0.8476\n",
      "Epoch 59/500\n",
      "80/80 [==============================] - 0s 842us/step - loss: 0.3719 - accuracy: 0.8484\n",
      "Epoch 60/500\n",
      "80/80 [==============================] - 0s 842us/step - loss: 0.3708 - accuracy: 0.8501\n",
      "Epoch 61/500\n",
      "80/80 [==============================] - 0s 844us/step - loss: 0.3695 - accuracy: 0.8499\n",
      "Epoch 62/500\n",
      "80/80 [==============================] - 0s 845us/step - loss: 0.3686 - accuracy: 0.8511\n",
      "Epoch 63/500\n",
      "80/80 [==============================] - 0s 850us/step - loss: 0.3676 - accuracy: 0.8500\n",
      "Epoch 64/500\n",
      "80/80 [==============================] - 0s 848us/step - loss: 0.3667 - accuracy: 0.8529\n",
      "Epoch 65/500\n",
      "80/80 [==============================] - 0s 819us/step - loss: 0.3656 - accuracy: 0.8525\n",
      "Epoch 66/500\n",
      "80/80 [==============================] - 0s 843us/step - loss: 0.3649 - accuracy: 0.8533\n",
      "Epoch 67/500\n",
      "80/80 [==============================] - 0s 847us/step - loss: 0.3640 - accuracy: 0.8531\n",
      "Epoch 68/500\n",
      "80/80 [==============================] - 0s 839us/step - loss: 0.3632 - accuracy: 0.8541\n",
      "Epoch 69/500\n",
      "80/80 [==============================] - 0s 847us/step - loss: 0.3624 - accuracy: 0.8543\n",
      "Epoch 70/500\n",
      "80/80 [==============================] - 0s 842us/step - loss: 0.3617 - accuracy: 0.8560\n",
      "Epoch 71/500\n",
      "80/80 [==============================] - 0s 992us/step - loss: 0.3609 - accuracy: 0.8559\n",
      "Epoch 72/500\n",
      "80/80 [==============================] - 0s 870us/step - loss: 0.3604 - accuracy: 0.8568\n",
      "Epoch 73/500\n",
      "80/80 [==============================] - 0s 846us/step - loss: 0.3597 - accuracy: 0.8579\n",
      "Epoch 74/500\n",
      "80/80 [==============================] - 0s 846us/step - loss: 0.3589 - accuracy: 0.8575\n",
      "Epoch 75/500\n",
      "80/80 [==============================] - 0s 848us/step - loss: 0.3583 - accuracy: 0.8583\n",
      "Epoch 76/500\n",
      "80/80 [==============================] - 0s 983us/step - loss: 0.3576 - accuracy: 0.8586\n",
      "Epoch 77/500\n",
      "80/80 [==============================] - 0s 847us/step - loss: 0.3573 - accuracy: 0.8574\n",
      "Epoch 78/500\n",
      "80/80 [==============================] - 0s 845us/step - loss: 0.3568 - accuracy: 0.8571\n",
      "Epoch 79/500\n",
      "80/80 [==============================] - 0s 845us/step - loss: 0.3559 - accuracy: 0.8585\n",
      "Epoch 80/500\n",
      "80/80 [==============================] - 0s 842us/step - loss: 0.3554 - accuracy: 0.8599\n",
      "Epoch 81/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 857us/step - loss: 0.3549 - accuracy: 0.8596\n",
      "Epoch 82/500\n",
      "80/80 [==============================] - 0s 818us/step - loss: 0.3543 - accuracy: 0.8594\n",
      "Epoch 83/500\n",
      "80/80 [==============================] - 0s 842us/step - loss: 0.3540 - accuracy: 0.8585\n",
      "Epoch 84/500\n",
      "80/80 [==============================] - 0s 843us/step - loss: 0.3532 - accuracy: 0.8614\n",
      "Epoch 85/500\n",
      "80/80 [==============================] - 0s 840us/step - loss: 0.3527 - accuracy: 0.8605\n",
      "Epoch 86/500\n",
      "80/80 [==============================] - 0s 844us/step - loss: 0.3523 - accuracy: 0.8608\n",
      "Epoch 87/500\n",
      "80/80 [==============================] - 0s 868us/step - loss: 0.3515 - accuracy: 0.8616\n",
      "Epoch 88/500\n",
      "80/80 [==============================] - 0s 843us/step - loss: 0.3510 - accuracy: 0.8614\n",
      "Epoch 89/500\n",
      "80/80 [==============================] - 0s 845us/step - loss: 0.3508 - accuracy: 0.8618\n",
      "Epoch 90/500\n",
      "80/80 [==============================] - 0s 852us/step - loss: 0.3505 - accuracy: 0.8615\n",
      "Epoch 91/500\n",
      "80/80 [==============================] - 0s 834us/step - loss: 0.3497 - accuracy: 0.8620\n",
      "Epoch 92/500\n",
      "80/80 [==============================] - 0s 845us/step - loss: 0.3493 - accuracy: 0.8612\n",
      "Epoch 93/500\n",
      "80/80 [==============================] - 0s 843us/step - loss: 0.3487 - accuracy: 0.8629\n",
      "Epoch 94/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3485 - accuracy: 0.8630\n",
      "Epoch 95/500\n",
      "80/80 [==============================] - 0s 839us/step - loss: 0.3480 - accuracy: 0.8634\n",
      "Epoch 96/500\n",
      "80/80 [==============================] - 0s 856us/step - loss: 0.3479 - accuracy: 0.8612\n",
      "Epoch 97/500\n",
      "80/80 [==============================] - 0s 839us/step - loss: 0.3473 - accuracy: 0.8630\n",
      "Epoch 98/500\n",
      "80/80 [==============================] - 0s 846us/step - loss: 0.3471 - accuracy: 0.8615\n",
      "Epoch 99/500\n",
      "80/80 [==============================] - 0s 836us/step - loss: 0.3464 - accuracy: 0.8626\n",
      "Epoch 100/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3461 - accuracy: 0.8630\n",
      "Epoch 101/500\n",
      "80/80 [==============================] - 0s 828us/step - loss: 0.3456 - accuracy: 0.8627\n",
      "Epoch 102/500\n",
      "80/80 [==============================] - 0s 843us/step - loss: 0.3452 - accuracy: 0.8618\n",
      "Epoch 103/500\n",
      "80/80 [==============================] - 0s 847us/step - loss: 0.3451 - accuracy: 0.8622\n",
      "Epoch 104/500\n",
      "80/80 [==============================] - 0s 846us/step - loss: 0.3447 - accuracy: 0.8608\n",
      "Epoch 105/500\n",
      "80/80 [==============================] - 0s 836us/step - loss: 0.3444 - accuracy: 0.8615\n",
      "Epoch 106/500\n",
      "80/80 [==============================] - 0s 847us/step - loss: 0.3441 - accuracy: 0.8615\n",
      "Epoch 107/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3439 - accuracy: 0.8616\n",
      "Epoch 108/500\n",
      "80/80 [==============================] - 0s 853us/step - loss: 0.3434 - accuracy: 0.8624\n",
      "Epoch 109/500\n",
      "80/80 [==============================] - 0s 843us/step - loss: 0.3434 - accuracy: 0.8620\n",
      "Epoch 110/500\n",
      "80/80 [==============================] - 0s 847us/step - loss: 0.3429 - accuracy: 0.8604\n",
      "Epoch 111/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3428 - accuracy: 0.8622\n",
      "Epoch 112/500\n",
      "80/80 [==============================] - 0s 829us/step - loss: 0.3417 - accuracy: 0.8614\n",
      "Epoch 113/500\n",
      "80/80 [==============================] - 0s 840us/step - loss: 0.3422 - accuracy: 0.8619\n",
      "Epoch 114/500\n",
      "80/80 [==============================] - 0s 846us/step - loss: 0.3417 - accuracy: 0.8621\n",
      "Epoch 115/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3414 - accuracy: 0.8627\n",
      "Epoch 116/500\n",
      "80/80 [==============================] - 0s 840us/step - loss: 0.3410 - accuracy: 0.8620\n",
      "Epoch 117/500\n",
      "80/80 [==============================] - 0s 843us/step - loss: 0.3413 - accuracy: 0.8614\n",
      "Epoch 118/500\n",
      "80/80 [==============================] - 0s 844us/step - loss: 0.3406 - accuracy: 0.8633\n",
      "Epoch 119/500\n",
      "80/80 [==============================] - 0s 847us/step - loss: 0.3403 - accuracy: 0.8621\n",
      "Epoch 120/500\n",
      "80/80 [==============================] - 0s 842us/step - loss: 0.3402 - accuracy: 0.8605\n",
      "Epoch 121/500\n",
      "80/80 [==============================] - 0s 999us/step - loss: 0.3398 - accuracy: 0.8629\n",
      "Epoch 122/500\n",
      "80/80 [==============================] - 0s 850us/step - loss: 0.3398 - accuracy: 0.8629\n",
      "Epoch 123/500\n",
      "80/80 [==============================] - 0s 843us/step - loss: 0.3394 - accuracy: 0.8622\n",
      "Epoch 124/500\n",
      "80/80 [==============================] - 0s 845us/step - loss: 0.3390 - accuracy: 0.8615\n",
      "Epoch 125/500\n",
      "80/80 [==============================] - 0s 849us/step - loss: 0.3389 - accuracy: 0.8621\n",
      "Epoch 126/500\n",
      "80/80 [==============================] - 0s 839us/step - loss: 0.3388 - accuracy: 0.8615\n",
      "Epoch 127/500\n",
      "80/80 [==============================] - 0s 845us/step - loss: 0.3387 - accuracy: 0.8637\n",
      "Epoch 128/500\n",
      "80/80 [==============================] - 0s 844us/step - loss: 0.3386 - accuracy: 0.8633\n",
      "Epoch 129/500\n",
      "80/80 [==============================] - 0s 873us/step - loss: 0.3378 - accuracy: 0.8627\n",
      "Epoch 130/500\n",
      "80/80 [==============================] - 0s 990us/step - loss: 0.3384 - accuracy: 0.8631\n",
      "Epoch 131/500\n",
      "80/80 [==============================] - 0s 844us/step - loss: 0.3378 - accuracy: 0.8633\n",
      "Epoch 132/500\n",
      "80/80 [==============================] - 0s 840us/step - loss: 0.3379 - accuracy: 0.8626\n",
      "Epoch 133/500\n",
      "80/80 [==============================] - 0s 846us/step - loss: 0.3374 - accuracy: 0.8625\n",
      "Epoch 134/500\n",
      "80/80 [==============================] - 0s 841us/step - loss: 0.3375 - accuracy: 0.8624\n",
      "Epoch 135/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3371 - accuracy: 0.8627\n",
      "Epoch 136/500\n",
      "80/80 [==============================] - 0s 847us/step - loss: 0.3368 - accuracy: 0.8631\n",
      "Epoch 137/500\n",
      "80/80 [==============================] - 0s 807us/step - loss: 0.3367 - accuracy: 0.8626\n",
      "Epoch 138/500\n",
      "80/80 [==============================] - 0s 846us/step - loss: 0.3366 - accuracy: 0.8625\n",
      "Epoch 139/500\n",
      "80/80 [==============================] - 0s 880us/step - loss: 0.3369 - accuracy: 0.8631\n",
      "Epoch 140/500\n",
      "80/80 [==============================] - 0s 804us/step - loss: 0.3364 - accuracy: 0.8630\n",
      "Epoch 141/500\n",
      "80/80 [==============================] - 0s 849us/step - loss: 0.3362 - accuracy: 0.8627\n",
      "Epoch 142/500\n",
      "80/80 [==============================] - 0s 992us/step - loss: 0.3362 - accuracy: 0.8627\n",
      "Epoch 143/500\n",
      "80/80 [==============================] - 0s 840us/step - loss: 0.3358 - accuracy: 0.8633\n",
      "Epoch 144/500\n",
      "80/80 [==============================] - 0s 871us/step - loss: 0.3357 - accuracy: 0.8630\n",
      "Epoch 145/500\n",
      "80/80 [==============================] - 0s 819us/step - loss: 0.3355 - accuracy: 0.8625\n",
      "Epoch 146/500\n",
      "80/80 [==============================] - 0s 842us/step - loss: 0.3355 - accuracy: 0.8627\n",
      "Epoch 147/500\n",
      "80/80 [==============================] - 0s 996us/step - loss: 0.3357 - accuracy: 0.8619\n",
      "Epoch 148/500\n",
      "80/80 [==============================] - 0s 803us/step - loss: 0.3352 - accuracy: 0.8626\n",
      "Epoch 149/500\n",
      "80/80 [==============================] - 0s 832us/step - loss: 0.3348 - accuracy: 0.8633\n",
      "Epoch 150/500\n",
      "80/80 [==============================] - 0s 842us/step - loss: 0.3349 - accuracy: 0.8622\n",
      "Epoch 151/500\n",
      "80/80 [==============================] - 0s 851us/step - loss: 0.3349 - accuracy: 0.8634\n",
      "Epoch 152/500\n",
      "80/80 [==============================] - 0s 875us/step - loss: 0.3347 - accuracy: 0.8629\n",
      "Epoch 153/500\n",
      "80/80 [==============================] - 0s 804us/step - loss: 0.3347 - accuracy: 0.8630\n",
      "Epoch 154/500\n",
      "80/80 [==============================] - 0s 847us/step - loss: 0.3343 - accuracy: 0.8631\n",
      "Epoch 155/500\n",
      "80/80 [==============================] - 0s 999us/step - loss: 0.3342 - accuracy: 0.8633\n",
      "Epoch 156/500\n",
      "80/80 [==============================] - 0s 829us/step - loss: 0.3340 - accuracy: 0.8630\n",
      "Epoch 157/500\n",
      "80/80 [==============================] - 0s 827us/step - loss: 0.3341 - accuracy: 0.8621\n",
      "Epoch 158/500\n",
      "80/80 [==============================] - 0s 848us/step - loss: 0.3340 - accuracy: 0.8625\n",
      "Epoch 159/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3338 - accuracy: 0.8621\n",
      "Epoch 160/500\n",
      "80/80 [==============================] - 0s 829us/step - loss: 0.3341 - accuracy: 0.8624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161/500\n",
      "80/80 [==============================] - 0s 819us/step - loss: 0.3336 - accuracy: 0.8619\n",
      "Epoch 162/500\n",
      "80/80 [==============================] - 0s 860us/step - loss: 0.3334 - accuracy: 0.8635\n",
      "Epoch 163/500\n",
      "80/80 [==============================] - 0s 839us/step - loss: 0.3336 - accuracy: 0.8620\n",
      "Epoch 164/500\n",
      "80/80 [==============================] - 0s 845us/step - loss: 0.3338 - accuracy: 0.8635\n",
      "Epoch 165/500\n",
      "80/80 [==============================] - 0s 843us/step - loss: 0.3330 - accuracy: 0.8618\n",
      "Epoch 166/500\n",
      "80/80 [==============================] - 0s 847us/step - loss: 0.3335 - accuracy: 0.8626\n",
      "Epoch 167/500\n",
      "80/80 [==============================] - 0s 843us/step - loss: 0.3332 - accuracy: 0.8624\n",
      "Epoch 168/500\n",
      "80/80 [==============================] - 0s 848us/step - loss: 0.3335 - accuracy: 0.8626\n",
      "Epoch 169/500\n",
      "80/80 [==============================] - 0s 840us/step - loss: 0.3329 - accuracy: 0.8627\n",
      "Epoch 170/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3329 - accuracy: 0.8635\n",
      "Epoch 171/500\n",
      "80/80 [==============================] - 0s 827us/step - loss: 0.3330 - accuracy: 0.8633\n",
      "Epoch 172/500\n",
      "80/80 [==============================] - 0s 841us/step - loss: 0.3327 - accuracy: 0.8630\n",
      "Epoch 173/500\n",
      "80/80 [==============================] - 0s 846us/step - loss: 0.3328 - accuracy: 0.8625\n",
      "Epoch 174/500\n",
      "80/80 [==============================] - 0s 842us/step - loss: 0.3328 - accuracy: 0.8634\n",
      "Epoch 175/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3324 - accuracy: 0.8631\n",
      "Epoch 176/500\n",
      "80/80 [==============================] - 0s 829us/step - loss: 0.3324 - accuracy: 0.8618\n",
      "Epoch 177/500\n",
      "80/80 [==============================] - 0s 844us/step - loss: 0.3326 - accuracy: 0.8631\n",
      "Epoch 178/500\n",
      "80/80 [==============================] - 0s 844us/step - loss: 0.3325 - accuracy: 0.8644\n",
      "Epoch 179/500\n",
      "80/80 [==============================] - 0s 843us/step - loss: 0.3321 - accuracy: 0.8636\n",
      "Epoch 180/500\n",
      "80/80 [==============================] - 0s 844us/step - loss: 0.3322 - accuracy: 0.8627\n",
      "Epoch 181/500\n",
      "80/80 [==============================] - 0s 845us/step - loss: 0.3321 - accuracy: 0.8624\n",
      "Epoch 182/500\n",
      "80/80 [==============================] - 0s 845us/step - loss: 0.3321 - accuracy: 0.8612\n",
      "Epoch 183/500\n",
      "80/80 [==============================] - 0s 824us/step - loss: 0.3317 - accuracy: 0.8630\n",
      "Epoch 184/500\n",
      "80/80 [==============================] - 0s 841us/step - loss: 0.3320 - accuracy: 0.8627\n",
      "Epoch 185/500\n",
      "80/80 [==============================] - 0s 849us/step - loss: 0.3318 - accuracy: 0.8625\n",
      "Epoch 186/500\n",
      "80/80 [==============================] - 0s 839us/step - loss: 0.3318 - accuracy: 0.8621\n",
      "Epoch 187/500\n",
      "80/80 [==============================] - 0s 849us/step - loss: 0.3322 - accuracy: 0.8616\n",
      "Epoch 188/500\n",
      "80/80 [==============================] - 0s 974us/step - loss: 0.3320 - accuracy: 0.8621\n",
      "Epoch 189/500\n",
      "80/80 [==============================] - 0s 854us/step - loss: 0.3319 - accuracy: 0.8637\n",
      "Epoch 190/500\n",
      "80/80 [==============================] - 0s 840us/step - loss: 0.3314 - accuracy: 0.8626\n",
      "Epoch 191/500\n",
      "80/80 [==============================] - 0s 847us/step - loss: 0.3317 - accuracy: 0.8627\n",
      "Epoch 192/500\n",
      "80/80 [==============================] - 0s 864us/step - loss: 0.3314 - accuracy: 0.8624\n",
      "Epoch 193/500\n",
      "80/80 [==============================] - 0s 822us/step - loss: 0.3316 - accuracy: 0.8631\n",
      "Epoch 194/500\n",
      "80/80 [==============================] - 0s 848us/step - loss: 0.3317 - accuracy: 0.8626\n",
      "Epoch 195/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3311 - accuracy: 0.8627\n",
      "Epoch 196/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3316 - accuracy: 0.8630\n",
      "Epoch 197/500\n",
      "80/80 [==============================] - 0s 787us/step - loss: 0.3314 - accuracy: 0.8630\n",
      "Epoch 198/500\n",
      "80/80 [==============================] - 0s 866us/step - loss: 0.3314 - accuracy: 0.8620\n",
      "Epoch 199/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3314 - accuracy: 0.8620\n",
      "Epoch 200/500\n",
      "80/80 [==============================] - 0s 979us/step - loss: 0.3312 - accuracy: 0.8621\n",
      "Epoch 201/500\n",
      "80/80 [==============================] - 0s 966us/step - loss: 0.3312 - accuracy: 0.8643\n",
      "Epoch 202/500\n",
      "80/80 [==============================] - 0s 823us/step - loss: 0.3311 - accuracy: 0.8633\n",
      "Epoch 203/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3312 - accuracy: 0.8620\n",
      "Epoch 204/500\n",
      "80/80 [==============================] - 0s 767us/step - loss: 0.3309 - accuracy: 0.8633\n",
      "Epoch 205/500\n",
      "80/80 [==============================] - 0s 841us/step - loss: 0.3308 - accuracy: 0.8614\n",
      "Epoch 206/500\n",
      "80/80 [==============================] - 0s 900us/step - loss: 0.3308 - accuracy: 0.8631\n",
      "Epoch 207/500\n",
      "80/80 [==============================] - 0s 844us/step - loss: 0.3308 - accuracy: 0.8616\n",
      "Epoch 208/500\n",
      "80/80 [==============================] - 0s 848us/step - loss: 0.3308 - accuracy: 0.8614\n",
      "Epoch 209/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3307 - accuracy: 0.8624\n",
      "Epoch 210/500\n",
      "80/80 [==============================] - 0s 883us/step - loss: 0.3304 - accuracy: 0.8637\n",
      "Epoch 211/500\n",
      "80/80 [==============================] - 0s 871us/step - loss: 0.3305 - accuracy: 0.8622\n",
      "Epoch 212/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3308 - accuracy: 0.8618\n",
      "Epoch 213/500\n",
      "80/80 [==============================] - 0s 980us/step - loss: 0.3309 - accuracy: 0.8629\n",
      "Epoch 214/500\n",
      "80/80 [==============================] - 0s 978us/step - loss: 0.3306 - accuracy: 0.8615\n",
      "Epoch 215/500\n",
      "80/80 [==============================] - 0s 825us/step - loss: 0.3305 - accuracy: 0.8630\n",
      "Epoch 216/500\n",
      "80/80 [==============================] - 0s 875us/step - loss: 0.3305 - accuracy: 0.8619\n",
      "Epoch 217/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3305 - accuracy: 0.8620\n",
      "Epoch 218/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3304 - accuracy: 0.8634\n",
      "Epoch 219/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3308 - accuracy: 0.8616\n",
      "Epoch 220/500\n",
      "80/80 [==============================] - 0s 953us/step - loss: 0.3308 - accuracy: 0.8618\n",
      "Epoch 221/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3302 - accuracy: 0.8620\n",
      "Epoch 222/500\n",
      "80/80 [==============================] - 0s 989us/step - loss: 0.3303 - accuracy: 0.8621\n",
      "Epoch 223/500\n",
      "80/80 [==============================] - 0s 840us/step - loss: 0.3304 - accuracy: 0.8622\n",
      "Epoch 224/500\n",
      "80/80 [==============================] - 0s 852us/step - loss: 0.3301 - accuracy: 0.8612\n",
      "Epoch 225/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3304 - accuracy: 0.8619\n",
      "Epoch 226/500\n",
      "80/80 [==============================] - 0s 955us/step - loss: 0.3307 - accuracy: 0.8619\n",
      "Epoch 227/500\n",
      "80/80 [==============================] - 0s 811us/step - loss: 0.3300 - accuracy: 0.8633\n",
      "Epoch 228/500\n",
      "80/80 [==============================] - 0s 880us/step - loss: 0.3304 - accuracy: 0.8614\n",
      "Epoch 229/500\n",
      "80/80 [==============================] - 0s 880us/step - loss: 0.3304 - accuracy: 0.8611\n",
      "Epoch 230/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3300 - accuracy: 0.8608\n",
      "Epoch 231/500\n",
      "80/80 [==============================] - 0s 840us/step - loss: 0.3301 - accuracy: 0.8608\n",
      "Epoch 232/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3301 - accuracy: 0.8619\n",
      "Epoch 233/500\n",
      "80/80 [==============================] - 0s 795us/step - loss: 0.3304 - accuracy: 0.8629\n",
      "Epoch 234/500\n",
      "80/80 [==============================] - 0s 838us/step - loss: 0.3306 - accuracy: 0.8625\n",
      "Epoch 235/500\n",
      "80/80 [==============================] - 0s 866us/step - loss: 0.3304 - accuracy: 0.8634\n",
      "Epoch 236/500\n",
      "80/80 [==============================] - 0s 844us/step - loss: 0.3297 - accuracy: 0.8621\n",
      "Epoch 237/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3303 - accuracy: 0.8630\n",
      "Epoch 238/500\n",
      "80/80 [==============================] - 0s 842us/step - loss: 0.3300 - accuracy: 0.8641\n",
      "Epoch 239/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3300 - accuracy: 0.8624\n",
      "Epoch 240/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 954us/step - loss: 0.3300 - accuracy: 0.8629\n",
      "Epoch 241/500\n",
      "80/80 [==============================] - 0s 814us/step - loss: 0.3298 - accuracy: 0.8620\n",
      "Epoch 242/500\n",
      "80/80 [==============================] - 0s 836us/step - loss: 0.3299 - accuracy: 0.8616\n",
      "Epoch 243/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3300 - accuracy: 0.8626\n",
      "Epoch 244/500\n",
      "80/80 [==============================] - 0s 899us/step - loss: 0.3298 - accuracy: 0.8625\n",
      "Epoch 245/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3298 - accuracy: 0.8626\n",
      "Epoch 246/500\n",
      "80/80 [==============================] - 0s 798us/step - loss: 0.3300 - accuracy: 0.8626\n",
      "Epoch 247/500\n",
      "80/80 [==============================] - 0s 849us/step - loss: 0.3296 - accuracy: 0.8627\n",
      "Epoch 248/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3301 - accuracy: 0.8625\n",
      "Epoch 249/500\n",
      "80/80 [==============================] - 0s 956us/step - loss: 0.3298 - accuracy: 0.8614\n",
      "Epoch 250/500\n",
      "80/80 [==============================] - 0s 968us/step - loss: 0.3299 - accuracy: 0.8619\n",
      "Epoch 251/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3298 - accuracy: 0.8629\n",
      "Epoch 252/500\n",
      "80/80 [==============================] - 0s 825us/step - loss: 0.3300 - accuracy: 0.8606\n",
      "Epoch 253/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3296 - accuracy: 0.8616\n",
      "Epoch 254/500\n",
      "80/80 [==============================] - 0s 948us/step - loss: 0.3297 - accuracy: 0.8622\n",
      "Epoch 255/500\n",
      "80/80 [==============================] - 0s 801us/step - loss: 0.3293 - accuracy: 0.8625\n",
      "Epoch 256/500\n",
      "80/80 [==============================] - 0s 846us/step - loss: 0.3294 - accuracy: 0.8636\n",
      "Epoch 257/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3296 - accuracy: 0.8627\n",
      "Epoch 258/500\n",
      "80/80 [==============================] - 0s 819us/step - loss: 0.3296 - accuracy: 0.8626\n",
      "Epoch 259/500\n",
      "80/80 [==============================] - 0s 849us/step - loss: 0.3294 - accuracy: 0.8625\n",
      "Epoch 260/500\n",
      "80/80 [==============================] - 0s 840us/step - loss: 0.3296 - accuracy: 0.8625\n",
      "Epoch 261/500\n",
      "80/80 [==============================] - 0s 843us/step - loss: 0.3293 - accuracy: 0.8635\n",
      "Epoch 262/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3293 - accuracy: 0.8625\n",
      "Epoch 263/500\n",
      "80/80 [==============================] - 0s 817us/step - loss: 0.3296 - accuracy: 0.8630\n",
      "Epoch 264/500\n",
      "80/80 [==============================] - 0s 842us/step - loss: 0.3295 - accuracy: 0.8640\n",
      "Epoch 265/500\n",
      "80/80 [==============================] - 0s 847us/step - loss: 0.3298 - accuracy: 0.8626\n",
      "Epoch 266/500\n",
      "80/80 [==============================] - 0s 845us/step - loss: 0.3294 - accuracy: 0.8618\n",
      "Epoch 267/500\n",
      "80/80 [==============================] - 0s 887us/step - loss: 0.3293 - accuracy: 0.8631\n",
      "Epoch 268/500\n",
      "80/80 [==============================] - 0s 821us/step - loss: 0.3294 - accuracy: 0.8622\n",
      "Epoch 269/500\n",
      "80/80 [==============================] - 0s 828us/step - loss: 0.3293 - accuracy: 0.8622\n",
      "Epoch 270/500\n",
      "80/80 [==============================] - 0s 864us/step - loss: 0.3294 - accuracy: 0.8624\n",
      "Epoch 271/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3294 - accuracy: 0.8626\n",
      "Epoch 272/500\n",
      "80/80 [==============================] - 0s 811us/step - loss: 0.3292 - accuracy: 0.8622\n",
      "Epoch 273/500\n",
      "80/80 [==============================] - 0s 843us/step - loss: 0.3298 - accuracy: 0.8630\n",
      "Epoch 274/500\n",
      "80/80 [==============================] - 0s 849us/step - loss: 0.3293 - accuracy: 0.8629\n",
      "Epoch 275/500\n",
      "80/80 [==============================] - 0s 790us/step - loss: 0.3292 - accuracy: 0.8640\n",
      "Epoch 276/500\n",
      "80/80 [==============================] - 0s 842us/step - loss: 0.3294 - accuracy: 0.8614\n",
      "Epoch 277/500\n",
      "80/80 [==============================] - 0s 901us/step - loss: 0.3291 - accuracy: 0.8637\n",
      "Epoch 278/500\n",
      "80/80 [==============================] - 0s 787us/step - loss: 0.3297 - accuracy: 0.8626\n",
      "Epoch 279/500\n",
      "80/80 [==============================] - 0s 852us/step - loss: 0.3291 - accuracy: 0.8631\n",
      "Epoch 280/500\n",
      "80/80 [==============================] - 0s 844us/step - loss: 0.3293 - accuracy: 0.8625\n",
      "Epoch 281/500\n",
      "80/80 [==============================] - 0s 845us/step - loss: 0.3296 - accuracy: 0.8624\n",
      "Epoch 282/500\n",
      "80/80 [==============================] - 0s 841us/step - loss: 0.3293 - accuracy: 0.8631\n",
      "Epoch 283/500\n",
      "80/80 [==============================] - 0s 846us/step - loss: 0.3289 - accuracy: 0.8633\n",
      "Epoch 284/500\n",
      "80/80 [==============================] - 0s 844us/step - loss: 0.3296 - accuracy: 0.8625\n",
      "Epoch 285/500\n",
      "80/80 [==============================] - 0s 844us/step - loss: 0.3291 - accuracy: 0.8624\n",
      "Epoch 286/500\n",
      "80/80 [==============================] - 0s 844us/step - loss: 0.3292 - accuracy: 0.8627\n",
      "Epoch 287/500\n",
      "80/80 [==============================] - 0s 847us/step - loss: 0.3287 - accuracy: 0.8636\n",
      "Epoch 288/500\n",
      "80/80 [==============================] - 0s 842us/step - loss: 0.3290 - accuracy: 0.8629\n",
      "Epoch 289/500\n",
      "80/80 [==============================] - 0s 844us/step - loss: 0.3289 - accuracy: 0.8633\n",
      "Epoch 290/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3292 - accuracy: 0.8626\n",
      "Epoch 291/500\n",
      "80/80 [==============================] - 0s 822us/step - loss: 0.3290 - accuracy: 0.8624\n",
      "Epoch 292/500\n",
      "80/80 [==============================] - 0s 847us/step - loss: 0.3290 - accuracy: 0.8630\n",
      "Epoch 293/500\n",
      "80/80 [==============================] - 0s 841us/step - loss: 0.3290 - accuracy: 0.8629\n",
      "Epoch 294/500\n",
      "80/80 [==============================] - 0s 758us/step - loss: 0.3291 - accuracy: 0.8627\n",
      "Epoch 295/500\n",
      "80/80 [==============================] - 0s 840us/step - loss: 0.3291 - accuracy: 0.8640\n",
      "Epoch 296/500\n",
      "80/80 [==============================] - 0s 932us/step - loss: 0.3292 - accuracy: 0.8631\n",
      "Epoch 297/500\n",
      "80/80 [==============================] - 0s 869us/step - loss: 0.3293 - accuracy: 0.8641\n",
      "Epoch 298/500\n",
      "80/80 [==============================] - 0s 871us/step - loss: 0.3290 - accuracy: 0.8635\n",
      "Epoch 299/500\n",
      "80/80 [==============================] - 0s 891us/step - loss: 0.3288 - accuracy: 0.8626\n",
      "Epoch 300/500\n",
      "80/80 [==============================] - 0s 839us/step - loss: 0.3292 - accuracy: 0.8626\n",
      "Epoch 301/500\n",
      "80/80 [==============================] - 0s 855us/step - loss: 0.3289 - accuracy: 0.8641\n",
      "Epoch 302/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3293 - accuracy: 0.8625\n",
      "Epoch 303/500\n",
      "80/80 [==============================] - 0s 845us/step - loss: 0.3289 - accuracy: 0.8622\n",
      "Epoch 304/500\n",
      "80/80 [==============================] - 0s 848us/step - loss: 0.3289 - accuracy: 0.8630\n",
      "Epoch 305/500\n",
      "80/80 [==============================] - 0s 747us/step - loss: 0.3290 - accuracy: 0.8627\n",
      "Epoch 306/500\n",
      "80/80 [==============================] - 0s 890us/step - loss: 0.3286 - accuracy: 0.8635\n",
      "Epoch 307/500\n",
      "80/80 [==============================] - 0s 838us/step - loss: 0.3288 - accuracy: 0.8637\n",
      "Epoch 308/500\n",
      "80/80 [==============================] - 0s 841us/step - loss: 0.3289 - accuracy: 0.8625\n",
      "Epoch 309/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3289 - accuracy: 0.8635\n",
      "Epoch 310/500\n",
      "80/80 [==============================] - 0s 828us/step - loss: 0.3286 - accuracy: 0.8626\n",
      "Epoch 311/500\n",
      "80/80 [==============================] - 0s 828us/step - loss: 0.3289 - accuracy: 0.8626\n",
      "Epoch 312/500\n",
      "80/80 [==============================] - 0s 850us/step - loss: 0.3289 - accuracy: 0.8619\n",
      "Epoch 313/500\n",
      "80/80 [==============================] - 0s 843us/step - loss: 0.3286 - accuracy: 0.8635\n",
      "Epoch 314/500\n",
      "80/80 [==============================] - 0s 841us/step - loss: 0.3288 - accuracy: 0.8633\n",
      "Epoch 315/500\n",
      "80/80 [==============================] - 0s 729us/step - loss: 0.3286 - accuracy: 0.8633\n",
      "Epoch 316/500\n",
      "80/80 [==============================] - 0s 855us/step - loss: 0.3291 - accuracy: 0.8631\n",
      "Epoch 317/500\n",
      "80/80 [==============================] - 0s 854us/step - loss: 0.3287 - accuracy: 0.8633\n",
      "Epoch 318/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3287 - accuracy: 0.8648\n",
      "Epoch 319/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 842us/step - loss: 0.3287 - accuracy: 0.8637\n",
      "Epoch 320/500\n",
      "80/80 [==============================] - 0s 823us/step - loss: 0.3287 - accuracy: 0.8639\n",
      "Epoch 321/500\n",
      "80/80 [==============================] - 0s 840us/step - loss: 0.3286 - accuracy: 0.8633\n",
      "Epoch 322/500\n",
      "80/80 [==============================] - 0s 913us/step - loss: 0.3287 - accuracy: 0.8636\n",
      "Epoch 323/500\n",
      "80/80 [==============================] - 0s 841us/step - loss: 0.3286 - accuracy: 0.8639\n",
      "Epoch 324/500\n",
      "80/80 [==============================] - 0s 850us/step - loss: 0.3286 - accuracy: 0.8639\n",
      "Epoch 325/500\n",
      "80/80 [==============================] - 0s 841us/step - loss: 0.3286 - accuracy: 0.8629\n",
      "Epoch 326/500\n",
      "80/80 [==============================] - 0s 844us/step - loss: 0.3284 - accuracy: 0.8643\n",
      "Epoch 327/500\n",
      "80/80 [==============================] - 0s 841us/step - loss: 0.3286 - accuracy: 0.8644\n",
      "Epoch 328/500\n",
      "80/80 [==============================] - 0s 846us/step - loss: 0.3287 - accuracy: 0.8631\n",
      "Epoch 329/500\n",
      "80/80 [==============================] - 0s 847us/step - loss: 0.3286 - accuracy: 0.8639\n",
      "Epoch 330/500\n",
      "80/80 [==============================] - 0s 844us/step - loss: 0.3286 - accuracy: 0.8633\n",
      "Epoch 331/500\n",
      "80/80 [==============================] - 0s 843us/step - loss: 0.3286 - accuracy: 0.8630\n",
      "Epoch 332/500\n",
      "80/80 [==============================] - 0s 844us/step - loss: 0.3285 - accuracy: 0.8627\n",
      "Epoch 333/500\n",
      "80/80 [==============================] - 0s 847us/step - loss: 0.3287 - accuracy: 0.8658\n",
      "Epoch 334/500\n",
      "80/80 [==============================] - 0s 845us/step - loss: 0.3285 - accuracy: 0.8634\n",
      "Epoch 335/500\n",
      "80/80 [==============================] - 0s 701us/step - loss: 0.3286 - accuracy: 0.8631\n",
      "Epoch 336/500\n",
      "80/80 [==============================] - 0s 854us/step - loss: 0.3285 - accuracy: 0.8634\n",
      "Epoch 337/500\n",
      "80/80 [==============================] - 0s 804us/step - loss: 0.3284 - accuracy: 0.8634\n",
      "Epoch 338/500\n",
      "80/80 [==============================] - 0s 852us/step - loss: 0.3285 - accuracy: 0.8636\n",
      "Epoch 339/500\n",
      "80/80 [==============================] - 0s 838us/step - loss: 0.3283 - accuracy: 0.8634\n",
      "Epoch 340/500\n",
      "80/80 [==============================] - 0s 845us/step - loss: 0.3285 - accuracy: 0.8654\n",
      "Epoch 341/500\n",
      "80/80 [==============================] - 0s 842us/step - loss: 0.3287 - accuracy: 0.8636\n",
      "Epoch 342/500\n",
      "80/80 [==============================] - 0s 845us/step - loss: 0.3292 - accuracy: 0.8649\n",
      "Epoch 343/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3285 - accuracy: 0.8640\n",
      "Epoch 344/500\n",
      "80/80 [==============================] - 0s 822us/step - loss: 0.3288 - accuracy: 0.8636\n",
      "Epoch 345/500\n",
      "80/80 [==============================] - 0s 845us/step - loss: 0.3284 - accuracy: 0.8627\n",
      "Epoch 346/500\n",
      "80/80 [==============================] - 0s 841us/step - loss: 0.3287 - accuracy: 0.8622\n",
      "Epoch 347/500\n",
      "80/80 [==============================] - 0s 858us/step - loss: 0.3287 - accuracy: 0.8622\n",
      "Epoch 348/500\n",
      "80/80 [==============================] - 0s 834us/step - loss: 0.3283 - accuracy: 0.8639\n",
      "Epoch 349/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3282 - accuracy: 0.8635\n",
      "Epoch 350/500\n",
      "80/80 [==============================] - 0s 787us/step - loss: 0.3283 - accuracy: 0.8636\n",
      "Epoch 351/500\n",
      "80/80 [==============================] - 0s 855us/step - loss: 0.3282 - accuracy: 0.8639\n",
      "Epoch 352/500\n",
      "80/80 [==============================] - 0s 833us/step - loss: 0.3283 - accuracy: 0.8649\n",
      "Epoch 353/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3282 - accuracy: 0.8637\n",
      "Epoch 354/500\n",
      "80/80 [==============================] - 0s 846us/step - loss: 0.3282 - accuracy: 0.8645\n",
      "Epoch 355/500\n",
      "80/80 [==============================] - 0s 851us/step - loss: 0.3283 - accuracy: 0.8650\n",
      "Epoch 356/500\n",
      "80/80 [==============================] - 0s 842us/step - loss: 0.3284 - accuracy: 0.8637\n",
      "Epoch 357/500\n",
      "80/80 [==============================] - 0s 846us/step - loss: 0.3285 - accuracy: 0.8654\n",
      "Epoch 358/500\n",
      "80/80 [==============================] - 0s 837us/step - loss: 0.3281 - accuracy: 0.8639\n",
      "Epoch 359/500\n",
      "80/80 [==============================] - 0s 851us/step - loss: 0.3285 - accuracy: 0.8625\n",
      "Epoch 360/500\n",
      "80/80 [==============================] - 0s 840us/step - loss: 0.3284 - accuracy: 0.8633\n",
      "Epoch 361/500\n",
      "80/80 [==============================] - 0s 843us/step - loss: 0.3283 - accuracy: 0.8636\n",
      "Epoch 362/500\n",
      "80/80 [==============================] - 0s 850us/step - loss: 0.3284 - accuracy: 0.8631\n",
      "Epoch 363/500\n",
      "80/80 [==============================] - 0s 838us/step - loss: 0.3281 - accuracy: 0.8641\n",
      "Epoch 364/500\n",
      "80/80 [==============================] - 0s 825us/step - loss: 0.3281 - accuracy: 0.8634\n",
      "Epoch 365/500\n",
      "80/80 [==============================] - 0s 832us/step - loss: 0.3283 - accuracy: 0.8636\n",
      "Epoch 366/500\n",
      "80/80 [==============================] - 0s 844us/step - loss: 0.3286 - accuracy: 0.8637\n",
      "Epoch 367/500\n",
      "80/80 [==============================] - 0s 843us/step - loss: 0.3283 - accuracy: 0.8631\n",
      "Epoch 368/500\n",
      "80/80 [==============================] - 0s 845us/step - loss: 0.3282 - accuracy: 0.8639\n",
      "Epoch 369/500\n",
      "80/80 [==============================] - 0s 849us/step - loss: 0.3286 - accuracy: 0.8639\n",
      "Epoch 370/500\n",
      "80/80 [==============================] - 0s 842us/step - loss: 0.3283 - accuracy: 0.8636\n",
      "Epoch 371/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3282 - accuracy: 0.8637\n",
      "Epoch 372/500\n",
      "80/80 [==============================] - 0s 817us/step - loss: 0.3279 - accuracy: 0.8664\n",
      "Epoch 373/500\n",
      "80/80 [==============================] - 0s 843us/step - loss: 0.3281 - accuracy: 0.8637\n",
      "Epoch 374/500\n",
      "80/80 [==============================] - 0s 846us/step - loss: 0.3281 - accuracy: 0.8643\n",
      "Epoch 375/500\n",
      "80/80 [==============================] - 0s 849us/step - loss: 0.3280 - accuracy: 0.8654\n",
      "Epoch 376/500\n",
      "80/80 [==============================] - 0s 864us/step - loss: 0.3281 - accuracy: 0.8650\n",
      "Epoch 377/500\n",
      "80/80 [==============================] - 0s 845us/step - loss: 0.3283 - accuracy: 0.8633\n",
      "Epoch 378/500\n",
      "80/80 [==============================] - 0s 991us/step - loss: 0.3282 - accuracy: 0.8634\n",
      "Epoch 379/500\n",
      "80/80 [==============================] - 0s 845us/step - loss: 0.3281 - accuracy: 0.8640\n",
      "Epoch 380/500\n",
      "80/80 [==============================] - 0s 848us/step - loss: 0.3281 - accuracy: 0.8645\n",
      "Epoch 381/500\n",
      "80/80 [==============================] - 0s 842us/step - loss: 0.3286 - accuracy: 0.8627\n",
      "Epoch 382/500\n",
      "80/80 [==============================] - 0s 844us/step - loss: 0.3284 - accuracy: 0.8631\n",
      "Epoch 383/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3280 - accuracy: 0.8633\n",
      "Epoch 384/500\n",
      "80/80 [==============================] - 0s 770us/step - loss: 0.3285 - accuracy: 0.8648\n",
      "Epoch 385/500\n",
      "80/80 [==============================] - 0s 848us/step - loss: 0.3278 - accuracy: 0.8640\n",
      "Epoch 386/500\n",
      "80/80 [==============================] - 0s 874us/step - loss: 0.3282 - accuracy: 0.8639\n",
      "Epoch 387/500\n",
      "80/80 [==============================] - 0s 846us/step - loss: 0.3281 - accuracy: 0.8635\n",
      "Epoch 388/500\n",
      "80/80 [==============================] - 0s 839us/step - loss: 0.3283 - accuracy: 0.8627\n",
      "Epoch 389/500\n",
      "80/80 [==============================] - 0s 844us/step - loss: 0.3279 - accuracy: 0.8620\n",
      "Epoch 390/500\n",
      "80/80 [==============================] - 0s 847us/step - loss: 0.3279 - accuracy: 0.8637\n",
      "Epoch 391/500\n",
      "80/80 [==============================] - 0s 844us/step - loss: 0.3278 - accuracy: 0.8636\n",
      "Epoch 392/500\n",
      "80/80 [==============================] - 0s 855us/step - loss: 0.3281 - accuracy: 0.8637\n",
      "Epoch 393/500\n",
      "80/80 [==============================] - 0s 861us/step - loss: 0.3287 - accuracy: 0.8633\n",
      "Epoch 394/500\n",
      "80/80 [==============================] - 0s 871us/step - loss: 0.3276 - accuracy: 0.8646\n",
      "Epoch 395/500\n",
      "80/80 [==============================] - 0s 819us/step - loss: 0.3280 - accuracy: 0.8637\n",
      "Epoch 396/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3281 - accuracy: 0.8641\n",
      "Epoch 397/500\n",
      "80/80 [==============================] - 0s 820us/step - loss: 0.3280 - accuracy: 0.8646\n",
      "Epoch 398/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 853us/step - loss: 0.3279 - accuracy: 0.8641\n",
      "Epoch 399/500\n",
      "80/80 [==============================] - 0s 822us/step - loss: 0.3285 - accuracy: 0.8631\n",
      "Epoch 400/500\n",
      "80/80 [==============================] - 0s 844us/step - loss: 0.3278 - accuracy: 0.8634\n",
      "Epoch 401/500\n",
      "80/80 [==============================] - 0s 852us/step - loss: 0.3279 - accuracy: 0.8639\n",
      "Epoch 402/500\n",
      "80/80 [==============================] - 0s 846us/step - loss: 0.3284 - accuracy: 0.8630\n",
      "Epoch 403/500\n",
      "80/80 [==============================] - 0s 840us/step - loss: 0.3283 - accuracy: 0.8630\n",
      "Epoch 404/500\n",
      "80/80 [==============================] - 0s 874us/step - loss: 0.3279 - accuracy: 0.8633\n",
      "Epoch 405/500\n",
      "80/80 [==============================] - 0s 814us/step - loss: 0.3280 - accuracy: 0.8637\n",
      "Epoch 406/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3279 - accuracy: 0.8637\n",
      "Epoch 407/500\n",
      "80/80 [==============================] - 0s 841us/step - loss: 0.3279 - accuracy: 0.8646\n",
      "Epoch 408/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3277 - accuracy: 0.8640\n",
      "Epoch 409/500\n",
      "80/80 [==============================] - 0s 819us/step - loss: 0.3278 - accuracy: 0.8643\n",
      "Epoch 410/500\n",
      "80/80 [==============================] - 0s 844us/step - loss: 0.3283 - accuracy: 0.8631\n",
      "Epoch 411/500\n",
      "80/80 [==============================] - 0s 883us/step - loss: 0.3283 - accuracy: 0.8630\n",
      "Epoch 412/500\n",
      "80/80 [==============================] - 0s 808us/step - loss: 0.3278 - accuracy: 0.8641\n",
      "Epoch 413/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3282 - accuracy: 0.8646\n",
      "Epoch 414/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3281 - accuracy: 0.8633\n",
      "Epoch 415/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3278 - accuracy: 0.8639\n",
      "Epoch 416/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3281 - accuracy: 0.8640\n",
      "Epoch 417/500\n",
      "80/80 [==============================] - 0s 903us/step - loss: 0.3281 - accuracy: 0.8631\n",
      "Epoch 418/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3276 - accuracy: 0.8636\n",
      "Epoch 419/500\n",
      "80/80 [==============================] - 0s 912us/step - loss: 0.3278 - accuracy: 0.8624\n",
      "Epoch 420/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3277 - accuracy: 0.8648\n",
      "Epoch 421/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3279 - accuracy: 0.8637\n",
      "Epoch 422/500\n",
      "80/80 [==============================] - 0s 916us/step - loss: 0.3277 - accuracy: 0.8636\n",
      "Epoch 423/500\n",
      "80/80 [==============================] - 0s 841us/step - loss: 0.3276 - accuracy: 0.8646\n",
      "Epoch 424/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3277 - accuracy: 0.8640\n",
      "Epoch 425/500\n",
      "80/80 [==============================] - 0s 988us/step - loss: 0.3276 - accuracy: 0.8648\n",
      "Epoch 426/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3278 - accuracy: 0.8635\n",
      "Epoch 427/500\n",
      "80/80 [==============================] - 0s 886us/step - loss: 0.3283 - accuracy: 0.8643\n",
      "Epoch 428/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3283 - accuracy: 0.8633\n",
      "Epoch 429/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3277 - accuracy: 0.8640\n",
      "Epoch 430/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3274 - accuracy: 0.8631\n",
      "Epoch 431/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3275 - accuracy: 0.8644\n",
      "Epoch 432/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3276 - accuracy: 0.8630\n",
      "Epoch 433/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3280 - accuracy: 0.8633\n",
      "Epoch 434/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3275 - accuracy: 0.8640\n",
      "Epoch 435/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3274 - accuracy: 0.8637\n",
      "Epoch 436/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3279 - accuracy: 0.8630\n",
      "Epoch 437/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3276 - accuracy: 0.8639\n",
      "Epoch 438/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3277 - accuracy: 0.8649\n",
      "Epoch 439/500\n",
      "80/80 [==============================] - 0s 915us/step - loss: 0.3279 - accuracy: 0.8649\n",
      "Epoch 440/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3278 - accuracy: 0.8631\n",
      "Epoch 441/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3277 - accuracy: 0.8633\n",
      "Epoch 442/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3280 - accuracy: 0.8633\n",
      "Epoch 443/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3277 - accuracy: 0.8645\n",
      "Epoch 444/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3280 - accuracy: 0.8643\n",
      "Epoch 445/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3277 - accuracy: 0.8626\n",
      "Epoch 446/500\n",
      "80/80 [==============================] - 0s 975us/step - loss: 0.3275 - accuracy: 0.8640\n",
      "Epoch 447/500\n",
      "80/80 [==============================] - 0s 983us/step - loss: 0.3276 - accuracy: 0.8645\n",
      "Epoch 448/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3277 - accuracy: 0.8643\n",
      "Epoch 449/500\n",
      "80/80 [==============================] - 0s 967us/step - loss: 0.3279 - accuracy: 0.8644\n",
      "Epoch 450/500\n",
      "80/80 [==============================] - 0s 825us/step - loss: 0.3275 - accuracy: 0.8640\n",
      "Epoch 451/500\n",
      "80/80 [==============================] - 0s 933us/step - loss: 0.3272 - accuracy: 0.8635\n",
      "Epoch 452/500\n",
      "80/80 [==============================] - 0s 892us/step - loss: 0.3278 - accuracy: 0.8637\n",
      "Epoch 453/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3276 - accuracy: 0.8630\n",
      "Epoch 454/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3277 - accuracy: 0.8641\n",
      "Epoch 455/500\n",
      "80/80 [==============================] - 0s 796us/step - loss: 0.3275 - accuracy: 0.8634\n",
      "Epoch 456/500\n",
      "80/80 [==============================] - 0s 935us/step - loss: 0.3277 - accuracy: 0.8629\n",
      "Epoch 457/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3280 - accuracy: 0.8637\n",
      "Epoch 458/500\n",
      "80/80 [==============================] - 0s 993us/step - loss: 0.3276 - accuracy: 0.8637\n",
      "Epoch 459/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3277 - accuracy: 0.8631\n",
      "Epoch 460/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3277 - accuracy: 0.8639\n",
      "Epoch 461/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3276 - accuracy: 0.8640\n",
      "Epoch 462/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3276 - accuracy: 0.8633\n",
      "Epoch 463/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3275 - accuracy: 0.8635\n",
      "Epoch 464/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3277 - accuracy: 0.8651\n",
      "Epoch 465/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3276 - accuracy: 0.8630\n",
      "Epoch 466/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3277 - accuracy: 0.8649\n",
      "Epoch 467/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3274 - accuracy: 0.8633\n",
      "Epoch 468/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3276 - accuracy: 0.8634\n",
      "Epoch 469/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3277 - accuracy: 0.8649\n",
      "Epoch 470/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3271 - accuracy: 0.8640\n",
      "Epoch 471/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3274 - accuracy: 0.8631\n",
      "Epoch 472/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3276 - accuracy: 0.8644\n",
      "Epoch 473/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3274 - accuracy: 0.8637\n",
      "Epoch 474/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3273 - accuracy: 0.8636\n",
      "Epoch 475/500\n",
      "80/80 [==============================] - 0s 762us/step - loss: 0.3284 - accuracy: 0.8637\n",
      "Epoch 476/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3274 - accuracy: 0.8634\n",
      "Epoch 477/500\n",
      "80/80 [==============================] - 0s 778us/step - loss: 0.3277 - accuracy: 0.8629\n",
      "Epoch 478/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 873us/step - loss: 0.3272 - accuracy: 0.8645\n",
      "Epoch 479/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3273 - accuracy: 0.8627\n",
      "Epoch 480/500\n",
      "80/80 [==============================] - 0s 797us/step - loss: 0.3273 - accuracy: 0.8640\n",
      "Epoch 481/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3279 - accuracy: 0.8633\n",
      "Epoch 482/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3273 - accuracy: 0.8637\n",
      "Epoch 483/500\n",
      "80/80 [==============================] - 0s 929us/step - loss: 0.3274 - accuracy: 0.8635\n",
      "Epoch 484/500\n",
      "80/80 [==============================] - 0s 879us/step - loss: 0.3276 - accuracy: 0.8634\n",
      "Epoch 485/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3275 - accuracy: 0.8636\n",
      "Epoch 486/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3274 - accuracy: 0.8645\n",
      "Epoch 487/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3273 - accuracy: 0.8654\n",
      "Epoch 488/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3276 - accuracy: 0.8641\n",
      "Epoch 489/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3277 - accuracy: 0.8641\n",
      "Epoch 490/500\n",
      "80/80 [==============================] - 0s 790us/step - loss: 0.3273 - accuracy: 0.8625\n",
      "Epoch 491/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3278 - accuracy: 0.8634\n",
      "Epoch 492/500\n",
      "80/80 [==============================] - 0s 849us/step - loss: 0.3275 - accuracy: 0.8643\n",
      "Epoch 493/500\n",
      "80/80 [==============================] - 0s 890us/step - loss: 0.3276 - accuracy: 0.8639\n",
      "Epoch 494/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3275 - accuracy: 0.8655\n",
      "Epoch 495/500\n",
      "80/80 [==============================] - 0s 949us/step - loss: 0.3273 - accuracy: 0.8645\n",
      "Epoch 496/500\n",
      "80/80 [==============================] - 0s 883us/step - loss: 0.3276 - accuracy: 0.8631\n",
      "Epoch 497/500\n",
      "80/80 [==============================] - 0s 930us/step - loss: 0.3278 - accuracy: 0.8643\n",
      "Epoch 498/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3276 - accuracy: 0.8635\n",
      "Epoch 499/500\n",
      "80/80 [==============================] - 0s 847us/step - loss: 0.3274 - accuracy: 0.8641\n",
      "Epoch 500/500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3272 - accuracy: 0.8640\n"
     ]
    }
   ],
   "source": [
    "#### Last step in creation of NNmodel. NNmodel is trained on the training set here with Tensor-Keras .fit based on Compiler\n",
    "#Fitting NNmodel\n",
    "history=NNmodel.fit(X_train,Y_train,batch_size=100,epochs = 500)\n",
    "### Note that tf.keras.models.Sequential() by default uses glorot initializer -- drawing intial weights from a uniform \n",
    "### distribution -- see other possibilities in https://keras.io/api/layers/initializers/\n",
    "### Or you could try own customized wts inputs using\n",
    "### for layer in model.layers:\n",
    "###    init_layer_weight = [] # the weights yourself in this layer\n",
    "###    layer.set_weights(init_layer_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "73398e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (100, 6)                  78        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (100, 6)                  42        \n",
      "                                                                 \n",
      " dense_2 (Dense)             (100, 6)                  42        \n",
      "                                                                 \n",
      " dense_3 (Dense)             (100, 1)                  7         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 169 (676.00 Byte)\n",
      "Trainable params: 169 (676.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NNmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3afe099c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.7973750233650208, 0.8015000224113464, 0.8087499737739563, 0.8153749704360962, 0.8201249837875366, 0.8211249709129333, 0.8206250071525574, 0.8228750228881836, 0.8240000009536743, 0.8247500061988831, 0.8271250128746033, 0.8270000219345093, 0.8287500143051147, 0.828499972820282, 0.8295000195503235, 0.8286250233650208, 0.8291249871253967, 0.8307499885559082, 0.8299999833106995, 0.8323749899864197, 0.8335000276565552, 0.8345000147819519, 0.8368750214576721, 0.8371250033378601, 0.8395000100135803, 0.8412500023841858, 0.8414999842643738, 0.8423749804496765, 0.8443750143051147, 0.8452500104904175, 0.8460000157356262, 0.8475000262260437, 0.8476250171661377, 0.8483750224113464, 0.8501250147819519, 0.8498749732971191, 0.8511250019073486, 0.8500000238418579, 0.8528749942779541, 0.8525000214576721, 0.8532500267028809, 0.8531249761581421, 0.8541250228881836, 0.8542500138282776, 0.8560000061988831, 0.8558750152587891, 0.8567500114440918, 0.8578749895095825, 0.8575000166893005, 0.8582500219345093, 0.8586249947547913, 0.8573750257492065, 0.8571249842643738, 0.8585000038146973, 0.8598750233650208, 0.859624981880188, 0.859375, 0.8585000038146973, 0.8613749742507935, 0.8604999780654907, 0.8607500195503235, 0.8616250157356262, 0.8613749742507935, 0.8617500066757202, 0.8615000247955322, 0.8619999885559082, 0.8612499833106995, 0.8628749847412109, 0.8629999756813049, 0.8633750081062317, 0.8612499833106995, 0.8629999756813049, 0.8615000247955322, 0.862625002861023, 0.8629999756813049, 0.8627499938011169, 0.8617500066757202, 0.8622499704360962, 0.8607500195503235, 0.8615000247955322, 0.8615000247955322, 0.8616250157356262, 0.862375020980835, 0.8619999885559082, 0.8603749871253967, 0.8622499704360962, 0.8613749742507935, 0.8618749976158142, 0.8621249794960022, 0.8627499938011169, 0.8619999885559082, 0.8613749742507935, 0.8632500171661377, 0.8621249794960022, 0.8604999780654907, 0.8628749847412109, 0.8628749847412109, 0.8622499704360962, 0.8615000247955322, 0.8621249794960022, 0.8615000247955322, 0.8637499809265137, 0.8632500171661377, 0.8627499938011169, 0.8631250262260437, 0.8632500171661377, 0.862625002861023, 0.862500011920929, 0.862375020980835, 0.8627499938011169, 0.8631250262260437, 0.862625002861023, 0.862500011920929, 0.8631250262260437, 0.8629999756813049, 0.8627499938011169, 0.8627499938011169, 0.8632500171661377, 0.8629999756813049, 0.862500011920929, 0.8627499938011169, 0.8618749976158142, 0.862625002861023, 0.8632500171661377, 0.8622499704360962, 0.8633750081062317, 0.8628749847412109, 0.8629999756813049, 0.8631250262260437, 0.8632500171661377, 0.8629999756813049, 0.8621249794960022, 0.862500011920929, 0.8621249794960022, 0.862375020980835, 0.8618749976158142, 0.8634999990463257, 0.8619999885559082, 0.8634999990463257, 0.8617500066757202, 0.862625002861023, 0.862375020980835, 0.862625002861023, 0.8627499938011169, 0.8634999990463257, 0.8632500171661377, 0.8629999756813049, 0.862500011920929, 0.8633750081062317, 0.8631250262260437, 0.8617500066757202, 0.8631250262260437, 0.8643749952316284, 0.8636249899864197, 0.8627499938011169, 0.862375020980835, 0.8612499833106995, 0.8629999756813049, 0.8627499938011169, 0.862500011920929, 0.8621249794960022, 0.8616250157356262, 0.8621249794960022, 0.8637499809265137, 0.862625002861023, 0.8627499938011169, 0.862375020980835, 0.8631250262260437, 0.862625002861023, 0.8627499938011169, 0.8629999756813049, 0.8629999756813049, 0.8619999885559082, 0.8619999885559082, 0.8621249794960022, 0.8642500042915344, 0.8632500171661377, 0.8619999885559082, 0.8632500171661377, 0.8613749742507935, 0.8631250262260437, 0.8616250157356262, 0.8613749742507935, 0.862375020980835, 0.8637499809265137, 0.8622499704360962, 0.8617500066757202, 0.8628749847412109, 0.8615000247955322, 0.8629999756813049, 0.8618749976158142, 0.8619999885559082, 0.8633750081062317, 0.8616250157356262, 0.8617500066757202, 0.8619999885559082, 0.8621249794960022, 0.8622499704360962, 0.8612499833106995, 0.8618749976158142, 0.8618749976158142, 0.8632500171661377, 0.8613749742507935, 0.8611249923706055, 0.8607500195503235, 0.8607500195503235, 0.8618749976158142, 0.8628749847412109, 0.862500011920929, 0.8633750081062317, 0.8621249794960022, 0.8629999756813049, 0.8641250133514404, 0.862375020980835, 0.8628749847412109, 0.8619999885559082, 0.8616250157356262, 0.862625002861023, 0.862500011920929, 0.862625002861023, 0.862625002861023, 0.8627499938011169, 0.862500011920929, 0.8613749742507935, 0.8618749976158142, 0.8628749847412109, 0.8606250286102295, 0.8616250157356262, 0.8622499704360962, 0.862500011920929, 0.8636249899864197, 0.8627499938011169, 0.862625002861023, 0.862500011920929, 0.862500011920929, 0.8634999990463257, 0.862500011920929, 0.8629999756813049, 0.8640000224113464, 0.862625002861023, 0.8617500066757202, 0.8631250262260437, 0.8622499704360962, 0.8622499704360962, 0.862375020980835, 0.862625002861023, 0.8622499704360962, 0.8629999756813049, 0.8628749847412109, 0.8640000224113464, 0.8613749742507935, 0.8637499809265137, 0.862625002861023, 0.8631250262260437, 0.862500011920929, 0.862375020980835, 0.8631250262260437, 0.8632500171661377, 0.862500011920929, 0.862375020980835, 0.8627499938011169, 0.8636249899864197, 0.8628749847412109, 0.8632500171661377, 0.862625002861023, 0.862375020980835, 0.8629999756813049, 0.8628749847412109, 0.8627499938011169, 0.8640000224113464, 0.8631250262260437, 0.8641250133514404, 0.8634999990463257, 0.862625002861023, 0.862625002861023, 0.8641250133514404, 0.862500011920929, 0.8622499704360962, 0.8629999756813049, 0.8627499938011169, 0.8634999990463257, 0.8637499809265137, 0.862500011920929, 0.8634999990463257, 0.862625002861023, 0.862625002861023, 0.8618749976158142, 0.8634999990463257, 0.8632500171661377, 0.8632500171661377, 0.8631250262260437, 0.8632500171661377, 0.8647500276565552, 0.8637499809265137, 0.8638749718666077, 0.8632500171661377, 0.8636249899864197, 0.8638749718666077, 0.8638749718666077, 0.8628749847412109, 0.8642500042915344, 0.8643749952316284, 0.8631250262260437, 0.8638749718666077, 0.8632500171661377, 0.8629999756813049, 0.8627499938011169, 0.8657500147819519, 0.8633750081062317, 0.8631250262260437, 0.8633750081062317, 0.8633750081062317, 0.8636249899864197, 0.8633750081062317, 0.8653749823570251, 0.8636249899864197, 0.8648750185966492, 0.8640000224113464, 0.8636249899864197, 0.8627499938011169, 0.8622499704360962, 0.8622499704360962, 0.8638749718666077, 0.8634999990463257, 0.8636249899864197, 0.8638749718666077, 0.8648750185966492, 0.8637499809265137, 0.8644999861717224, 0.8650000095367432, 0.8637499809265137, 0.8653749823570251, 0.8638749718666077, 0.862500011920929, 0.8632500171661377, 0.8636249899864197, 0.8631250262260437, 0.8641250133514404, 0.8633750081062317, 0.8636249899864197, 0.8637499809265137, 0.8631250262260437, 0.8638749718666077, 0.8638749718666077, 0.8636249899864197, 0.8637499809265137, 0.8663750290870667, 0.8637499809265137, 0.8642500042915344, 0.8653749823570251, 0.8650000095367432, 0.8632500171661377, 0.8633750081062317, 0.8640000224113464, 0.8644999861717224, 0.8627499938011169, 0.8631250262260437, 0.8632500171661377, 0.8647500276565552, 0.8640000224113464, 0.8638749718666077, 0.8634999990463257, 0.8627499938011169, 0.8619999885559082, 0.8637499809265137, 0.8636249899864197, 0.8637499809265137, 0.8632500171661377, 0.8646249771118164, 0.8637499809265137, 0.8641250133514404, 0.8646249771118164, 0.8641250133514404, 0.8631250262260437, 0.8633750081062317, 0.8638749718666077, 0.8629999756813049, 0.8629999756813049, 0.8632500171661377, 0.8637499809265137, 0.8637499809265137, 0.8646249771118164, 0.8640000224113464, 0.8642500042915344, 0.8631250262260437, 0.8629999756813049, 0.8641250133514404, 0.8646249771118164, 0.8632500171661377, 0.8638749718666077, 0.8640000224113464, 0.8631250262260437, 0.8636249899864197, 0.862375020980835, 0.8647500276565552, 0.8637499809265137, 0.8636249899864197, 0.8646249771118164, 0.8640000224113464, 0.8647500276565552, 0.8634999990463257, 0.8642500042915344, 0.8632500171661377, 0.8640000224113464, 0.8631250262260437, 0.8643749952316284, 0.8629999756813049, 0.8632500171661377, 0.8640000224113464, 0.8637499809265137, 0.8629999756813049, 0.8638749718666077, 0.8648750185966492, 0.8648750185966492, 0.8631250262260437, 0.8632500171661377, 0.8632500171661377, 0.8644999861717224, 0.8642500042915344, 0.862625002861023, 0.8640000224113464, 0.8644999861717224, 0.8642500042915344, 0.8643749952316284, 0.8640000224113464, 0.8634999990463257, 0.8637499809265137, 0.8629999756813049, 0.8641250133514404, 0.8633750081062317, 0.8628749847412109, 0.8637499809265137, 0.8637499809265137, 0.8631250262260437, 0.8638749718666077, 0.8640000224113464, 0.8632500171661377, 0.8634999990463257, 0.8651250004768372, 0.8629999756813049, 0.8648750185966492, 0.8632500171661377, 0.8633750081062317, 0.8648750185966492, 0.8640000224113464, 0.8631250262260437, 0.8643749952316284, 0.8637499809265137, 0.8636249899864197, 0.8637499809265137, 0.8633750081062317, 0.8628749847412109, 0.8644999861717224, 0.8627499938011169, 0.8640000224113464, 0.8632500171661377, 0.8637499809265137, 0.8634999990463257, 0.8633750081062317, 0.8636249899864197, 0.8644999861717224, 0.8653749823570251, 0.8641250133514404, 0.8641250133514404, 0.862500011920929, 0.8633750081062317, 0.8642500042915344, 0.8638749718666077, 0.8654999732971191, 0.8644999861717224, 0.8631250262260437, 0.8642500042915344, 0.8634999990463257, 0.8641250133514404, 0.8640000224113464]\n",
      "[0.5893016457557678, 0.5326760411262512, 0.5103490352630615, 0.503338098526001, 0.5006687641143799, 0.4986584484577179, 0.4964179992675781, 0.4936690926551819, 0.4901726245880127, 0.48568347096443176, 0.48010972142219543, 0.4735148549079895, 0.4662652313709259, 0.45885521173477173, 0.4521009624004364, 0.4459957480430603, 0.4408891499042511, 0.4369219243526459, 0.43379640579223633, 0.43146440386772156, 0.429727703332901, 0.4284154772758484, 0.42740383744239807, 0.4265585243701935, 0.4256289005279541, 0.4247536063194275, 0.4238203763961792, 0.4228481650352478, 0.42195960879325867, 0.4207391142845154, 0.4195077121257782, 0.41824427247047424, 0.41679707169532776, 0.4153968393802643, 0.4138506054878235, 0.4120217263698578, 0.4102395176887512, 0.4081365168094635, 0.40610170364379883, 0.4039492607116699, 0.40191763639450073, 0.3998189866542816, 0.39781054854393005, 0.3958270251750946, 0.39393892884254456, 0.3920172154903412, 0.3901055157184601, 0.38828614354133606, 0.38661280274391174, 0.3848974406719208, 0.3831782937049866, 0.38159456849098206, 0.3800228238105774, 0.37843382358551025, 0.3770548105239868, 0.3756897449493408, 0.37427303194999695, 0.37344470620155334, 0.3719380795955658, 0.37075889110565186, 0.3695344924926758, 0.3685770630836487, 0.36755651235580444, 0.3667295575141907, 0.3655540645122528, 0.3649452328681946, 0.3639896810054779, 0.3632229268550873, 0.3624051511287689, 0.36168432235717773, 0.3609168827533722, 0.3603725731372833, 0.35973215103149414, 0.35887008905410767, 0.35828647017478943, 0.3575718104839325, 0.3573157489299774, 0.35677289962768555, 0.35586774349212646, 0.3553571403026581, 0.35489416122436523, 0.35425469279289246, 0.35404521226882935, 0.353183776140213, 0.3527449369430542, 0.3522794842720032, 0.35148531198501587, 0.35104671120643616, 0.3507833182811737, 0.35046878457069397, 0.34970584511756897, 0.34931182861328125, 0.3487468361854553, 0.3485272228717804, 0.348002552986145, 0.347942590713501, 0.34734803438186646, 0.3470546007156372, 0.3463990092277527, 0.3460822105407715, 0.34562116861343384, 0.34523266553878784, 0.3450617492198944, 0.3447292149066925, 0.34436342120170593, 0.3441086709499359, 0.34388455748558044, 0.3434160053730011, 0.34339988231658936, 0.34292861819267273, 0.3428128957748413, 0.34165331721305847, 0.3421768546104431, 0.3417413830757141, 0.34136122465133667, 0.3410200774669647, 0.3412899971008301, 0.34059634804725647, 0.3403303921222687, 0.34018123149871826, 0.3397934138774872, 0.33977818489074707, 0.3393953740596771, 0.339042067527771, 0.3389439880847931, 0.33881163597106934, 0.33869093656539917, 0.3385627269744873, 0.3378196060657501, 0.33842289447784424, 0.3378240168094635, 0.3378659784793854, 0.3373605012893677, 0.3375045657157898, 0.3370955288410187, 0.3368073105812073, 0.33672043681144714, 0.33656153082847595, 0.3369047939777374, 0.3364008069038391, 0.33624106645584106, 0.3361591696739197, 0.33582597970962524, 0.3357302248477936, 0.3354988992214203, 0.3354959785938263, 0.3356790244579315, 0.33522796630859375, 0.33482107520103455, 0.33492612838745117, 0.3348923623561859, 0.33473101258277893, 0.33467018604278564, 0.33434292674064636, 0.3341612219810486, 0.33404502272605896, 0.3341163992881775, 0.3339810371398926, 0.3338016867637634, 0.3340528905391693, 0.33359986543655396, 0.333418071269989, 0.3336278200149536, 0.33378756046295166, 0.3329910337924957, 0.3335469365119934, 0.3331772983074188, 0.3335282504558563, 0.33291345834732056, 0.33293095231056213, 0.3329809308052063, 0.33270561695098877, 0.3328336179256439, 0.33277031779289246, 0.3324439227581024, 0.33239513635635376, 0.3325516879558563, 0.33254897594451904, 0.33209267258644104, 0.33217233419418335, 0.3320610225200653, 0.3321188986301422, 0.33172523975372314, 0.3319866955280304, 0.3317538797855377, 0.33181679248809814, 0.3321667015552521, 0.331963449716568, 0.33188706636428833, 0.33142390847206116, 0.33169934153556824, 0.3313586115837097, 0.3316454589366913, 0.33168303966522217, 0.3310859799385071, 0.3315502405166626, 0.33140841126441956, 0.33144620060920715, 0.33143216371536255, 0.3311767876148224, 0.3311811685562134, 0.33109724521636963, 0.3312057852745056, 0.330911785364151, 0.33081120252609253, 0.3307912349700928, 0.33080145716667175, 0.3308365046977997, 0.3306802809238434, 0.3304101824760437, 0.3304634988307953, 0.33084794878959656, 0.3309191167354584, 0.33055758476257324, 0.3305271863937378, 0.33049502968788147, 0.3304835557937622, 0.33038219809532166, 0.3307885527610779, 0.3308214545249939, 0.3301893472671509, 0.33034390211105347, 0.3303581476211548, 0.33006787300109863, 0.33035364747047424, 0.3307023048400879, 0.3299866020679474, 0.3304193615913391, 0.33044910430908203, 0.33003801107406616, 0.3301408290863037, 0.3301490843296051, 0.33039233088493347, 0.33062267303466797, 0.3304263949394226, 0.32968294620513916, 0.33027368783950806, 0.3299613893032074, 0.3300054371356964, 0.32998374104499817, 0.3298417627811432, 0.3299303352832794, 0.3300469219684601, 0.3297904431819916, 0.3297695219516754, 0.3300258219242096, 0.32960593700408936, 0.3300984501838684, 0.3297945261001587, 0.32985037565231323, 0.32977771759033203, 0.32995185256004333, 0.3295666575431824, 0.32972490787506104, 0.32925528287887573, 0.32937517762184143, 0.3295503854751587, 0.32957497239112854, 0.3294038474559784, 0.3296287953853607, 0.329343318939209, 0.32929959893226624, 0.3295656144618988, 0.32952895760536194, 0.32979345321655273, 0.3294098377227783, 0.32926538586616516, 0.32943615317344666, 0.32929936051368713, 0.32944706082344055, 0.3293522298336029, 0.32917264103889465, 0.3297501504421234, 0.3293074369430542, 0.32923775911331177, 0.3293907046318054, 0.32909876108169556, 0.3296632766723633, 0.3290734887123108, 0.32932716608047485, 0.3295953869819641, 0.3293035924434662, 0.32889246940612793, 0.329559862613678, 0.32909703254699707, 0.32924407720565796, 0.3287114202976227, 0.32902613282203674, 0.32889389991760254, 0.32915908098220825, 0.32897141575813293, 0.3289950489997864, 0.3290305435657501, 0.32913878560066223, 0.32906460762023926, 0.3291974663734436, 0.32930248975753784, 0.32901543378829956, 0.3288268744945526, 0.3292127251625061, 0.3289237320423126, 0.32932746410369873, 0.3288581073284149, 0.3288768529891968, 0.32903873920440674, 0.3286122679710388, 0.3288057744503021, 0.32894816994667053, 0.32894885540008545, 0.3286445140838623, 0.3288867473602295, 0.32890796661376953, 0.32860541343688965, 0.32876962423324585, 0.32861432433128357, 0.3291146755218506, 0.3286789655685425, 0.32868388295173645, 0.3287124037742615, 0.328671932220459, 0.3286444842815399, 0.3286820352077484, 0.3285931348800659, 0.3286491632461548, 0.3286229372024536, 0.32838305830955505, 0.32859382033348083, 0.3287106156349182, 0.3285985291004181, 0.32860425114631653, 0.32856979966163635, 0.3284860849380493, 0.32865458726882935, 0.3284913897514343, 0.32864758372306824, 0.3285087049007416, 0.32835111021995544, 0.3285224735736847, 0.32825273275375366, 0.3285035490989685, 0.3286837637424469, 0.32924675941467285, 0.3284817636013031, 0.32880374789237976, 0.3283960819244385, 0.32870569825172424, 0.32873108983039856, 0.3283260464668274, 0.328208863735199, 0.3283059298992157, 0.32824909687042236, 0.3283251225948334, 0.3282422423362732, 0.3282058537006378, 0.32827484607696533, 0.32839271426200867, 0.32851508259773254, 0.328063040971756, 0.32851293683052063, 0.32837381958961487, 0.3283122479915619, 0.32840242981910706, 0.32809513807296753, 0.3281013071537018, 0.3282659351825714, 0.3286396563053131, 0.3283092677593231, 0.3282173275947571, 0.3286442756652832, 0.3282546103000641, 0.32820090651512146, 0.3279319703578949, 0.3281398415565491, 0.32813167572021484, 0.32804128527641296, 0.3280680477619171, 0.3282632529735565, 0.328159898519516, 0.3280605375766754, 0.3281145691871643, 0.3285534679889679, 0.3284275531768799, 0.32796597480773926, 0.32846805453300476, 0.3278278112411499, 0.3282032907009125, 0.32813507318496704, 0.32834291458129883, 0.32794296741485596, 0.3279390335083008, 0.32778942584991455, 0.3280739486217499, 0.32869207859039307, 0.32756802439689636, 0.32803112268447876, 0.3280968964099884, 0.3280102610588074, 0.32787755131721497, 0.3284718096256256, 0.3277975022792816, 0.3279268443584442, 0.32843759655952454, 0.3282805383205414, 0.3278687298297882, 0.32799142599105835, 0.32793405652046204, 0.3279482424259186, 0.32774117588996887, 0.3277896046638489, 0.32825326919555664, 0.3283114433288574, 0.3278246223926544, 0.3281918168067932, 0.3280635178089142, 0.32777637243270874, 0.3280503451824188, 0.32806313037872314, 0.3276181221008301, 0.3278172016143799, 0.3276766240596771, 0.3279102146625519, 0.32765862345695496, 0.32756665349006653, 0.3277151584625244, 0.327619731426239, 0.3278488218784332, 0.32832565903663635, 0.32828909158706665, 0.32774919271469116, 0.3273782730102539, 0.3275235593318939, 0.3276342749595642, 0.32798221707344055, 0.3274751603603363, 0.3273589015007019, 0.32792285084724426, 0.3276039659976959, 0.32771846652030945, 0.3278648853302002, 0.3277891278266907, 0.3277023732662201, 0.32796406745910645, 0.3277052044868469, 0.32798606157302856, 0.3277135193347931, 0.3275153636932373, 0.3275858461856842, 0.3276689052581787, 0.3278818428516388, 0.32746973633766174, 0.3271884620189667, 0.32779014110565186, 0.327566921710968, 0.3277299106121063, 0.32754355669021606, 0.32771191000938416, 0.32797420024871826, 0.32764241099357605, 0.3277255594730377, 0.32770833373069763, 0.32760584354400635, 0.32757461071014404, 0.32751360535621643, 0.3276951313018799, 0.32758575677871704, 0.32771193981170654, 0.327419638633728, 0.3275620937347412, 0.327743798494339, 0.32708320021629333, 0.327405720949173, 0.3275656998157501, 0.32744452357292175, 0.3273259699344635, 0.3283930718898773, 0.32743024826049805, 0.32767510414123535, 0.3271813988685608, 0.3273002803325653, 0.32731714844703674, 0.32788699865341187, 0.3272692561149597, 0.32740235328674316, 0.3275509774684906, 0.3274751603603363, 0.3274230360984802, 0.3273482918739319, 0.3275695741176605, 0.32771652936935425, 0.32729434967041016, 0.32783758640289307, 0.32747259736061096, 0.3276413083076477, 0.3274913728237152, 0.32725775241851807, 0.3275904059410095, 0.3278225064277649, 0.3275874853134155, 0.32744696736335754, 0.3271850049495697]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(history.history['accuracy'])\n",
    "print(history.history['loss'])\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy and loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['accuracy', 'loss'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0be7271f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 698us/step - loss: 0.3269 - accuracy: 0.8645\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.32685568928718567, 0.8644999861717224]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Now we use the trained NNmodel to predict output in X_train sample\n",
    "NNmodel.evaluate(X_train,Y_train)  ### evaluates the loss and accuracy as specified in the Compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5ba646c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 647us/step\n"
     ]
    }
   ],
   "source": [
    "### Now we use the trained NNmodel to predict output in X_train sample -- computing manually via .predict\n",
    "TE=NNmodel.predict(X_train)  ### note X_train has 8000 data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "82ce3372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TE.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b21a584d",
   "metadata": {},
   "outputs": [],
   "source": [
    "h=(TE > 0.5).astype(int) ### Convert TE>0.5 == true ==> 1, False to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bb651f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8000, 1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(h)\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4a697768",
   "metadata": {},
   "outputs": [],
   "source": [
    "### replace all elements in numpy array of value 0 with value -1\n",
    "h[h==0]=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c9d41502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " ...\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8000, 1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(h)\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e8b950e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### replace all elements in numpy array of value 0 with value -1\n",
    "Y_train1=Y_train\n",
    "Y_train1[Y_train1==0]=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "53b50eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1 -1  1 ...  1 -1  1]\n"
     ]
    }
   ],
   "source": [
    "print(Y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ec6af287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fe3de8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6916 0.8645\n"
     ]
    }
   ],
   "source": [
    "J=np.multiply(Y_train1.T,h.T)  ### element by element multiplication.\n",
    "### Note Y_train1 1 vs fitted h 1 gives +1,Y_train1 -1 vs fitted h -1 gives +1; these are correct predictions\n",
    "###  but Y_train1 -1 vs fitted h 1 gives -1,Y_train1 -1 vs fitted h 1 gives -1; these are incorrect predictions\n",
    "c=np.count_nonzero(J > 0) \n",
    "print(c,c/8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4416dde4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 805us/step - loss: 0.3279 - accuracy: 0.8675\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.32790276408195496, 0.8675000071525574]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Now we use the trained NNmodel to predict output in X_test sample\n",
    "NNmodel.evaluate(X_test,Y_test)  ### evaluates the loss and accuracy as specified in the Compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f34f4bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 861us/step\n"
     ]
    }
   ],
   "source": [
    "### Now we use the trained NNmodel to predict output in X_test sample -- computing manually via .predict\n",
    "TE1=NNmodel.predict(X_test)  ### note X_test has 2000 data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "14712483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1735 0.8675\n"
     ]
    }
   ],
   "source": [
    "Y_test1 = np.copy(Y_test)\n",
    "\n",
    "h1=(TE1 > 0.5).astype(int) ### Convert TE1>0.5 == true ==> 1, False to 0\n",
    "h1[h1==0]=-1\n",
    "#Y_test1=Y_test strangely this turns Y_test into Y_test1 later, so we use copy on first line to avoid this\n",
    "Y_test1[Y_test1==0]=-1\n",
    "J1=np.multiply(Y_test1.T,h1.T)  ### element by element multiplication\n",
    "### Note Y_test1 1 vs fitted h 1 gives +1, Y_test1 -1 vs fitted h -1 gives +1; these are correct predictions\n",
    "###  but Y_train -1 vs fitted h 1 gives -1, Y_test1 -1 vs fitted h 1 gives -1; these are incorrect predictions\n",
    "c1=np.count_nonzero(J1 > 0) \n",
    "print(c1,c1/2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "18c17b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(Y_test) ### check that Y_test is not altered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f09fbbc",
   "metadata": {},
   "source": [
    "Predicting a new case - generalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5667ff07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-9.98501123e-01 -5.83123916e-01  1.74601919e+00 -3.24425289e-03\n",
      "   9.15090654e-01  2.01683866e+00 -1.04828130e+00  3.58411758e+00\n",
      "   8.02256955e-01  6.43760170e-01 -1.02327069e+00 -3.49886744e-01]]\n"
     ]
    }
   ],
   "source": [
    "### Now we use the trained weights and biases to try to predict based on a new case\n",
    "tr=sc.transform([[0, 0, 1, 650, 1, 60, 2, 300000, 2, 1, 0, 80000]])\n",
    "print(tr)  ### tr.shape is (1,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9c0874c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n",
      "[[0.72948253]]\n"
     ]
    }
   ],
   "source": [
    "### Example\n",
    "### Predicting result for Single Observation\n",
    "print(NNmodel.predict(tr))\n",
    "### note in each recompute -- this no. will change slightly because of the random initiation of the weights unless we fix\n",
    "###  the keras random seed as in code line [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5da9e9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Since prob = 0.7295 (> 0.5) of Y=1, yes, prediction of exit within a year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3cae7cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr1=[[0, 0, 1, 650, 1, 60, 2, 300000, 2, 1, 0, 80000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aea397bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 53ms/step\n",
      "[[0.0060353]]\n"
     ]
    }
   ],
   "source": [
    "print(NNmodel.predict(tr1)) \n",
    "### we get a contrasting wrong result if we do not standardize tr\n",
    "###  since the trained NN is based on standardized features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a436d3",
   "metadata": {},
   "source": [
    "Compute Confusion Matrix. Classification Report, and ROC AUC below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5768b210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0397587  0.07301321 0.04221635 ... 0.01421291 0.04618154 0.5515203 ]\n"
     ]
    }
   ],
   "source": [
    "### recall TE1=NNmodel.predict(X_test)\n",
    "import numpy as np\n",
    "TEf = TE1.flatten() ### convert a 2D numpy array to 1D for input to sklearn\n",
    "print(TEf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8c7969a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "hTEf=(TEf > 0.5).astype(int) ### Convert TE1>0.5 == true ==> 1, False to 0\n",
    "print(hTEf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a81f9c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1535   50]\n",
      " [ 215  200]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(Y_test, hTEf)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2d336d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.97      0.92      1585\n",
      "           1       0.80      0.48      0.60       415\n",
      "\n",
      "    accuracy                           0.87      2000\n",
      "   macro avg       0.84      0.73      0.76      2000\n",
      "weighted avg       0.86      0.87      0.85      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(Y_test, hTEf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "da276c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0397587  0.07301321 0.04221635 ... 0.01421291 0.04618154 0.5515203 ]\n"
     ]
    }
   ],
   "source": [
    "### rem in keras, TE1=NNmodel.predict(X_test) already gives the predicted probs, TEf is also probs\n",
    "print(TEf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "72cd1317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8747596062483372"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "# calculate the fpr and tpr for all thresholds of the classification\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(Y_test, TEf)  \n",
    "    ### matches y_test of 1's and 0's versus pred prob of 1's for each of the 508 test cases\n",
    "    ###    sklearn.metrics.roc_curve(y_true, y_score,...) requires y_true as 0,1 input and y_score as prob inputs\n",
    "    ###    this metrics.roc_curve returns fpr, tpr, thresholds (Decreasing thresholds used to compute fpr and tpr)\n",
    "roc_auc_NN = metrics.auc(fpr, tpr)\n",
    "    ### sklearn.metrics.auc(fpr,tpr) returns AUC using trapezoidal rule\n",
    "roc_auc_NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8b80a2e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc_NN)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb328f91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
