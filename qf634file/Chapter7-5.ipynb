{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c65022cc",
   "metadata": {},
   "source": [
    "In this model -- same adam, sigmoids, 12-4-4-1 structure, batch_size 100, epoch 500. Thus 40,000 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "617ee135",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c6deff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Dataset\n",
    "data = pd.read_csv(\"Churn_Modelling.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a80540e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CreditScore Geography  Gender  Age  Tenure    Balance  NumOfProducts  \\\n",
       "0          619    France  Female   42       2       0.00              1   \n",
       "1          608     Spain  Female   41       1   83807.86              1   \n",
       "2          502    France  Female   42       8  159660.80              3   \n",
       "3          699    France  Female   39       1       0.00              2   \n",
       "4          850     Spain  Female   43       2  125510.82              1   \n",
       "\n",
       "   HasCrCard  IsActiveMember  EstimatedSalary  \n",
       "0          1               1        101348.88  \n",
       "1          0               1        112542.58  \n",
       "2          1               0        113931.57  \n",
       "3          0               0         93826.63  \n",
       "4          1               1         79084.10  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generating Dependent Variable Vectors\n",
    "Y = data.iloc[:,-1].values\n",
    "X = data.iloc[:,3:13]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a98c381c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0\n",
      "1       0\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "       ..\n",
      "9995    1\n",
      "9996    1\n",
      "9997    0\n",
      "9998    1\n",
      "9999    0\n",
      "Name: Gender, Length: 10000, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Generating Dependent Variable Vectors\n",
    "Y = data.iloc[:,-1].values\n",
    "X = data.iloc[:,3:13]\n",
    "X['Gender']=X['Gender'].map({'Female':0,'Male':1})\n",
    "### above is used instead of a more complicated package involving -- from sklearn.preprocessing import LabelEncoder\n",
    "### converts Female -- 0, Male -- 1, i.e. hot-encoding categorical variables\n",
    "print (X['Gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e436f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding Categorical variable Geography\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ct =ColumnTransformer(transformers=[('encoder',OneHotEncoder(),[1])],remainder=\"passthrough\")\n",
    "X = np.array(ct.fit_transform(X))\n",
    "### Geography is transformed into France -- 1,0,0; Spain -- 0,0,1; Germany -- 0,1,0.\n",
    "### Moreover -- this encoded vector of ones-zeros is now put in first 3 cols. Credit Score pushed to 4th col."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3166a509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>619.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101348.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>608.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>112542.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>502.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>113931.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>699.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93826.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>850.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>79084.10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0    1    2      3    4     5    6          7    8    9    10         11\n",
       "0  1.0  0.0  0.0  619.0  0.0  42.0  2.0       0.00  1.0  1.0  1.0  101348.88\n",
       "1  0.0  0.0  1.0  608.0  0.0  41.0  1.0   83807.86  1.0  0.0  1.0  112542.58\n",
       "2  1.0  0.0  0.0  502.0  0.0  42.0  8.0  159660.80  3.0  1.0  0.0  113931.57\n",
       "3  1.0  0.0  0.0  699.0  0.0  39.0  1.0       0.00  2.0  0.0  0.0   93826.63\n",
       "4  0.0  0.0  1.0  850.0  0.0  43.0  2.0  125510.82  1.0  1.0  1.0   79084.10"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### convert X to dataframe X1\n",
    "X1 = pd.DataFrame(X)\n",
    "X1.head()\n",
    "### Note there are 12 features including onehotencoder for the Geography feature-- \n",
    "### The features are encoded using a one-hot (aka ‘one-of-K’ or ‘dummy’) encoding scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ef915ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting dataset into training and testing dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "740ede9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9850b84b",
   "metadata": {},
   "source": [
    "We call fit_transform() method on our training data and transform() method on our test data. Each feature in the training\n",
    "set is scaled to mean 0, variance 1. In sklearn.preprocessing.StandardScaler(), centering and scaling happens independently on each feature. The fit method is calculating the mean and variance of each of the features present in the data. The transform method is transforming all the features using the respective feature's mean and variance that are calculated in the statement\n",
    "before on X_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f43f1db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### This is the very first step while creating NNmodel. Here we are going to create our ann object by using a certain class of Keras \n",
    "### named Sequential. As a part of tensorflow 2.0, Keras is now integrated with tensorflow and is now considered as a \n",
    "### sub-library of tensorflow. The Sequential class is a part of the models module of Keras library which is a part of the \n",
    "### tensorflow library now. \n",
    "### It used to be \"import tensorflow as tf; from tensorflow import keras; from tensorflow.keras import layers\"\n",
    "### See documentation at https://keras.io/guides/sequential_model/\n",
    "\n",
    "#Initialising the NN model name -- NNmodel\n",
    "NNmodel = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34730cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating a network that has 2 hidden layers together with 1 input layer and 1 output layer. \n",
    "#Adding First Hidden Layer\n",
    "NNmodel.add(tf.keras.layers.Dense(units=4,activation=\"sigmoid\"))\n",
    "### units = 4 refer to 4 neurons in hidden layer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab12edcf",
   "metadata": {},
   "source": [
    "Above -- first hidden layer is created using the Dense class which is part of the layers module. This class accepts 2 inputs:-\n",
    "(1) units:- number of neurons that will be present in the respective layer (2) activation:- specify which activation function to be used. This example uses first input as 4. There is no correct answer which is the right number of neurons in the layer -- trial and error. Not too large to be computationally impractical or redundant; not too small to be ineffective.\n",
    "For the second input, we try the sigmoid or logistic function as an activation function for hidden layers. We can also try “relu”[rectified linear unit]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e22a225d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating 2nd hidden layer \n",
    "#Adding Second Hidden Layer -- note this is added sequentially to the first hidden layer\n",
    "NNmodel.add(tf.keras.layers.Dense(units=4,activation=\"sigmoid\"))\n",
    "### units = 4 refer to 4 neurons in hidden layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "303997d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### now we create the output layer -- this is added sequentially\n",
    "#Adding Output Layer\n",
    "NNmodel.add(tf.keras.layers.Dense(units=1,activation=\"sigmoid\"))\n",
    "### Only 1 output neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b4a623",
   "metadata": {},
   "source": [
    "For a binary classification problem as above, actual case output is 1 or 0. Hence we require only one neuron to output layer - output could be estimated probability of case actual output = 1. For multiclass classification problem, if the output contains m categories then we need to create m different neurons, one for each category. In the binary output case, the suitable activation function is the sigmoid function. For multiclass classification problem, the activation function is typically softmax. The softmax function predicts a multinomial probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70baaa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "### After creating the layers -- require compiling the NNmodel. Compiling allows the computer to run and understand the program \n",
    "### without the need of more fundamental steps in the programming. Compiling adds other elements or linking other libraries, and optimization,\n",
    "### such that after compiling the results are readily computed e.g. in a binary executable program as an output. \n",
    "#Compiling NNmodel\n",
    "NNmodel.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=['accuracy'])\n",
    "### Note optimizer here is a more sophisticated version of the Mean Square loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f9d245",
   "metadata": {},
   "source": [
    "Compile method above accepts inputs: (1) optimizer:- specifies which optimizer to be used in order to perform stochastic gradient descent (2) error/loss function, e.g., 'binary_crossentropy' here. For multiclass classification, it should be categorical_crossentropy, (3) metrics - the performance metrics to use in order to compute performance. 'accuracy' is one such  performance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51697cc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 12)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36a93b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "80/80 [==============================] - 0s 694us/step - loss: 0.6838 - accuracy: 0.5627\n",
      "Epoch 2/500\n",
      "80/80 [==============================] - 0s 559us/step - loss: 0.6002 - accuracy: 0.7972\n",
      "Epoch 3/500\n",
      "80/80 [==============================] - 0s 671us/step - loss: 0.5487 - accuracy: 0.7972\n",
      "Epoch 4/500\n",
      "80/80 [==============================] - 0s 618us/step - loss: 0.5194 - accuracy: 0.7972\n",
      "Epoch 5/500\n",
      "80/80 [==============================] - 0s 561us/step - loss: 0.5028 - accuracy: 0.7972\n",
      "Epoch 6/500\n",
      "80/80 [==============================] - 0s 628us/step - loss: 0.4934 - accuracy: 0.7972\n",
      "Epoch 7/500\n",
      "80/80 [==============================] - 0s 573us/step - loss: 0.4878 - accuracy: 0.7972\n",
      "Epoch 8/500\n",
      "80/80 [==============================] - 0s 552us/step - loss: 0.4839 - accuracy: 0.7972\n",
      "Epoch 9/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.4810 - accuracy: 0.7972\n",
      "Epoch 10/500\n",
      "80/80 [==============================] - 0s 621us/step - loss: 0.4782 - accuracy: 0.7972\n",
      "Epoch 11/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.4754 - accuracy: 0.7972\n",
      "Epoch 12/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.4723 - accuracy: 0.7972\n",
      "Epoch 13/500\n",
      "80/80 [==============================] - 0s 548us/step - loss: 0.4690 - accuracy: 0.7972\n",
      "Epoch 14/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.4654 - accuracy: 0.7972\n",
      "Epoch 15/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.4617 - accuracy: 0.7972\n",
      "Epoch 16/500\n",
      "80/80 [==============================] - 0s 563us/step - loss: 0.4581 - accuracy: 0.7972\n",
      "Epoch 17/500\n",
      "80/80 [==============================] - 0s 582us/step - loss: 0.4545 - accuracy: 0.7972\n",
      "Epoch 18/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.4510 - accuracy: 0.7972\n",
      "Epoch 19/500\n",
      "80/80 [==============================] - 0s 565us/step - loss: 0.4477 - accuracy: 0.7972\n",
      "Epoch 20/500\n",
      "80/80 [==============================] - 0s 574us/step - loss: 0.4447 - accuracy: 0.7972\n",
      "Epoch 21/500\n",
      "80/80 [==============================] - 0s 582us/step - loss: 0.4418 - accuracy: 0.7972\n",
      "Epoch 22/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.4394 - accuracy: 0.7972\n",
      "Epoch 23/500\n",
      "80/80 [==============================] - 0s 562us/step - loss: 0.4371 - accuracy: 0.7972\n",
      "Epoch 24/500\n",
      "80/80 [==============================] - 0s 571us/step - loss: 0.4352 - accuracy: 0.7972\n",
      "Epoch 25/500\n",
      "80/80 [==============================] - 0s 567us/step - loss: 0.4335 - accuracy: 0.7972\n",
      "Epoch 26/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.4320 - accuracy: 0.7972\n",
      "Epoch 27/500\n",
      "80/80 [==============================] - 0s 568us/step - loss: 0.4308 - accuracy: 0.7972\n",
      "Epoch 28/500\n",
      "80/80 [==============================] - 0s 574us/step - loss: 0.4297 - accuracy: 0.7972\n",
      "Epoch 29/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.4288 - accuracy: 0.7972\n",
      "Epoch 30/500\n",
      "80/80 [==============================] - 0s 549us/step - loss: 0.4280 - accuracy: 0.7972\n",
      "Epoch 31/500\n",
      "80/80 [==============================] - 0s 576us/step - loss: 0.4273 - accuracy: 0.7976\n",
      "Epoch 32/500\n",
      "80/80 [==============================] - 0s 582us/step - loss: 0.4266 - accuracy: 0.8008\n",
      "Epoch 33/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.4259 - accuracy: 0.8083\n",
      "Epoch 34/500\n",
      "80/80 [==============================] - 0s 561us/step - loss: 0.4253 - accuracy: 0.8091\n",
      "Epoch 35/500\n",
      "80/80 [==============================] - 0s 571us/step - loss: 0.4246 - accuracy: 0.8135\n",
      "Epoch 36/500\n",
      "80/80 [==============================] - 0s 594us/step - loss: 0.4239 - accuracy: 0.8148\n",
      "Epoch 37/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.4233 - accuracy: 0.8177\n",
      "Epoch 38/500\n",
      "80/80 [==============================] - 0s 566us/step - loss: 0.4225 - accuracy: 0.8184\n",
      "Epoch 39/500\n",
      "80/80 [==============================] - 0s 563us/step - loss: 0.4219 - accuracy: 0.8199\n",
      "Epoch 40/500\n",
      "80/80 [==============================] - 0s 571us/step - loss: 0.4211 - accuracy: 0.8209\n",
      "Epoch 41/500\n",
      "80/80 [==============================] - 0s 556us/step - loss: 0.4204 - accuracy: 0.8221\n",
      "Epoch 42/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.4196 - accuracy: 0.8230\n",
      "Epoch 43/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.4189 - accuracy: 0.8249\n",
      "Epoch 44/500\n",
      "80/80 [==============================] - 0s 561us/step - loss: 0.4182 - accuracy: 0.8256\n",
      "Epoch 45/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.4176 - accuracy: 0.8270\n",
      "Epoch 46/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.4169 - accuracy: 0.8273\n",
      "Epoch 47/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.4163 - accuracy: 0.8278\n",
      "Epoch 48/500\n",
      "80/80 [==============================] - 0s 573us/step - loss: 0.4158 - accuracy: 0.8286\n",
      "Epoch 49/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.4153 - accuracy: 0.8292\n",
      "Epoch 50/500\n",
      "80/80 [==============================] - 0s 543us/step - loss: 0.4148 - accuracy: 0.8294\n",
      "Epoch 51/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.4142 - accuracy: 0.8299\n",
      "Epoch 52/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.4137 - accuracy: 0.8299\n",
      "Epoch 53/500\n",
      "80/80 [==============================] - 0s 572us/step - loss: 0.4132 - accuracy: 0.8295\n",
      "Epoch 54/500\n",
      "80/80 [==============================] - 0s 573us/step - loss: 0.4127 - accuracy: 0.8300\n",
      "Epoch 55/500\n",
      "80/80 [==============================] - 0s 554us/step - loss: 0.4122 - accuracy: 0.8311\n",
      "Epoch 56/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.4118 - accuracy: 0.8316\n",
      "Epoch 57/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.4114 - accuracy: 0.8298\n",
      "Epoch 58/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.4110 - accuracy: 0.8303\n",
      "Epoch 59/500\n",
      "80/80 [==============================] - 0s 573us/step - loss: 0.4105 - accuracy: 0.8311\n",
      "Epoch 60/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.4103 - accuracy: 0.8319\n",
      "Epoch 61/500\n",
      "80/80 [==============================] - 0s 569us/step - loss: 0.4098 - accuracy: 0.8310\n",
      "Epoch 62/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.4094 - accuracy: 0.8317\n",
      "Epoch 63/500\n",
      "80/80 [==============================] - 0s 559us/step - loss: 0.4091 - accuracy: 0.8325\n",
      "Epoch 64/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.4087 - accuracy: 0.8324\n",
      "Epoch 65/500\n",
      "80/80 [==============================] - 0s 545us/step - loss: 0.4085 - accuracy: 0.8331\n",
      "Epoch 66/500\n",
      "80/80 [==============================] - 0s 595us/step - loss: 0.4081 - accuracy: 0.8335\n",
      "Epoch 67/500\n",
      "80/80 [==============================] - 0s 569us/step - loss: 0.4078 - accuracy: 0.8339\n",
      "Epoch 68/500\n",
      "80/80 [==============================] - 0s 568us/step - loss: 0.4076 - accuracy: 0.8338\n",
      "Epoch 69/500\n",
      "80/80 [==============================] - 0s 559us/step - loss: 0.4072 - accuracy: 0.8349\n",
      "Epoch 70/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.4069 - accuracy: 0.8345\n",
      "Epoch 71/500\n",
      "80/80 [==============================] - 0s 569us/step - loss: 0.4067 - accuracy: 0.8346\n",
      "Epoch 72/500\n",
      "80/80 [==============================] - 0s 559us/step - loss: 0.4065 - accuracy: 0.8338\n",
      "Epoch 73/500\n",
      "80/80 [==============================] - 0s 573us/step - loss: 0.4064 - accuracy: 0.8347\n",
      "Epoch 74/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.4061 - accuracy: 0.8347\n",
      "Epoch 75/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.4058 - accuracy: 0.8340\n",
      "Epoch 76/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.4057 - accuracy: 0.8344\n",
      "Epoch 77/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.4056 - accuracy: 0.8345\n",
      "Epoch 78/500\n",
      "80/80 [==============================] - 0s 562us/step - loss: 0.4053 - accuracy: 0.8342\n",
      "Epoch 79/500\n",
      "80/80 [==============================] - 0s 556us/step - loss: 0.4051 - accuracy: 0.8347\n",
      "Epoch 80/500\n",
      "80/80 [==============================] - 0s 549us/step - loss: 0.4049 - accuracy: 0.8350\n",
      "Epoch 81/500\n",
      "80/80 [==============================] - 0s 554us/step - loss: 0.4048 - accuracy: 0.8349\n",
      "Epoch 82/500\n",
      "80/80 [==============================] - 0s 561us/step - loss: 0.4046 - accuracy: 0.8354\n",
      "Epoch 83/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.4045 - accuracy: 0.8351\n",
      "Epoch 84/500\n",
      "80/80 [==============================] - 0s 544us/step - loss: 0.4045 - accuracy: 0.8356\n",
      "Epoch 85/500\n",
      "80/80 [==============================] - 0s 595us/step - loss: 0.4043 - accuracy: 0.8357\n",
      "Epoch 86/500\n",
      "80/80 [==============================] - 0s 573us/step - loss: 0.4041 - accuracy: 0.8351\n",
      "Epoch 87/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.4041 - accuracy: 0.8354\n",
      "Epoch 88/500\n",
      "80/80 [==============================] - 0s 552us/step - loss: 0.4039 - accuracy: 0.8354\n",
      "Epoch 89/500\n",
      "80/80 [==============================] - 0s 549us/step - loss: 0.4038 - accuracy: 0.8355\n",
      "Epoch 90/500\n",
      "80/80 [==============================] - 0s 571us/step - loss: 0.4037 - accuracy: 0.8355\n",
      "Epoch 91/500\n",
      "80/80 [==============================] - 0s 561us/step - loss: 0.4036 - accuracy: 0.8359\n",
      "Epoch 92/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.4034 - accuracy: 0.8360\n",
      "Epoch 93/500\n",
      "80/80 [==============================] - 0s 546us/step - loss: 0.4033 - accuracy: 0.8363\n",
      "Epoch 94/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.4032 - accuracy: 0.8359\n",
      "Epoch 95/500\n",
      "80/80 [==============================] - 0s 571us/step - loss: 0.4031 - accuracy: 0.8371\n",
      "Epoch 96/500\n",
      "80/80 [==============================] - 0s 573us/step - loss: 0.4031 - accuracy: 0.8365\n",
      "Epoch 97/500\n",
      "80/80 [==============================] - 0s 559us/step - loss: 0.4029 - accuracy: 0.8370\n",
      "Epoch 98/500\n",
      "80/80 [==============================] - 0s 556us/step - loss: 0.4030 - accuracy: 0.8371\n",
      "Epoch 99/500\n",
      "80/80 [==============================] - 0s 569us/step - loss: 0.4028 - accuracy: 0.8372\n",
      "Epoch 100/500\n",
      "80/80 [==============================] - 0s 549us/step - loss: 0.4027 - accuracy: 0.8370\n",
      "Epoch 101/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.4028 - accuracy: 0.8370\n",
      "Epoch 102/500\n",
      "80/80 [==============================] - 0s 554us/step - loss: 0.4026 - accuracy: 0.8367\n",
      "Epoch 103/500\n",
      "80/80 [==============================] - 0s 561us/step - loss: 0.4024 - accuracy: 0.8370\n",
      "Epoch 104/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.4024 - accuracy: 0.8372\n",
      "Epoch 105/500\n",
      "80/80 [==============================] - 0s 574us/step - loss: 0.4024 - accuracy: 0.8369\n",
      "Epoch 106/500\n",
      "80/80 [==============================] - 0s 545us/step - loss: 0.4023 - accuracy: 0.8374\n",
      "Epoch 107/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.4022 - accuracy: 0.8367\n",
      "Epoch 108/500\n",
      "80/80 [==============================] - 0s 546us/step - loss: 0.4021 - accuracy: 0.8369\n",
      "Epoch 109/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.4020 - accuracy: 0.8379\n",
      "Epoch 110/500\n",
      "80/80 [==============================] - 0s 548us/step - loss: 0.4020 - accuracy: 0.8370\n",
      "Epoch 111/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.4020 - accuracy: 0.8367\n",
      "Epoch 112/500\n",
      "80/80 [==============================] - 0s 556us/step - loss: 0.4019 - accuracy: 0.8370\n",
      "Epoch 113/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.4018 - accuracy: 0.8375\n",
      "Epoch 114/500\n",
      "80/80 [==============================] - 0s 599us/step - loss: 0.4017 - accuracy: 0.8369\n",
      "Epoch 115/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.4017 - accuracy: 0.8371\n",
      "Epoch 116/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.4017 - accuracy: 0.8374\n",
      "Epoch 117/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.4016 - accuracy: 0.8374\n",
      "Epoch 118/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.4017 - accuracy: 0.8371\n",
      "Epoch 119/500\n",
      "80/80 [==============================] - 0s 560us/step - loss: 0.4016 - accuracy: 0.8374\n",
      "Epoch 120/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.4014 - accuracy: 0.8372\n",
      "Epoch 121/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.4013 - accuracy: 0.8375\n",
      "Epoch 122/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.4013 - accuracy: 0.8378\n",
      "Epoch 123/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.4012 - accuracy: 0.8376\n",
      "Epoch 124/500\n",
      "80/80 [==============================] - 0s 573us/step - loss: 0.4012 - accuracy: 0.8372\n",
      "Epoch 125/500\n",
      "80/80 [==============================] - 0s 544us/step - loss: 0.4011 - accuracy: 0.8367\n",
      "Epoch 126/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.4010 - accuracy: 0.8375\n",
      "Epoch 127/500\n",
      "80/80 [==============================] - 0s 571us/step - loss: 0.4010 - accuracy: 0.8381\n",
      "Epoch 128/500\n",
      "80/80 [==============================] - 0s 574us/step - loss: 0.4010 - accuracy: 0.8371\n",
      "Epoch 129/500\n",
      "80/80 [==============================] - 0s 545us/step - loss: 0.4009 - accuracy: 0.8380\n",
      "Epoch 130/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.4008 - accuracy: 0.8378\n",
      "Epoch 131/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.4007 - accuracy: 0.8374\n",
      "Epoch 132/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.4006 - accuracy: 0.8378\n",
      "Epoch 133/500\n",
      "80/80 [==============================] - 0s 561us/step - loss: 0.4006 - accuracy: 0.8390\n",
      "Epoch 134/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.4004 - accuracy: 0.8382\n",
      "Epoch 135/500\n",
      "80/80 [==============================] - 0s 559us/step - loss: 0.4004 - accuracy: 0.8376\n",
      "Epoch 136/500\n",
      "80/80 [==============================] - 0s 569us/step - loss: 0.4002 - accuracy: 0.8389\n",
      "Epoch 137/500\n",
      "80/80 [==============================] - 0s 569us/step - loss: 0.4001 - accuracy: 0.8389\n",
      "Epoch 138/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.4000 - accuracy: 0.8393\n",
      "Epoch 139/500\n",
      "80/80 [==============================] - 0s 544us/step - loss: 0.3999 - accuracy: 0.8376\n",
      "Epoch 140/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3997 - accuracy: 0.8382\n",
      "Epoch 141/500\n",
      "80/80 [==============================] - 0s 545us/step - loss: 0.3995 - accuracy: 0.8386\n",
      "Epoch 142/500\n",
      "80/80 [==============================] - 0s 562us/step - loss: 0.3993 - accuracy: 0.8382\n",
      "Epoch 143/500\n",
      "80/80 [==============================] - 0s 555us/step - loss: 0.3990 - accuracy: 0.8388\n",
      "Epoch 144/500\n",
      "80/80 [==============================] - 0s 566us/step - loss: 0.3987 - accuracy: 0.8391\n",
      "Epoch 145/500\n",
      "80/80 [==============================] - 0s 574us/step - loss: 0.3984 - accuracy: 0.8391\n",
      "Epoch 146/500\n",
      "80/80 [==============================] - 0s 550us/step - loss: 0.3980 - accuracy: 0.8393\n",
      "Epoch 147/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3976 - accuracy: 0.8393\n",
      "Epoch 148/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.3971 - accuracy: 0.8399\n",
      "Epoch 149/500\n",
      "80/80 [==============================] - 0s 569us/step - loss: 0.3966 - accuracy: 0.8403\n",
      "Epoch 150/500\n",
      "80/80 [==============================] - 0s 552us/step - loss: 0.3960 - accuracy: 0.8404\n",
      "Epoch 151/500\n",
      "80/80 [==============================] - 0s 553us/step - loss: 0.3953 - accuracy: 0.8404\n",
      "Epoch 152/500\n",
      "80/80 [==============================] - 0s 544us/step - loss: 0.3946 - accuracy: 0.8397\n",
      "Epoch 153/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.3940 - accuracy: 0.8403\n",
      "Epoch 154/500\n",
      "80/80 [==============================] - 0s 569us/step - loss: 0.3931 - accuracy: 0.8403\n",
      "Epoch 155/500\n",
      "80/80 [==============================] - 0s 568us/step - loss: 0.3924 - accuracy: 0.8404\n",
      "Epoch 156/500\n",
      "80/80 [==============================] - 0s 563us/step - loss: 0.3916 - accuracy: 0.8403\n",
      "Epoch 157/500\n",
      "80/80 [==============================] - 0s 569us/step - loss: 0.3907 - accuracy: 0.8409\n",
      "Epoch 158/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.3899 - accuracy: 0.8401\n",
      "Epoch 159/500\n",
      "80/80 [==============================] - 0s 561us/step - loss: 0.3892 - accuracy: 0.8404\n",
      "Epoch 160/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 557us/step - loss: 0.3884 - accuracy: 0.8409\n",
      "Epoch 161/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3876 - accuracy: 0.8410\n",
      "Epoch 162/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.3868 - accuracy: 0.8416\n",
      "Epoch 163/500\n",
      "80/80 [==============================] - 0s 561us/step - loss: 0.3861 - accuracy: 0.8418\n",
      "Epoch 164/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3855 - accuracy: 0.8425\n",
      "Epoch 165/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.3847 - accuracy: 0.8424\n",
      "Epoch 166/500\n",
      "80/80 [==============================] - 0s 569us/step - loss: 0.3840 - accuracy: 0.8428\n",
      "Epoch 167/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.3833 - accuracy: 0.8426\n",
      "Epoch 168/500\n",
      "80/80 [==============================] - 0s 573us/step - loss: 0.3826 - accuracy: 0.8429\n",
      "Epoch 169/500\n",
      "80/80 [==============================] - 0s 569us/step - loss: 0.3819 - accuracy: 0.8432\n",
      "Epoch 170/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.3812 - accuracy: 0.8440\n",
      "Epoch 171/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.3806 - accuracy: 0.8440\n",
      "Epoch 172/500\n",
      "80/80 [==============================] - 0s 569us/step - loss: 0.3799 - accuracy: 0.8441\n",
      "Epoch 173/500\n",
      "80/80 [==============================] - 0s 549us/step - loss: 0.3793 - accuracy: 0.8444\n",
      "Epoch 174/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3787 - accuracy: 0.8445\n",
      "Epoch 175/500\n",
      "80/80 [==============================] - 0s 546us/step - loss: 0.3780 - accuracy: 0.8455\n",
      "Epoch 176/500\n",
      "80/80 [==============================] - 0s 569us/step - loss: 0.3773 - accuracy: 0.8450\n",
      "Epoch 177/500\n",
      "80/80 [==============================] - 0s 560us/step - loss: 0.3767 - accuracy: 0.8454\n",
      "Epoch 178/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3760 - accuracy: 0.8460\n",
      "Epoch 179/500\n",
      "80/80 [==============================] - 0s 545us/step - loss: 0.3754 - accuracy: 0.8469\n",
      "Epoch 180/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3749 - accuracy: 0.8468\n",
      "Epoch 181/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.3742 - accuracy: 0.8474\n",
      "Epoch 182/500\n",
      "80/80 [==============================] - 0s 561us/step - loss: 0.3736 - accuracy: 0.8479\n",
      "Epoch 183/500\n",
      "80/80 [==============================] - 0s 545us/step - loss: 0.3730 - accuracy: 0.8487\n",
      "Epoch 184/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3725 - accuracy: 0.8485\n",
      "Epoch 185/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3720 - accuracy: 0.8486\n",
      "Epoch 186/500\n",
      "80/80 [==============================] - 0s 573us/step - loss: 0.3714 - accuracy: 0.8495\n",
      "Epoch 187/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.3709 - accuracy: 0.8496\n",
      "Epoch 188/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.3703 - accuracy: 0.8499\n",
      "Epoch 189/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.3699 - accuracy: 0.8503\n",
      "Epoch 190/500\n",
      "80/80 [==============================] - 0s 545us/step - loss: 0.3694 - accuracy: 0.8503\n",
      "Epoch 191/500\n",
      "80/80 [==============================] - 0s 585us/step - loss: 0.3689 - accuracy: 0.8503\n",
      "Epoch 192/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3686 - accuracy: 0.8506\n",
      "Epoch 193/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.3680 - accuracy: 0.8505\n",
      "Epoch 194/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.3676 - accuracy: 0.8506\n",
      "Epoch 195/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.3674 - accuracy: 0.8508\n",
      "Epoch 196/500\n",
      "80/80 [==============================] - 0s 561us/step - loss: 0.3668 - accuracy: 0.8516\n",
      "Epoch 197/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.3665 - accuracy: 0.8518\n",
      "Epoch 198/500\n",
      "80/80 [==============================] - 0s 556us/step - loss: 0.3661 - accuracy: 0.8515\n",
      "Epoch 199/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3657 - accuracy: 0.8520\n",
      "Epoch 200/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.3653 - accuracy: 0.8533\n",
      "Epoch 201/500\n",
      "80/80 [==============================] - 0s 561us/step - loss: 0.3648 - accuracy: 0.8528\n",
      "Epoch 202/500\n",
      "80/80 [==============================] - 0s 569us/step - loss: 0.3644 - accuracy: 0.8535\n",
      "Epoch 203/500\n",
      "80/80 [==============================] - 0s 545us/step - loss: 0.3642 - accuracy: 0.8539\n",
      "Epoch 204/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3638 - accuracy: 0.8536\n",
      "Epoch 205/500\n",
      "80/80 [==============================] - 0s 561us/step - loss: 0.3636 - accuracy: 0.8541\n",
      "Epoch 206/500\n",
      "80/80 [==============================] - 0s 556us/step - loss: 0.3632 - accuracy: 0.8549\n",
      "Epoch 207/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.3629 - accuracy: 0.8546\n",
      "Epoch 208/500\n",
      "80/80 [==============================] - 0s 569us/step - loss: 0.3625 - accuracy: 0.8550\n",
      "Epoch 209/500\n",
      "80/80 [==============================] - 0s 571us/step - loss: 0.3624 - accuracy: 0.8550\n",
      "Epoch 210/500\n",
      "80/80 [==============================] - 0s 560us/step - loss: 0.3620 - accuracy: 0.8556\n",
      "Epoch 211/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.3617 - accuracy: 0.8550\n",
      "Epoch 212/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3615 - accuracy: 0.8556\n",
      "Epoch 213/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.3611 - accuracy: 0.8556\n",
      "Epoch 214/500\n",
      "80/80 [==============================] - 0s 575us/step - loss: 0.3609 - accuracy: 0.8559\n",
      "Epoch 215/500\n",
      "80/80 [==============================] - 0s 555us/step - loss: 0.3607 - accuracy: 0.8556\n",
      "Epoch 216/500\n",
      "80/80 [==============================] - 0s 545us/step - loss: 0.3605 - accuracy: 0.8559\n",
      "Epoch 217/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3602 - accuracy: 0.8554\n",
      "Epoch 218/500\n",
      "80/80 [==============================] - 0s 571us/step - loss: 0.3600 - accuracy: 0.8564\n",
      "Epoch 219/500\n",
      "80/80 [==============================] - 0s 562us/step - loss: 0.3597 - accuracy: 0.8561\n",
      "Epoch 220/500\n",
      "80/80 [==============================] - 0s 569us/step - loss: 0.3595 - accuracy: 0.8570\n",
      "Epoch 221/500\n",
      "80/80 [==============================] - 0s 582us/step - loss: 0.3592 - accuracy: 0.8568\n",
      "Epoch 222/500\n",
      "80/80 [==============================] - 0s 582us/step - loss: 0.3590 - accuracy: 0.8569\n",
      "Epoch 223/500\n",
      "80/80 [==============================] - 0s 545us/step - loss: 0.3589 - accuracy: 0.8574\n",
      "Epoch 224/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.3586 - accuracy: 0.8585\n",
      "Epoch 225/500\n",
      "80/80 [==============================] - 0s 560us/step - loss: 0.3585 - accuracy: 0.8568\n",
      "Epoch 226/500\n",
      "80/80 [==============================] - 0s 545us/step - loss: 0.3583 - accuracy: 0.8560\n",
      "Epoch 227/500\n",
      "80/80 [==============================] - 0s 569us/step - loss: 0.3581 - accuracy: 0.8580\n",
      "Epoch 228/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.3578 - accuracy: 0.8586\n",
      "Epoch 229/500\n",
      "80/80 [==============================] - 0s 561us/step - loss: 0.3577 - accuracy: 0.8576\n",
      "Epoch 230/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.3575 - accuracy: 0.8587\n",
      "Epoch 231/500\n",
      "80/80 [==============================] - 0s 560us/step - loss: 0.3572 - accuracy: 0.8595\n",
      "Epoch 232/500\n",
      "80/80 [==============================] - 0s 542us/step - loss: 0.3571 - accuracy: 0.8590\n",
      "Epoch 233/500\n",
      "80/80 [==============================] - 0s 571us/step - loss: 0.3568 - accuracy: 0.8587\n",
      "Epoch 234/500\n",
      "80/80 [==============================] - 0s 573us/step - loss: 0.3568 - accuracy: 0.8595\n",
      "Epoch 235/500\n",
      "80/80 [==============================] - 0s 556us/step - loss: 0.3565 - accuracy: 0.8587\n",
      "Epoch 236/500\n",
      "80/80 [==============================] - 0s 571us/step - loss: 0.3563 - accuracy: 0.8595\n",
      "Epoch 237/500\n",
      "80/80 [==============================] - 0s 544us/step - loss: 0.3561 - accuracy: 0.8594\n",
      "Epoch 238/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.3560 - accuracy: 0.8597\n",
      "Epoch 239/500\n",
      "80/80 [==============================] - 0s 561us/step - loss: 0.3558 - accuracy: 0.8594\n",
      "Epoch 240/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.3557 - accuracy: 0.8590\n",
      "Epoch 241/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3556 - accuracy: 0.8593\n",
      "Epoch 242/500\n",
      "80/80 [==============================] - 0s 582us/step - loss: 0.3553 - accuracy: 0.8597\n",
      "Epoch 243/500\n",
      "80/80 [==============================] - 0s 573us/step - loss: 0.3552 - accuracy: 0.8584\n",
      "Epoch 244/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.3551 - accuracy: 0.8591\n",
      "Epoch 245/500\n",
      "80/80 [==============================] - 0s 684us/step - loss: 0.3548 - accuracy: 0.8589\n",
      "Epoch 246/500\n",
      "80/80 [==============================] - 0s 604us/step - loss: 0.3547 - accuracy: 0.8600\n",
      "Epoch 247/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3546 - accuracy: 0.8594\n",
      "Epoch 248/500\n",
      "80/80 [==============================] - 0s 624us/step - loss: 0.3545 - accuracy: 0.8597\n",
      "Epoch 249/500\n",
      "80/80 [==============================] - 0s 650us/step - loss: 0.3544 - accuracy: 0.8594\n",
      "Epoch 250/500\n",
      "80/80 [==============================] - 0s 601us/step - loss: 0.3541 - accuracy: 0.8595\n",
      "Epoch 251/500\n",
      "80/80 [==============================] - 0s 608us/step - loss: 0.3539 - accuracy: 0.8601\n",
      "Epoch 252/500\n",
      "80/80 [==============================] - 0s 596us/step - loss: 0.3539 - accuracy: 0.8597\n",
      "Epoch 253/500\n",
      "80/80 [==============================] - 0s 581us/step - loss: 0.3538 - accuracy: 0.8590\n",
      "Epoch 254/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3536 - accuracy: 0.8594\n",
      "Epoch 255/500\n",
      "80/80 [==============================] - 0s 608us/step - loss: 0.3534 - accuracy: 0.8599\n",
      "Epoch 256/500\n",
      "80/80 [==============================] - 0s 620us/step - loss: 0.3533 - accuracy: 0.8597\n",
      "Epoch 257/500\n",
      "80/80 [==============================] - 0s 608us/step - loss: 0.3532 - accuracy: 0.8596\n",
      "Epoch 258/500\n",
      "80/80 [==============================] - 0s 582us/step - loss: 0.3531 - accuracy: 0.8590\n",
      "Epoch 259/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3531 - accuracy: 0.8593\n",
      "Epoch 260/500\n",
      "80/80 [==============================] - 0s 607us/step - loss: 0.3527 - accuracy: 0.8597\n",
      "Epoch 261/500\n",
      "80/80 [==============================] - 0s 595us/step - loss: 0.3527 - accuracy: 0.8593\n",
      "Epoch 262/500\n",
      "80/80 [==============================] - 0s 621us/step - loss: 0.3525 - accuracy: 0.8597\n",
      "Epoch 263/500\n",
      "80/80 [==============================] - 0s 596us/step - loss: 0.3525 - accuracy: 0.8601\n",
      "Epoch 264/500\n",
      "80/80 [==============================] - 0s 621us/step - loss: 0.3523 - accuracy: 0.8594\n",
      "Epoch 265/500\n",
      "80/80 [==============================] - 0s 569us/step - loss: 0.3522 - accuracy: 0.8595\n",
      "Epoch 266/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.3522 - accuracy: 0.8595\n",
      "Epoch 267/500\n",
      "80/80 [==============================] - 0s 587us/step - loss: 0.3522 - accuracy: 0.8594\n",
      "Epoch 268/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.3518 - accuracy: 0.8594\n",
      "Epoch 269/500\n",
      "80/80 [==============================] - 0s 569us/step - loss: 0.3518 - accuracy: 0.8585\n",
      "Epoch 270/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.3517 - accuracy: 0.8597\n",
      "Epoch 271/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.3517 - accuracy: 0.8604\n",
      "Epoch 272/500\n",
      "80/80 [==============================] - 0s 574us/step - loss: 0.3514 - accuracy: 0.8601\n",
      "Epoch 273/500\n",
      "80/80 [==============================] - 0s 569us/step - loss: 0.3513 - accuracy: 0.8597\n",
      "Epoch 274/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.3513 - accuracy: 0.8610\n",
      "Epoch 275/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3511 - accuracy: 0.8602\n",
      "Epoch 276/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3510 - accuracy: 0.8602\n",
      "Epoch 277/500\n",
      "80/80 [==============================] - 0s 574us/step - loss: 0.3508 - accuracy: 0.8604\n",
      "Epoch 278/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3509 - accuracy: 0.8600\n",
      "Epoch 279/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3508 - accuracy: 0.8602\n",
      "Epoch 280/500\n",
      "80/80 [==============================] - 0s 545us/step - loss: 0.3506 - accuracy: 0.8596\n",
      "Epoch 281/500\n",
      "80/80 [==============================] - 0s 561us/step - loss: 0.3505 - accuracy: 0.8597\n",
      "Epoch 282/500\n",
      "80/80 [==============================] - 0s 556us/step - loss: 0.3505 - accuracy: 0.8605\n",
      "Epoch 283/500\n",
      "80/80 [==============================] - 0s 545us/step - loss: 0.3503 - accuracy: 0.8584\n",
      "Epoch 284/500\n",
      "80/80 [==============================] - 0s 571us/step - loss: 0.3505 - accuracy: 0.8608\n",
      "Epoch 285/500\n",
      "80/80 [==============================] - 0s 571us/step - loss: 0.3502 - accuracy: 0.8602\n",
      "Epoch 286/500\n",
      "80/80 [==============================] - 0s 572us/step - loss: 0.3501 - accuracy: 0.8606\n",
      "Epoch 287/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3501 - accuracy: 0.8605\n",
      "Epoch 288/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.3499 - accuracy: 0.8597\n",
      "Epoch 289/500\n",
      "80/80 [==============================] - 0s 544us/step - loss: 0.3498 - accuracy: 0.8606\n",
      "Epoch 290/500\n",
      "80/80 [==============================] - 0s 561us/step - loss: 0.3498 - accuracy: 0.8605\n",
      "Epoch 291/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3497 - accuracy: 0.8599\n",
      "Epoch 292/500\n",
      "80/80 [==============================] - 0s 564us/step - loss: 0.3496 - accuracy: 0.8614\n",
      "Epoch 293/500\n",
      "80/80 [==============================] - 0s 564us/step - loss: 0.3496 - accuracy: 0.8609\n",
      "Epoch 294/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.3496 - accuracy: 0.8596\n",
      "Epoch 295/500\n",
      "80/80 [==============================] - 0s 573us/step - loss: 0.3493 - accuracy: 0.8611\n",
      "Epoch 296/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.3494 - accuracy: 0.8616\n",
      "Epoch 297/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.3493 - accuracy: 0.8608\n",
      "Epoch 298/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3492 - accuracy: 0.8604\n",
      "Epoch 299/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.3491 - accuracy: 0.8600\n",
      "Epoch 300/500\n",
      "80/80 [==============================] - 0s 561us/step - loss: 0.3490 - accuracy: 0.8609\n",
      "Epoch 301/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.3490 - accuracy: 0.8605\n",
      "Epoch 302/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3488 - accuracy: 0.8604\n",
      "Epoch 303/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.3488 - accuracy: 0.8611\n",
      "Epoch 304/500\n",
      "80/80 [==============================] - 0s 595us/step - loss: 0.3489 - accuracy: 0.8612\n",
      "Epoch 305/500\n",
      "80/80 [==============================] - 0s 586us/step - loss: 0.3488 - accuracy: 0.8605\n",
      "Epoch 306/500\n",
      "80/80 [==============================] - 0s 569us/step - loss: 0.3487 - accuracy: 0.8606\n",
      "Epoch 307/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.3486 - accuracy: 0.8601\n",
      "Epoch 308/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.3485 - accuracy: 0.8604\n",
      "Epoch 309/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3485 - accuracy: 0.8606\n",
      "Epoch 310/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.3483 - accuracy: 0.8610\n",
      "Epoch 311/500\n",
      "80/80 [==============================] - 0s 574us/step - loss: 0.3484 - accuracy: 0.8608\n",
      "Epoch 312/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.3482 - accuracy: 0.8606\n",
      "Epoch 313/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.3482 - accuracy: 0.8604\n",
      "Epoch 314/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3481 - accuracy: 0.8611\n",
      "Epoch 315/500\n",
      "80/80 [==============================] - 0s 584us/step - loss: 0.3482 - accuracy: 0.8614\n",
      "Epoch 316/500\n",
      "80/80 [==============================] - 0s 569us/step - loss: 0.3481 - accuracy: 0.8605\n",
      "Epoch 317/500\n",
      "80/80 [==============================] - 0s 559us/step - loss: 0.3480 - accuracy: 0.8614\n",
      "Epoch 318/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 559us/step - loss: 0.3478 - accuracy: 0.8605\n",
      "Epoch 319/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.3479 - accuracy: 0.8611\n",
      "Epoch 320/500\n",
      "80/80 [==============================] - 0s 574us/step - loss: 0.3478 - accuracy: 0.8614\n",
      "Epoch 321/500\n",
      "80/80 [==============================] - 0s 560us/step - loss: 0.3477 - accuracy: 0.8614\n",
      "Epoch 322/500\n",
      "80/80 [==============================] - 0s 572us/step - loss: 0.3477 - accuracy: 0.8619\n",
      "Epoch 323/500\n",
      "80/80 [==============================] - 0s 550us/step - loss: 0.3477 - accuracy: 0.8609\n",
      "Epoch 324/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.3475 - accuracy: 0.8606\n",
      "Epoch 325/500\n",
      "80/80 [==============================] - 0s 569us/step - loss: 0.3475 - accuracy: 0.8609\n",
      "Epoch 326/500\n",
      "80/80 [==============================] - 0s 574us/step - loss: 0.3475 - accuracy: 0.8614\n",
      "Epoch 327/500\n",
      "80/80 [==============================] - 0s 569us/step - loss: 0.3475 - accuracy: 0.8608\n",
      "Epoch 328/500\n",
      "80/80 [==============================] - 0s 658us/step - loss: 0.3474 - accuracy: 0.8612\n",
      "Epoch 329/500\n",
      "80/80 [==============================] - 0s 609us/step - loss: 0.3474 - accuracy: 0.8610\n",
      "Epoch 330/500\n",
      "80/80 [==============================] - 0s 555us/step - loss: 0.3472 - accuracy: 0.8606\n",
      "Epoch 331/500\n",
      "80/80 [==============================] - 0s 571us/step - loss: 0.3472 - accuracy: 0.8618\n",
      "Epoch 332/500\n",
      "80/80 [==============================] - 0s 594us/step - loss: 0.3472 - accuracy: 0.8610\n",
      "Epoch 333/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.3471 - accuracy: 0.8612\n",
      "Epoch 334/500\n",
      "80/80 [==============================] - 0s 596us/step - loss: 0.3471 - accuracy: 0.8616\n",
      "Epoch 335/500\n",
      "80/80 [==============================] - 0s 581us/step - loss: 0.3469 - accuracy: 0.8616\n",
      "Epoch 336/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.3471 - accuracy: 0.8610\n",
      "Epoch 337/500\n",
      "80/80 [==============================] - 0s 574us/step - loss: 0.3469 - accuracy: 0.8614\n",
      "Epoch 338/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3468 - accuracy: 0.8619\n",
      "Epoch 339/500\n",
      "80/80 [==============================] - 0s 571us/step - loss: 0.3468 - accuracy: 0.8612\n",
      "Epoch 340/500\n",
      "80/80 [==============================] - 0s 573us/step - loss: 0.3468 - accuracy: 0.8611\n",
      "Epoch 341/500\n",
      "80/80 [==============================] - 0s 561us/step - loss: 0.3467 - accuracy: 0.8619\n",
      "Epoch 342/500\n",
      "80/80 [==============================] - 0s 584us/step - loss: 0.3467 - accuracy: 0.8608\n",
      "Epoch 343/500\n",
      "80/80 [==============================] - 0s 556us/step - loss: 0.3467 - accuracy: 0.8625\n",
      "Epoch 344/500\n",
      "80/80 [==============================] - 0s 563us/step - loss: 0.3465 - accuracy: 0.8612\n",
      "Epoch 345/500\n",
      "80/80 [==============================] - 0s 576us/step - loss: 0.3466 - accuracy: 0.8609\n",
      "Epoch 346/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3466 - accuracy: 0.8615\n",
      "Epoch 347/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.3464 - accuracy: 0.8626\n",
      "Epoch 348/500\n",
      "80/80 [==============================] - 0s 573us/step - loss: 0.3465 - accuracy: 0.8620\n",
      "Epoch 349/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3464 - accuracy: 0.8616\n",
      "Epoch 350/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.3463 - accuracy: 0.8605\n",
      "Epoch 351/500\n",
      "80/80 [==============================] - 0s 569us/step - loss: 0.3463 - accuracy: 0.8614\n",
      "Epoch 352/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.3463 - accuracy: 0.8621\n",
      "Epoch 353/500\n",
      "80/80 [==============================] - 0s 573us/step - loss: 0.3462 - accuracy: 0.8622\n",
      "Epoch 354/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3462 - accuracy: 0.8612\n",
      "Epoch 355/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.3462 - accuracy: 0.8611\n",
      "Epoch 356/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.3461 - accuracy: 0.8610\n",
      "Epoch 357/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3461 - accuracy: 0.8614\n",
      "Epoch 358/500\n",
      "80/80 [==============================] - 0s 577us/step - loss: 0.3460 - accuracy: 0.8610\n",
      "Epoch 359/500\n",
      "80/80 [==============================] - 0s 568us/step - loss: 0.3459 - accuracy: 0.8614\n",
      "Epoch 360/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.3461 - accuracy: 0.8615\n",
      "Epoch 361/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.3460 - accuracy: 0.8610\n",
      "Epoch 362/500\n",
      "80/80 [==============================] - 0s 594us/step - loss: 0.3458 - accuracy: 0.8625\n",
      "Epoch 363/500\n",
      "80/80 [==============================] - 0s 573us/step - loss: 0.3458 - accuracy: 0.8622\n",
      "Epoch 364/500\n",
      "80/80 [==============================] - 0s 571us/step - loss: 0.3458 - accuracy: 0.8610\n",
      "Epoch 365/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.3457 - accuracy: 0.8620\n",
      "Epoch 366/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.3458 - accuracy: 0.8622\n",
      "Epoch 367/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.3456 - accuracy: 0.8619\n",
      "Epoch 368/500\n",
      "80/80 [==============================] - 0s 569us/step - loss: 0.3459 - accuracy: 0.8606\n",
      "Epoch 369/500\n",
      "80/80 [==============================] - 0s 561us/step - loss: 0.3456 - accuracy: 0.8611\n",
      "Epoch 370/500\n",
      "80/80 [==============================] - 0s 571us/step - loss: 0.3455 - accuracy: 0.8615\n",
      "Epoch 371/500\n",
      "80/80 [==============================] - 0s 568us/step - loss: 0.3456 - accuracy: 0.8621\n",
      "Epoch 372/500\n",
      "80/80 [==============================] - 0s 571us/step - loss: 0.3455 - accuracy: 0.8616\n",
      "Epoch 373/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3455 - accuracy: 0.8611\n",
      "Epoch 374/500\n",
      "80/80 [==============================] - 0s 561us/step - loss: 0.3454 - accuracy: 0.8621\n",
      "Epoch 375/500\n",
      "80/80 [==============================] - 0s 569us/step - loss: 0.3454 - accuracy: 0.8629\n",
      "Epoch 376/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3453 - accuracy: 0.8629\n",
      "Epoch 377/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.3453 - accuracy: 0.8616\n",
      "Epoch 378/500\n",
      "80/80 [==============================] - 0s 571us/step - loss: 0.3453 - accuracy: 0.8614\n",
      "Epoch 379/500\n",
      "80/80 [==============================] - 0s 560us/step - loss: 0.3453 - accuracy: 0.8618\n",
      "Epoch 380/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.3453 - accuracy: 0.8614\n",
      "Epoch 381/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3452 - accuracy: 0.8627\n",
      "Epoch 382/500\n",
      "80/80 [==============================] - 0s 571us/step - loss: 0.3451 - accuracy: 0.8609\n",
      "Epoch 383/500\n",
      "80/80 [==============================] - 0s 584us/step - loss: 0.3452 - accuracy: 0.8621\n",
      "Epoch 384/500\n",
      "80/80 [==============================] - 0s 560us/step - loss: 0.3452 - accuracy: 0.8620\n",
      "Epoch 385/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3451 - accuracy: 0.8619\n",
      "Epoch 386/500\n",
      "80/80 [==============================] - 0s 571us/step - loss: 0.3451 - accuracy: 0.8616\n",
      "Epoch 387/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.3451 - accuracy: 0.8619\n",
      "Epoch 388/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3450 - accuracy: 0.8614\n",
      "Epoch 389/500\n",
      "80/80 [==============================] - 0s 560us/step - loss: 0.3450 - accuracy: 0.8624\n",
      "Epoch 390/500\n",
      "80/80 [==============================] - 0s 556us/step - loss: 0.3449 - accuracy: 0.8609\n",
      "Epoch 391/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.3449 - accuracy: 0.8620\n",
      "Epoch 392/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.3449 - accuracy: 0.8622\n",
      "Epoch 393/500\n",
      "80/80 [==============================] - 0s 573us/step - loss: 0.3449 - accuracy: 0.8610\n",
      "Epoch 394/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.3449 - accuracy: 0.8614\n",
      "Epoch 395/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.3447 - accuracy: 0.8614\n",
      "Epoch 396/500\n",
      "80/80 [==============================] - 0s 569us/step - loss: 0.3449 - accuracy: 0.8602\n",
      "Epoch 397/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.3447 - accuracy: 0.8626\n",
      "Epoch 398/500\n",
      "80/80 [==============================] - 0s 586us/step - loss: 0.3446 - accuracy: 0.8612\n",
      "Epoch 399/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.3447 - accuracy: 0.8615\n",
      "Epoch 400/500\n",
      "80/80 [==============================] - 0s 569us/step - loss: 0.3446 - accuracy: 0.8618\n",
      "Epoch 401/500\n",
      "80/80 [==============================] - 0s 569us/step - loss: 0.3446 - accuracy: 0.8611\n",
      "Epoch 402/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.3447 - accuracy: 0.8606\n",
      "Epoch 403/500\n",
      "80/80 [==============================] - 0s 561us/step - loss: 0.3449 - accuracy: 0.8615\n",
      "Epoch 404/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3446 - accuracy: 0.8621\n",
      "Epoch 405/500\n",
      "80/80 [==============================] - 0s 595us/step - loss: 0.3445 - accuracy: 0.8612\n",
      "Epoch 406/500\n",
      "80/80 [==============================] - 0s 571us/step - loss: 0.3446 - accuracy: 0.8608\n",
      "Epoch 407/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.3444 - accuracy: 0.8619\n",
      "Epoch 408/500\n",
      "80/80 [==============================] - 0s 594us/step - loss: 0.3445 - accuracy: 0.8621\n",
      "Epoch 409/500\n",
      "80/80 [==============================] - 0s 561us/step - loss: 0.3444 - accuracy: 0.8615\n",
      "Epoch 410/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.3443 - accuracy: 0.8611\n",
      "Epoch 411/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.3445 - accuracy: 0.8611\n",
      "Epoch 412/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3444 - accuracy: 0.8606\n",
      "Epoch 413/500\n",
      "80/80 [==============================] - 0s 569us/step - loss: 0.3444 - accuracy: 0.8614\n",
      "Epoch 414/500\n",
      "80/80 [==============================] - 0s 573us/step - loss: 0.3443 - accuracy: 0.8620\n",
      "Epoch 415/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3444 - accuracy: 0.8612\n",
      "Epoch 416/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3442 - accuracy: 0.8611\n",
      "Epoch 417/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.3444 - accuracy: 0.8606\n",
      "Epoch 418/500\n",
      "80/80 [==============================] - 0s 574us/step - loss: 0.3443 - accuracy: 0.8616\n",
      "Epoch 419/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.3443 - accuracy: 0.8614\n",
      "Epoch 420/500\n",
      "80/80 [==============================] - 0s 571us/step - loss: 0.3444 - accuracy: 0.8612\n",
      "Epoch 421/500\n",
      "80/80 [==============================] - 0s 556us/step - loss: 0.3442 - accuracy: 0.8610\n",
      "Epoch 422/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.3442 - accuracy: 0.8615\n",
      "Epoch 423/500\n",
      "80/80 [==============================] - 0s 574us/step - loss: 0.3441 - accuracy: 0.8604\n",
      "Epoch 424/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3441 - accuracy: 0.8606\n",
      "Epoch 425/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3441 - accuracy: 0.8615\n",
      "Epoch 426/500\n",
      "80/80 [==============================] - 0s 559us/step - loss: 0.3440 - accuracy: 0.8618\n",
      "Epoch 427/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3440 - accuracy: 0.8610\n",
      "Epoch 428/500\n",
      "80/80 [==============================] - 0s 560us/step - loss: 0.3440 - accuracy: 0.8605\n",
      "Epoch 429/500\n",
      "80/80 [==============================] - 0s 571us/step - loss: 0.3441 - accuracy: 0.8611\n",
      "Epoch 430/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3440 - accuracy: 0.8612\n",
      "Epoch 431/500\n",
      "80/80 [==============================] - 0s 595us/step - loss: 0.3439 - accuracy: 0.8621\n",
      "Epoch 432/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3440 - accuracy: 0.8612\n",
      "Epoch 433/500\n",
      "80/80 [==============================] - 0s 585us/step - loss: 0.3439 - accuracy: 0.8611\n",
      "Epoch 434/500\n",
      "80/80 [==============================] - 0s 584us/step - loss: 0.3439 - accuracy: 0.8614\n",
      "Epoch 435/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.3439 - accuracy: 0.8611\n",
      "Epoch 436/500\n",
      "80/80 [==============================] - 0s 582us/step - loss: 0.3438 - accuracy: 0.8615\n",
      "Epoch 437/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3439 - accuracy: 0.8612\n",
      "Epoch 438/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3438 - accuracy: 0.8610\n",
      "Epoch 439/500\n",
      "80/80 [==============================] - 0s 573us/step - loss: 0.3439 - accuracy: 0.8601\n",
      "Epoch 440/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3441 - accuracy: 0.8619\n",
      "Epoch 441/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.3437 - accuracy: 0.8611\n",
      "Epoch 442/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.3438 - accuracy: 0.8616\n",
      "Epoch 443/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.3438 - accuracy: 0.8610\n",
      "Epoch 444/500\n",
      "80/80 [==============================] - 0s 561us/step - loss: 0.3437 - accuracy: 0.8611\n",
      "Epoch 445/500\n",
      "80/80 [==============================] - 0s 571us/step - loss: 0.3436 - accuracy: 0.8615\n",
      "Epoch 446/500\n",
      "80/80 [==============================] - 0s 556us/step - loss: 0.3437 - accuracy: 0.8614\n",
      "Epoch 447/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.3436 - accuracy: 0.8611\n",
      "Epoch 448/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3438 - accuracy: 0.8608\n",
      "Epoch 449/500\n",
      "80/80 [==============================] - 0s 560us/step - loss: 0.3436 - accuracy: 0.8614\n",
      "Epoch 450/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3440 - accuracy: 0.8595\n",
      "Epoch 451/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.3436 - accuracy: 0.8614\n",
      "Epoch 452/500\n",
      "80/80 [==============================] - 0s 569us/step - loss: 0.3435 - accuracy: 0.8618\n",
      "Epoch 453/500\n",
      "80/80 [==============================] - 0s 574us/step - loss: 0.3437 - accuracy: 0.8616\n",
      "Epoch 454/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3437 - accuracy: 0.8606\n",
      "Epoch 455/500\n",
      "80/80 [==============================] - 0s 569us/step - loss: 0.3436 - accuracy: 0.8606\n",
      "Epoch 456/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.3435 - accuracy: 0.8614\n",
      "Epoch 457/500\n",
      "80/80 [==============================] - 0s 556us/step - loss: 0.3435 - accuracy: 0.8608\n",
      "Epoch 458/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.3435 - accuracy: 0.8606\n",
      "Epoch 459/500\n",
      "80/80 [==============================] - 0s 573us/step - loss: 0.3434 - accuracy: 0.8611\n",
      "Epoch 460/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.3436 - accuracy: 0.8609\n",
      "Epoch 461/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.3434 - accuracy: 0.8614\n",
      "Epoch 462/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.3435 - accuracy: 0.8610\n",
      "Epoch 463/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.3434 - accuracy: 0.8602\n",
      "Epoch 464/500\n",
      "80/80 [==============================] - 0s 555us/step - loss: 0.3436 - accuracy: 0.8612\n",
      "Epoch 465/500\n",
      "80/80 [==============================] - 0s 575us/step - loss: 0.3433 - accuracy: 0.8604\n",
      "Epoch 466/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3434 - accuracy: 0.8611\n",
      "Epoch 467/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3435 - accuracy: 0.8615\n",
      "Epoch 468/500\n",
      "80/80 [==============================] - 0s 566us/step - loss: 0.3434 - accuracy: 0.8606\n",
      "Epoch 469/500\n",
      "80/80 [==============================] - 0s 565us/step - loss: 0.3436 - accuracy: 0.8606\n",
      "Epoch 470/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.3434 - accuracy: 0.8615\n",
      "Epoch 471/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.3434 - accuracy: 0.8612\n",
      "Epoch 472/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.3433 - accuracy: 0.8616\n",
      "Epoch 473/500\n",
      "80/80 [==============================] - 0s 560us/step - loss: 0.3434 - accuracy: 0.8614\n",
      "Epoch 474/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3433 - accuracy: 0.8614\n",
      "Epoch 475/500\n",
      "80/80 [==============================] - 0s 546us/step - loss: 0.3433 - accuracy: 0.8612\n",
      "Epoch 476/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 557us/step - loss: 0.3432 - accuracy: 0.8611\n",
      "Epoch 477/500\n",
      "80/80 [==============================] - 0s 560us/step - loss: 0.3432 - accuracy: 0.8600\n",
      "Epoch 478/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.3433 - accuracy: 0.8609\n",
      "Epoch 479/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3432 - accuracy: 0.8618\n",
      "Epoch 480/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3432 - accuracy: 0.8614\n",
      "Epoch 481/500\n",
      "80/80 [==============================] - 0s 560us/step - loss: 0.3432 - accuracy: 0.8612\n",
      "Epoch 482/500\n",
      "80/80 [==============================] - 0s 544us/step - loss: 0.3431 - accuracy: 0.8612\n",
      "Epoch 483/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3433 - accuracy: 0.8604\n",
      "Epoch 484/500\n",
      "80/80 [==============================] - 0s 571us/step - loss: 0.3431 - accuracy: 0.8614\n",
      "Epoch 485/500\n",
      "80/80 [==============================] - 0s 548us/step - loss: 0.3433 - accuracy: 0.8611\n",
      "Epoch 486/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3432 - accuracy: 0.8612\n",
      "Epoch 487/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.3431 - accuracy: 0.8610\n",
      "Epoch 488/500\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.3431 - accuracy: 0.8608\n",
      "Epoch 489/500\n",
      "80/80 [==============================] - 0s 548us/step - loss: 0.3431 - accuracy: 0.8606\n",
      "Epoch 490/500\n",
      "80/80 [==============================] - 0s 557us/step - loss: 0.3430 - accuracy: 0.8612\n",
      "Epoch 491/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.3433 - accuracy: 0.8606\n",
      "Epoch 492/500\n",
      "80/80 [==============================] - 0s 543us/step - loss: 0.3434 - accuracy: 0.8606\n",
      "Epoch 493/500\n",
      "80/80 [==============================] - 0s 583us/step - loss: 0.3431 - accuracy: 0.8614\n",
      "Epoch 494/500\n",
      "80/80 [==============================] - 0s 561us/step - loss: 0.3430 - accuracy: 0.8611\n",
      "Epoch 495/500\n",
      "80/80 [==============================] - 0s 546us/step - loss: 0.3430 - accuracy: 0.8608\n",
      "Epoch 496/500\n",
      "80/80 [==============================] - 0s 545us/step - loss: 0.3431 - accuracy: 0.8601\n",
      "Epoch 497/500\n",
      "80/80 [==============================] - 0s 556us/step - loss: 0.3430 - accuracy: 0.8606\n",
      "Epoch 498/500\n",
      "80/80 [==============================] - 0s 573us/step - loss: 0.3431 - accuracy: 0.8615\n",
      "Epoch 499/500\n",
      "80/80 [==============================] - 0s 558us/step - loss: 0.3429 - accuracy: 0.8610\n",
      "Epoch 500/500\n",
      "80/80 [==============================] - 0s 556us/step - loss: 0.3430 - accuracy: 0.8609\n"
     ]
    }
   ],
   "source": [
    "#### Last step in creation of NNmodel. NNmodel is trained on the training set here with Tensor-Keras .fit based on Compiler\n",
    "#Fitting NNmodel\n",
    "history=NNmodel.fit(X_train,Y_train,batch_size=100,epochs = 500)\n",
    "### Note that tf.keras.models.Sequential() by default uses glorot initializer -- drawing intial weights from a uniform \n",
    "### distribution -- see other possibilities in https://keras.io/api/layers/initializers/\n",
    "### Or you could try own customized wts inputs using\n",
    "### for layer in model.layers:\n",
    "###    init_layer_weight = [] # the weights yourself in this layer\n",
    "###    layer.set_weights(init_layer_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73398e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (100, 4)                  52        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (100, 4)                  20        \n",
      "                                                                 \n",
      " dense_2 (Dense)             (100, 1)                  5         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 77\n",
      "Trainable params: 77\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NNmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6656bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.562749981880188, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.7976250052452087, 0.8007500171661377, 0.8082500100135803, 0.8091250061988831, 0.8134999871253967, 0.8147500157356262, 0.8177499771118164, 0.8183749914169312, 0.8198750019073486, 0.8208749890327454, 0.8221250176429749, 0.8230000138282776, 0.824874997138977, 0.8256250023841858, 0.8270000219345093, 0.8272500038146973, 0.827750027179718, 0.8286250233650208, 0.8292499780654907, 0.8293750286102295, 0.8298749923706055, 0.8298749923706055, 0.8295000195503235, 0.8299999833106995, 0.831125020980835, 0.8316249847412109, 0.8297500014305115, 0.8302500247955322, 0.831125020980835, 0.8318750262260437, 0.8309999704360962, 0.8317499756813049, 0.8324999809265137, 0.8323749899864197, 0.8331249952316284, 0.8335000276565552, 0.8338750004768372, 0.8337500095367432, 0.8348749876022339, 0.8345000147819519, 0.8346250057220459, 0.8337500095367432, 0.8347499966621399, 0.8347499966621399, 0.8339999914169312, 0.8343750238418579, 0.8345000147819519, 0.8342499732971191, 0.8347499966621399, 0.8349999785423279, 0.8348749876022339, 0.8353750109672546, 0.8351250290870667, 0.8356249928474426, 0.8357499837875366, 0.8351250290870667, 0.8353750109672546, 0.8353750109672546, 0.8355000019073486, 0.8355000019073486, 0.8358749747276306, 0.8360000252723694, 0.8362500071525574, 0.8358749747276306, 0.8371250033378601, 0.8364999890327454, 0.8370000123977661, 0.8371250033378601, 0.8372499942779541, 0.8370000123977661, 0.8370000123977661, 0.8367499709129333, 0.8370000123977661, 0.8372499942779541, 0.8368750214576721, 0.8373749852180481, 0.8367499709129333, 0.8368750214576721, 0.8378750085830688, 0.8370000123977661, 0.8367499709129333, 0.8370000123977661, 0.8374999761581421, 0.8368750214576721, 0.8371250033378601, 0.8373749852180481, 0.8373749852180481, 0.8371250033378601, 0.8373749852180481, 0.8372499942779541, 0.8374999761581421, 0.8377500176429749, 0.8376250267028809, 0.8372499942779541, 0.8367499709129333, 0.8374999761581421, 0.8381249904632568, 0.8371250033378601, 0.8379999995231628, 0.8377500176429749, 0.8373749852180481, 0.8377500176429749, 0.8389999866485596, 0.8382499814033508, 0.8376250267028809, 0.8388749957084656, 0.8388749957084656, 0.8392500281333923, 0.8376250267028809, 0.8382499814033508, 0.8386250138282776, 0.8382499814033508, 0.8387500047683716, 0.8391249775886536, 0.8391249775886536, 0.8392500281333923, 0.8392500281333923, 0.8398749828338623, 0.8402500152587891, 0.8403750061988831, 0.8403750061988831, 0.8397499918937683, 0.8402500152587891, 0.8402500152587891, 0.8403750061988831, 0.8402500152587891, 0.8408750295639038, 0.8401250243186951, 0.8403750061988831, 0.8408750295639038, 0.8410000205039978, 0.8416249752044678, 0.8417500257492065, 0.8424999713897705, 0.8423749804496765, 0.8427500128746033, 0.8426250219345093, 0.8428750038146973, 0.8432499766349792, 0.843999981880188, 0.843999981880188, 0.844124972820282, 0.8443750143051147, 0.8445000052452087, 0.8454999923706055, 0.8450000286102295, 0.8453750014305115, 0.8460000157356262, 0.846875011920929, 0.846750020980835, 0.8473749756813049, 0.8478749990463257, 0.8487499952316284, 0.8485000133514404, 0.8486250042915344, 0.8495000004768372, 0.8496249914169312, 0.8498749732971191, 0.8502500057220459, 0.8502500057220459, 0.8502500057220459, 0.8506249785423279, 0.8504999876022339, 0.8506249785423279, 0.8507500290870667, 0.8516250252723694, 0.8517500162124634, 0.8514999747276306, 0.8519999980926514, 0.8532500267028809, 0.8527500033378601, 0.8535000085830688, 0.8538749814033508, 0.8536249995231628, 0.8541250228881836, 0.8548750281333923, 0.8546249866485596, 0.8550000190734863, 0.8550000190734863, 0.8556249737739563, 0.8550000190734863, 0.8556249737739563, 0.8556249737739563, 0.8558750152587891, 0.8556249737739563, 0.8558750152587891, 0.8553749918937683, 0.856374979019165, 0.856124997138977, 0.8569999933242798, 0.8567500114440918, 0.8568750023841858, 0.8573750257492065, 0.8585000038146973, 0.8567500114440918, 0.8560000061988831, 0.8579999804496765, 0.8586249947547913, 0.8576250076293945, 0.8587499856948853, 0.859499990940094, 0.859000027179718, 0.8587499856948853, 0.859499990940094, 0.8587499856948853, 0.859499990940094, 0.859375, 0.859749972820282, 0.859375, 0.859000027179718, 0.859250009059906, 0.859749972820282, 0.8583750128746033, 0.859125018119812, 0.8588749766349792, 0.8600000143051147, 0.859375, 0.859749972820282, 0.859375, 0.859499990940094, 0.8601250052452087, 0.859749972820282, 0.859000027179718, 0.859375, 0.8598750233650208, 0.859749972820282, 0.859624981880188, 0.859000027179718, 0.859250009059906, 0.859749972820282, 0.859250009059906, 0.859749972820282, 0.8601250052452087, 0.859375, 0.859499990940094, 0.859499990940094, 0.859375, 0.859375, 0.8585000038146973, 0.859749972820282, 0.8603749871253967, 0.8601250052452087, 0.859749972820282, 0.8610000014305115, 0.8602499961853027, 0.8602499961853027, 0.8603749871253967, 0.8600000143051147, 0.8602499961853027, 0.859624981880188, 0.859749972820282, 0.8604999780654907, 0.8583750128746033, 0.8607500195503235, 0.8602499961853027, 0.8606250286102295, 0.8604999780654907, 0.859749972820282, 0.8606250286102295, 0.8604999780654907, 0.8598750233650208, 0.8613749742507935, 0.8608750104904175, 0.859624981880188, 0.8611249923706055, 0.8616250157356262, 0.8607500195503235, 0.8603749871253967, 0.8600000143051147, 0.8608750104904175, 0.8604999780654907, 0.8603749871253967, 0.8611249923706055, 0.8612499833106995, 0.8604999780654907, 0.8606250286102295, 0.8601250052452087, 0.8603749871253967, 0.8606250286102295, 0.8610000014305115, 0.8607500195503235, 0.8606250286102295, 0.8603749871253967, 0.8611249923706055, 0.8613749742507935, 0.8604999780654907, 0.8613749742507935, 0.8604999780654907, 0.8611249923706055, 0.8613749742507935, 0.8613749742507935, 0.8618749976158142, 0.8608750104904175, 0.8606250286102295, 0.8608750104904175, 0.8613749742507935, 0.8607500195503235, 0.8612499833106995, 0.8610000014305115, 0.8606250286102295, 0.8617500066757202, 0.8610000014305115, 0.8612499833106995, 0.8616250157356262, 0.8616250157356262, 0.8610000014305115, 0.8613749742507935, 0.8618749976158142, 0.8612499833106995, 0.8611249923706055, 0.8618749976158142, 0.8607500195503235, 0.862500011920929, 0.8612499833106995, 0.8608750104904175, 0.8615000247955322, 0.862625002861023, 0.8619999885559082, 0.8616250157356262, 0.8604999780654907, 0.8613749742507935, 0.8621249794960022, 0.8622499704360962, 0.8612499833106995, 0.8611249923706055, 0.8610000014305115, 0.8613749742507935, 0.8610000014305115, 0.8613749742507935, 0.8615000247955322, 0.8610000014305115, 0.862500011920929, 0.8622499704360962, 0.8610000014305115, 0.8619999885559082, 0.8622499704360962, 0.8618749976158142, 0.8606250286102295, 0.8611249923706055, 0.8615000247955322, 0.8621249794960022, 0.8616250157356262, 0.8611249923706055, 0.8621249794960022, 0.8628749847412109, 0.8628749847412109, 0.8616250157356262, 0.8613749742507935, 0.8617500066757202, 0.8613749742507935, 0.8627499938011169, 0.8608750104904175, 0.8621249794960022, 0.8619999885559082, 0.8618749976158142, 0.8616250157356262, 0.8618749976158142, 0.8613749742507935, 0.862375020980835, 0.8608750104904175, 0.8619999885559082, 0.8622499704360962, 0.8610000014305115, 0.8613749742507935, 0.8613749742507935, 0.8602499961853027, 0.862625002861023, 0.8612499833106995, 0.8615000247955322, 0.8617500066757202, 0.8611249923706055, 0.8606250286102295, 0.8615000247955322, 0.8621249794960022, 0.8612499833106995, 0.8607500195503235, 0.8618749976158142, 0.8621249794960022, 0.8615000247955322, 0.8611249923706055, 0.8611249923706055, 0.8606250286102295, 0.8613749742507935, 0.8619999885559082, 0.8612499833106995, 0.8611249923706055, 0.8606250286102295, 0.8616250157356262, 0.8613749742507935, 0.8612499833106995, 0.8610000014305115, 0.8615000247955322, 0.8603749871253967, 0.8606250286102295, 0.8615000247955322, 0.8617500066757202, 0.8610000014305115, 0.8604999780654907, 0.8611249923706055, 0.8612499833106995, 0.8621249794960022, 0.8612499833106995, 0.8611249923706055, 0.8613749742507935, 0.8611249923706055, 0.8615000247955322, 0.8612499833106995, 0.8610000014305115, 0.8601250052452087, 0.8618749976158142, 0.8611249923706055, 0.8616250157356262, 0.8610000014305115, 0.8611249923706055, 0.8615000247955322, 0.8613749742507935, 0.8611249923706055, 0.8607500195503235, 0.8613749742507935, 0.859499990940094, 0.8613749742507935, 0.8617500066757202, 0.8616250157356262, 0.8606250286102295, 0.8606250286102295, 0.8613749742507935, 0.8607500195503235, 0.8606250286102295, 0.8611249923706055, 0.8608750104904175, 0.8613749742507935, 0.8610000014305115, 0.8602499961853027, 0.8612499833106995, 0.8603749871253967, 0.8611249923706055, 0.8615000247955322, 0.8606250286102295, 0.8606250286102295, 0.8615000247955322, 0.8612499833106995, 0.8616250157356262, 0.8613749742507935, 0.8613749742507935, 0.8612499833106995, 0.8611249923706055, 0.8600000143051147, 0.8608750104904175, 0.8617500066757202, 0.8613749742507935, 0.8612499833106995, 0.8612499833106995, 0.8603749871253967, 0.8613749742507935, 0.8611249923706055, 0.8612499833106995, 0.8610000014305115, 0.8607500195503235, 0.8606250286102295, 0.8612499833106995, 0.8606250286102295, 0.8606250286102295, 0.8613749742507935, 0.8611249923706055, 0.8607500195503235, 0.8601250052452087, 0.8606250286102295, 0.8615000247955322, 0.8610000014305115, 0.8608750104904175]\n",
      "[0.683786153793335, 0.6001931428909302, 0.5487366914749146, 0.5193601250648499, 0.5028218626976013, 0.4933633506298065, 0.4877566695213318, 0.48394834995269775, 0.48097455501556396, 0.4782205820083618, 0.4754011034965515, 0.47234871983528137, 0.46898695826530457, 0.46544480323791504, 0.46174657344818115, 0.45809900760650635, 0.4544507563114166, 0.45101016759872437, 0.4477096199989319, 0.44465044140815735, 0.44182437658309937, 0.4393514096736908, 0.43708309531211853, 0.4351670444011688, 0.4334995746612549, 0.43198978900909424, 0.4307962656021118, 0.42969655990600586, 0.42878732085227966, 0.4280487895011902, 0.42726296186447144, 0.42656460404396057, 0.4259289503097534, 0.42526036500930786, 0.42461031675338745, 0.42394283413887024, 0.42327529191970825, 0.4225468337535858, 0.4218820333480835, 0.42111313343048096, 0.42039257287979126, 0.4196094572544098, 0.4189110994338989, 0.41819098591804504, 0.4175785183906555, 0.41690051555633545, 0.41632723808288574, 0.4157741963863373, 0.4152887165546417, 0.4147678315639496, 0.4142078459262848, 0.4136861264705658, 0.4131948947906494, 0.4127395749092102, 0.4122237265110016, 0.41182419657707214, 0.4113835096359253, 0.41095635294914246, 0.41051042079925537, 0.4102555811405182, 0.4097663164138794, 0.4093654751777649, 0.40907755494117737, 0.4086846709251404, 0.40850043296813965, 0.4081437289714813, 0.40775933861732483, 0.4076312780380249, 0.40724804997444153, 0.40693044662475586, 0.4067094922065735, 0.4065147936344147, 0.4063645899295807, 0.4060765206813812, 0.40582436323165894, 0.40571898221969604, 0.4055536985397339, 0.405301034450531, 0.4051136076450348, 0.40494441986083984, 0.4048386216163635, 0.404611736536026, 0.40454089641571045, 0.40446144342422485, 0.4043256640434265, 0.4040798842906952, 0.4040526747703552, 0.40392646193504333, 0.4037667512893677, 0.403685599565506, 0.40356799960136414, 0.4034028649330139, 0.4033495783805847, 0.40323057770729065, 0.40309298038482666, 0.40306907892227173, 0.4028934836387634, 0.4029715955257416, 0.4027898907661438, 0.4027023911476135, 0.40277424454689026, 0.4026053249835968, 0.4024490714073181, 0.402404248714447, 0.40235254168510437, 0.40232548117637634, 0.40217065811157227, 0.4021252691745758, 0.4019840955734253, 0.40201547741889954, 0.4019715487957001, 0.401933491230011, 0.40177616477012634, 0.40173497796058655, 0.4016764163970947, 0.40165066719055176, 0.4015944302082062, 0.4017176330089569, 0.40155109763145447, 0.4014330208301544, 0.40132075548171997, 0.4013146162033081, 0.4012226462364197, 0.40121883153915405, 0.4010638892650604, 0.4009995758533478, 0.4009913206100464, 0.4009852111339569, 0.4009161591529846, 0.40075796842575073, 0.4006650447845459, 0.40058019757270813, 0.40060102939605713, 0.4004318118095398, 0.40037089586257935, 0.40023890137672424, 0.4001466929912567, 0.3999674916267395, 0.39985257387161255, 0.3996739983558655, 0.39945000410079956, 0.3992600440979004, 0.3989882469177246, 0.3987491726875305, 0.398381769657135, 0.39804479479789734, 0.39759308099746704, 0.3970605432987213, 0.39663612842559814, 0.3959658741950989, 0.3953225314617157, 0.3946463465690613, 0.39397159218788147, 0.393096923828125, 0.39244529604911804, 0.3916250467300415, 0.3907361924648285, 0.38989919424057007, 0.3891998827457428, 0.3883882761001587, 0.38764798641204834, 0.386841744184494, 0.3860960900783539, 0.3855265974998474, 0.3846592903137207, 0.38399118185043335, 0.38325953483581543, 0.38258248567581177, 0.38194817304611206, 0.3812220096588135, 0.3806192874908447, 0.3798689842224121, 0.3793047070503235, 0.37865370512008667, 0.3779709041118622, 0.3772655725479126, 0.37673500180244446, 0.3760351538658142, 0.3753625154495239, 0.3748537600040436, 0.3742383122444153, 0.37361839413642883, 0.373015433549881, 0.3725246787071228, 0.3719871938228607, 0.37143823504447937, 0.37088510394096375, 0.37032151222229004, 0.3699134588241577, 0.3694148659706116, 0.3689480125904083, 0.36857134103775024, 0.36803823709487915, 0.3676222264766693, 0.36741897463798523, 0.3668140470981598, 0.3664563000202179, 0.3661186397075653, 0.3656536638736725, 0.3653283417224884, 0.3648332357406616, 0.3644067943096161, 0.36418840289115906, 0.3638399541378021, 0.3635762333869934, 0.36319205164909363, 0.36287325620651245, 0.3625212609767914, 0.36237794160842896, 0.3620215654373169, 0.3617483377456665, 0.3614809513092041, 0.3611206114292145, 0.3609382212162018, 0.3607233166694641, 0.36045220494270325, 0.36021605134010315, 0.3600046932697296, 0.35966700315475464, 0.3595125675201416, 0.35915523767471313, 0.35895031690597534, 0.3588865399360657, 0.35857531428337097, 0.35846593976020813, 0.3582533597946167, 0.35813963413238525, 0.35783129930496216, 0.3576562702655792, 0.357476145029068, 0.35721728205680847, 0.35708922147750854, 0.3568287491798401, 0.3567829430103302, 0.35650554299354553, 0.35632559657096863, 0.35610175132751465, 0.35597410798072815, 0.35580670833587646, 0.35566961765289307, 0.3555702269077301, 0.35533639788627625, 0.35517892241477966, 0.3551371693611145, 0.35484105348587036, 0.3546978533267975, 0.35457268357276917, 0.3544703423976898, 0.3544028401374817, 0.35410240292549133, 0.35392069816589355, 0.35390305519104004, 0.35378530621528625, 0.3536435663700104, 0.3533870577812195, 0.3532652258872986, 0.3531862497329712, 0.3530676066875458, 0.3531368374824524, 0.3527417480945587, 0.35274970531463623, 0.3524753451347351, 0.35246413946151733, 0.3523073196411133, 0.35221439599990845, 0.3521973192691803, 0.35218170285224915, 0.3518330156803131, 0.351791650056839, 0.3516812324523926, 0.3516642451286316, 0.3513698875904083, 0.3513161838054657, 0.35128843784332275, 0.3510551154613495, 0.3510117530822754, 0.3507959246635437, 0.3509005308151245, 0.35084861516952515, 0.35059332847595215, 0.35045740008354187, 0.3504815399646759, 0.3503253161907196, 0.35049107670783997, 0.35017889738082886, 0.3500805199146271, 0.3500601649284363, 0.3499472439289093, 0.34982386231422424, 0.34982824325561523, 0.3496827185153961, 0.34962478280067444, 0.34956392645835876, 0.3495704233646393, 0.34934335947036743, 0.34937068819999695, 0.3493278920650482, 0.3491637706756592, 0.34912431240081787, 0.34900736808776855, 0.34900885820388794, 0.3488418757915497, 0.3488340377807617, 0.3489095866680145, 0.3488304316997528, 0.34868791699409485, 0.34862953424453735, 0.3485289216041565, 0.34853148460388184, 0.3483157455921173, 0.34840846061706543, 0.34823256731033325, 0.3481987714767456, 0.3481433391571045, 0.3481608033180237, 0.34806713461875916, 0.3479749262332916, 0.34776854515075684, 0.3478875160217285, 0.3478192985057831, 0.3477400839328766, 0.34771978855133057, 0.34767940640449524, 0.3475186824798584, 0.3475479483604431, 0.34747663140296936, 0.3475170433521271, 0.34737515449523926, 0.34735816717147827, 0.34720727801322937, 0.3471629321575165, 0.34717193245887756, 0.34709033370018005, 0.3471243679523468, 0.3469266891479492, 0.3471111059188843, 0.34688708186149597, 0.3468347191810608, 0.3467540144920349, 0.3467661142349243, 0.3467036187648773, 0.346670001745224, 0.3466571569442749, 0.3465382754802704, 0.34661003947257996, 0.34656620025634766, 0.34641316533088684, 0.3464633524417877, 0.3463769257068634, 0.3463349938392639, 0.34632402658462524, 0.3463335931301117, 0.34623268246650696, 0.3462386429309845, 0.34615981578826904, 0.3461136221885681, 0.34605586528778076, 0.34599101543426514, 0.3458811044692993, 0.3460889756679535, 0.34599214792251587, 0.34575721621513367, 0.34578028321266174, 0.34583503007888794, 0.34570279717445374, 0.3457745909690857, 0.3455769419670105, 0.34587210416793823, 0.3456335961818695, 0.3455309271812439, 0.34555742144584656, 0.34553489089012146, 0.3454606533050537, 0.3454258441925049, 0.34542787075042725, 0.345276802778244, 0.3452622890472412, 0.34532973170280457, 0.3453117311000824, 0.34529387950897217, 0.3451848030090332, 0.34513476490974426, 0.3452054560184479, 0.34517398476600647, 0.3450929820537567, 0.34509119391441345, 0.3451378643512726, 0.3449789881706238, 0.3450028598308563, 0.34490424394607544, 0.3449161946773529, 0.3449150025844574, 0.3448648750782013, 0.34485140442848206, 0.3446630835533142, 0.3448728621006012, 0.34466028213500977, 0.3446219861507416, 0.3447234630584717, 0.3445896506309509, 0.34458988904953003, 0.34468552470207214, 0.3448736369609833, 0.3445579409599304, 0.3445364236831665, 0.34461504220962524, 0.3443678617477417, 0.34445756673812866, 0.3444161117076874, 0.3443290889263153, 0.3444654643535614, 0.3443908393383026, 0.34435945749282837, 0.3443031311035156, 0.3443821668624878, 0.3442195653915405, 0.34437257051467896, 0.3443160951137543, 0.3442656993865967, 0.34443578124046326, 0.3441612422466278, 0.3441915214061737, 0.34407860040664673, 0.34413179755210876, 0.3440757095813751, 0.343966543674469, 0.3439720869064331, 0.34401047229766846, 0.3441126048564911, 0.34404370188713074, 0.3439180254936218, 0.3440127372741699, 0.3438832461833954, 0.34389057755470276, 0.3439064621925354, 0.34383052587509155, 0.3438650369644165, 0.3437741994857788, 0.3439229428768158, 0.34409618377685547, 0.34373560547828674, 0.3437768220901489, 0.34380242228507996, 0.34370502829551697, 0.3436444401741028, 0.3436588644981384, 0.34360620379447937, 0.34379053115844727, 0.3436272144317627, 0.34403282403945923, 0.3436434864997864, 0.3435494303703308, 0.3436973989009857, 0.3436504900455475, 0.3435686528682709, 0.34348300099372864, 0.34350165724754333, 0.3435308337211609, 0.34343719482421875, 0.34357112646102905, 0.3434372842311859, 0.34348705410957336, 0.3433845639228821, 0.34356945753097534, 0.34332525730133057, 0.3433975279331207, 0.3435053825378418, 0.3433683514595032, 0.3436015844345093, 0.3433617055416107, 0.3433946967124939, 0.34330475330352783, 0.3433503210544586, 0.3432653844356537, 0.34327951073646545, 0.34322163462638855, 0.3432287275791168, 0.3433164656162262, 0.3432466387748718, 0.3431856632232666, 0.3431653678417206, 0.34313130378723145, 0.34325140714645386, 0.34305569529533386, 0.34328991174697876, 0.3431967496871948, 0.34313592314720154, 0.3430744707584381, 0.3430631756782532, 0.34299471974372864, 0.3432704210281372, 0.3434118628501892, 0.3430720269680023, 0.3429943919181824, 0.3430493175983429, 0.34312134981155396, 0.34295833110809326, 0.3430502116680145, 0.34294822812080383, 0.3430083692073822]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(history.history['accuracy'])\n",
    "print(history.history['loss'])\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy and loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['accuracy', 'loss'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a4a041b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.00150113 -0.58312392 -0.57273139 -1.55489968  0.91509065  0.10629772\n",
      "  -0.70174202 -0.26396987  0.80225696  0.64376017  0.97725852 -0.00249134]]\n"
     ]
    }
   ],
   "source": [
    "### Now we use the trained weights and biases to try to predict based on a new case\n",
    "tr=sc.transform([[1, 0, 0, 500, 1, 40, 3, 60000, 2, 1, 1, 100000]])\n",
    "print(tr)  ### tr.shape is (1,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86520ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 65ms/step\n",
      "[[0.04314079]]\n"
     ]
    }
   ],
   "source": [
    "### Example\n",
    "### Predicting result for Single Observation\n",
    "print(NNmodel.predict(tr))\n",
    "### note in each recompute -- this no. will change slightly because of the random initiation of the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0be7271f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 533us/step - loss: 0.3424 - accuracy: 0.8611\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3424062430858612, 0.8611249923706055]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Now we use the trained NNmodel to predict output in X_train sample\n",
    "NNmodel.evaluate(X_train,Y_train)  ### evaluates the loss and accuracy as specified in the Compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ba646c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 472us/step\n"
     ]
    }
   ],
   "source": [
    "### Now we use the trained NNmodel to predict output in X_train sample -- computing manually via .predict\n",
    "TE=NNmodel.predict(X_train)  ### note X_train has 8000 data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "82ce3372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TE.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b21a584d",
   "metadata": {},
   "outputs": [],
   "source": [
    "h=(TE > 0.5).astype(int) ### Convert TE>0.5 == true ==> 1, False to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb651f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8000, 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(h)\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a697768",
   "metadata": {},
   "outputs": [],
   "source": [
    "### replace all elements in numpy array of value 0 with value -1\n",
    "h[h==0]=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c9d41502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1]\n",
      " [-1]\n",
      " [-1]\n",
      " ...\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8000, 1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(h)\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8b950e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### replace all elements in numpy array of value 0 with value -1\n",
    "Y_train1=Y_train\n",
    "Y_train1[Y_train1==0]=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "53b50eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1 -1  1 ...  1 -1  1]\n"
     ]
    }
   ],
   "source": [
    "print(Y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec6af287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fe3de8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6889 0.861125\n"
     ]
    }
   ],
   "source": [
    "J=np.multiply(Y_train1.T,h.T)  ### element by element multiplication\n",
    "c=np.count_nonzero(J > 0) \n",
    "print(c,c/8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4416dde4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 744us/step - loss: 0.3359 - accuracy: 0.8625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3359125852584839, 0.862500011920929]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Now we use the trained NNmodel to predict output in X_test sample\n",
    "NNmodel.evaluate(X_test,Y_test)  ### evaluates the loss and accuracy as specified in the Compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f34f4bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 574us/step\n"
     ]
    }
   ],
   "source": [
    "### Now we use the trained NNmodel to predict output in X_test sample -- computing manually via .predict\n",
    "TE1=NNmodel.predict(X_test)  ### note X_test has 2000 data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "14712483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1725 0.8625\n"
     ]
    }
   ],
   "source": [
    "h1=(TE1 > 0.5).astype(int) ### Convert TE1>0.5 == true ==> 1, False to 0\n",
    "h1[h1==0]=-1\n",
    "Y_test1=Y_test\n",
    "Y_test1[Y_test1==0]=-1\n",
    "J1=np.multiply(Y_test1.T,h1.T)  ### element by element multiplication\n",
    "c1=np.count_nonzero(J1 > 0) \n",
    "print(c1,c1/2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5667ff07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
